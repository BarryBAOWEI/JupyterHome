{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "\n",
    "import os, sys, pickle\n",
    "from datetime import date\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import datetime\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfon = pd.read_csv('C:/Users/jxjsj/Desktop/tianchi/o2ocoupon/ccf_online_stage1_train.csv')\n",
    "dfoff = pd.read_csv('C:/Users/jxjsj/Desktop/tianchi/o2ocoupon/ccf_offline_stage1_train.csv')\n",
    "dftest = pd.read_csv('C:/Users/jxjsj/Desktop/tianchi/o2ocoupon/ccf_offline_stage1_test_revised.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('1970-01-01 00:00:00')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成一个无效日期\n",
    "date_null = pd.to_datetime('1970-01-01', format='%Y-%m-%d')\n",
    "date_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 收到优惠券日期预处理 - 处理为日期格式\n",
    "def dateprocess(row):\n",
    "    if pd.isnull(row):\n",
    "        return date_null\n",
    "    else:\n",
    "        return pd.to_datetime(str(int(row)), format='%Y-%m-%d')\n",
    "\n",
    "dfoff.Date_received = dfoff.Date_received.apply(dateprocess)\n",
    "dfoff.Date = dfoff.Date.apply(dateprocess)\n",
    "dftest.Date_received = dftest.Date_received.apply(dateprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfoff.Distance.fillna(11, inplace=True)\n",
    "dfoff.Distance = dfoff.Distance.astype(int)\n",
    "dfoff.Coupon_id.fillna(0, inplace=True)\n",
    "dfoff.Coupon_id = dfoff.Coupon_id.astype(int)\n",
    "# dfoff.Date_received.fillna(date_null, inplace=True)\n",
    "# dfoff.Date.fillna(date_null, inplace=True)\n",
    "\n",
    "\n",
    "dftest.Distance.fillna(11, inplace=True)\n",
    "dftest.Distance = dftest.Distance.astype(int)\n",
    "dftest.Coupon_id.fillna(0, inplace=True)\n",
    "dftest.Coupon_id = dftest.Coupon_id.astype(int)\n",
    "# dftest.Date_received.fillna(date_null, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复制一个df 用来保存生成的特征，不要影响到原始df \n",
    "df_F_off = dfoff.copy()\n",
    "df_F_test = dftest.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   User_id  Merchant_id  Coupon_id Discount_rate  Distance Date_received  \\\n",
      "0  1439408         2632          0           NaN         0    1970-01-01   \n",
      "1  1439408         4663      11002        150:20         1    2016-05-28   \n",
      "2  1439408         2632       8591          20:1         0    2016-02-17   \n",
      "3  1439408         2632       1078          20:1         0    2016-03-19   \n",
      "4  1439408         2632       8591          20:1         0    2016-06-13   \n",
      "\n",
      "        Date  \n",
      "0 2016-02-17  \n",
      "1 1970-01-01  \n",
      "2 1970-01-01  \n",
      "3 1970-01-01  \n",
      "4 1970-01-01  \n",
      "   User_id  Merchant_id  Coupon_id Discount_rate  Distance Date_received\n",
      "0  4129537          450       9983          30:5         1    2016-07-12\n",
      "1  6949378         1300       3429          30:5        11    2016-07-06\n",
      "2  2166529         7113       6928        200:20         5    2016-07-27\n",
      "3  2166529         7113       1808        100:10         5    2016-07-27\n",
      "4  6172162         7605       6500          30:1         2    2016-07-08\n"
     ]
    }
   ],
   "source": [
    "print(dfoff.head(5))\n",
    "print(dftest.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1754884 entries, 0 to 1754883\n",
      "Data columns (total 7 columns):\n",
      "User_id          int64\n",
      "Merchant_id      int64\n",
      "Coupon_id        int32\n",
      "Discount_rate    object\n",
      "Distance         int32\n",
      "Date_received    datetime64[ns]\n",
      "Date             datetime64[ns]\n",
      "dtypes: datetime64[ns](2), int32(2), int64(2), object(1)\n",
      "memory usage: 80.3+ MB\n"
     ]
    }
   ],
   "source": [
    "dfoff.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有优惠券，购买商品条数，label 1 75382\n",
      "无优惠券，购买商品条数 701602\n",
      "有优惠券，不购买商品条数，label 0 977900\n",
      "无优惠券，不购买商品条数 0\n"
     ]
    }
   ],
   "source": [
    "print('有优惠券，购买商品条数，label 1', dfoff[(dfoff['Date_received'] != date_null) & (dfoff['Date'] != date_null)].shape[0])\n",
    "print('无优惠券，购买商品条数', dfoff[(dfoff['Date_received'] == date_null) & (dfoff['Date'] != date_null)].shape[0])\n",
    "print('有优惠券，不购买商品条数，label 0', dfoff[(dfoff['Date_received'] != date_null) & (dfoff['Date'] == date_null)].shape[0])\n",
    "print('无优惠券，不购买商品条数', dfoff[(dfoff['Date_received'] == date_null) & (dfoff['Date'] == date_null)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. User_id not in training set but in test set {2495873, 1286474} conut: 2\n",
      "2. Merchant_id not in training set but in test set {5920} conut: 1\n",
      "2. Coupon_id not in training set but in test set {6146, 12291, 7, 8, 4104, 14, 10254, 4111, 8209, 6161, 20, 6166, 12311, 10264, 2072, 6167, 31, 8224, 12321, 34, 12323, 6176, 2089, 10285, 2095, 2096, 8242, 6199, 4153, 2106, 10299, 10303, 10306, 4174, 85, 6233, 10331, 8285, 2142, 8287, 8289, 2147, 10342, 105, 2158, 113, 8306, 6258, 4217, 2178, 12428, 6285, 2192, 4241, 8338, 4251, 6299, 2206, 4256, 2209, 4257, 12455, 170, 172, 2224, 12465, 10418, 180, 4277, 6326, 8375, 4279, 10425, 6329, 10431, 4289, 4290, 10437, 10438, 8391, 6344, 2251, 8404, 10453, 12509, 8415, 8424, 232, 8426, 12522, 236, 6377, 2286, 12529, 4339, 2292, 6390, 10489, 6393, 251, 253, 10493, 259, 8453, 2309, 4357, 10505, 4361, 4362, 12559, 8464, 4369, 10521, 8479, 4383, 8482, 294, 297, 8490, 6445, 8499, 10547, 2357, 4403, 10551, 4405, 8506, 10554, 10558, 4415, 320, 6465, 327, 10567, 10568, 8523, 12619, 2381, 4427, 10575, 4431, 4433, 6477, 6480, 8533, 6487, 2395, 4446, 351, 2399, 6498, 4454, 4455, 360, 4456, 6505, 363, 6506, 2414, 369, 10609, 8568, 10618, 2435, 6532, 2440, 12681, 4491, 398, 10639, 2450, 4498, 8596, 10645, 4502, 6552, 409, 12700, 4509, 10654, 2467, 12713, 429, 2490, 8635, 12735, 4546, 10694, 2503, 8648, 12745, 461, 2509, 2511, 10703, 6609, 8659, 4563, 6614, 2521, 6619, 8676, 490, 6647, 12792, 6648, 10746, 507, 6653, 10750, 8703, 2565, 4613, 6662, 2570, 12811, 8717, 12814, 10771, 535, 10776, 540, 10781, 8739, 12836, 10793, 2602, 4649, 8750, 12846, 12848, 6703, 6705, 4661, 10807, 568, 6711, 8764, 2621, 574, 2623, 2624, 10817, 10818, 8771, 8772, 4679, 8779, 8780, 2637, 6733, 2639, 594, 8786, 6738, 6740, 10842, 4698, 6746, 8797, 10845, 6747, 6749, 2657, 2658, 10852, 613, 614, 8806, 615, 10855, 2664, 4714, 8812, 4717, 622, 12914, 10867, 12918, 632, 6777, 6780, 641, 644, 2693, 4741, 10887, 8840, 649, 12938, 10899, 4756, 8853, 662, 10903, 10906, 12954, 6811, 2719, 6818, 2723, 12963, 10919, 8872, 2730, 10925, 12973, 4785, 8886, 8890, 699, 700, 12991, 4800, 708, 8901, 12996, 8903, 2760, 4805, 4806, 4810, 8908, 4811, 719, 8911, 8915, 8921, 13022, 2784, 6882, 6883, 2795, 4844, 4850, 4851, 4854, 761, 764, 2812, 4863, 4864, 6911, 11014, 8967, 11017, 13067, 785, 2838, 791, 2844, 807, 2856, 809, 4912, 4914, 9012, 11060, 11062, 9016, 825, 4924, 11069, 9024, 9027, 838, 11083, 11084, 13131, 6990, 2899, 11093, 7005, 864, 13153, 866, 7012, 7013, 7014, 9063, 2920, 13164, 4972, 878, 11119, 880, 11124, 2934, 4987, 2941, 13181, 13183, 896, 9088, 4993, 7040, 7043, 901, 7045, 13192, 905, 11146, 5004, 5008, 914, 7058, 11156, 7059, 7060, 9111, 921, 13210, 9116, 5023, 7072, 2978, 9125, 7083, 940, 13230, 13231, 9136, 11184, 13234, 9140, 7093, 9143, 7096, 5052, 9149, 13246, 7101, 9152, 11200, 7110, 3016, 5065, 3018, 3021, 7117, 11216, 5072, 13267, 3028, 3029, 11221, 9175, 7123, 9179, 988, 9181, 9183, 11232, 5089, 994, 5091, 7138, 11239, 9193, 1003, 1004, 11243, 5099, 5100, 13293, 3057, 1010, 7147, 13302, 7160, 11261, 13309, 1023, 13311, 11266, 3074, 11270, 1031, 1032, 3080, 5126, 9227, 3084, 13320, 13330, 3093, 9238, 3095, 5148, 1053, 3102, 9252, 13350, 3112, 3113, 7209, 1067, 3115, 13357, 13360, 1079, 11319, 9273, 11322, 3131, 5178, 3134, 1089, 7233, 1092, 9288, 5194, 11340, 7244, 7245, 3157, 3159, 5208, 7255, 9306, 5215, 3169, 5218, 5224, 13416, 3180, 5229, 1135, 5231, 1137, 1138, 13424, 5235, 13431, 5245, 13440, 7296, 13444, 11400, 7307, 7310, 1170, 11410, 3221, 9370, 1191, 9385, 11436, 7341, 5294, 5299, 11447, 9400, 9401, 7353, 11452, 3261, 5311, 11456, 3264, 13506, 3268, 1223, 13513, 11467, 9420, 9422, 7375, 9424, 11473, 7378, 11483, 13535, 3299, 9444, 5349, 11494, 7398, 13547, 13548, 13549, 5365, 9462, 1271, 11512, 9465, 5370, 7417, 3331, 9477, 9478, 9479, 3336, 7437, 11535, 1297, 1298, 13586, 11541, 9498, 1309, 3357, 7453, 13600, 13602, 7459, 11556, 5412, 3366, 13614, 9523, 5428, 7475, 9529, 13627, 5438, 3391, 13630, 3393, 1348, 9543, 7499, 9548, 5452, 13648, 3409, 11602, 5457, 9559, 5463, 9562, 9565, 3421, 13662, 3424, 7519, 9573, 3429, 13669, 13670, 5482, 3438, 1391, 13678, 5488, 7535, 1395, 3443, 3445, 1399, 11639, 1402, 1403, 11642, 11646, 13696, 13700, 7557, 13702, 11656, 1419, 9612, 1420, 3468, 5516, 9617, 3473, 9619, 3475, 9622, 3482, 3484, 1438, 11682, 13731, 5547, 9648, 11696, 5554, 3515, 3518, 9666, 9667, 1490, 7635, 5593, 7642, 1502, 13790, 9700, 3556, 1510, 5608, 3561, 7657, 13803, 7658, 13805, 7662, 13807, 13813, 9718, 1528, 7673, 1530, 9723, 11770, 13821, 11778, 9731, 1540, 13831, 3594, 1550, 3603, 13843, 11799, 13849, 1562, 11802, 7706, 11806, 3619, 7715, 1573, 1574, 5669, 13863, 5673, 13870, 9775, 1587, 3636, 1591, 3639, 3640, 1595, 9790, 3647, 11840, 9793, 13890, 1606, 9798, 5702, 3657, 9803, 7763, 7766, 3671, 11876, 9829, 13926, 7782, 3688, 1642, 3692, 3693, 13933, 5743, 7788, 1650, 9843, 11892, 1653, 11894, 5750, 13946, 13950, 7809, 3716, 7814, 13963, 1678, 7826, 7831, 5785, 1690, 3738, 3742, 11935, 3744, 13983, 9894, 11943, 13991, 13993, 3758, 1711, 9905, 1715, 3764, 1717, 14003, 7861, 5816, 1721, 11962, 9916, 3773, 14014, 3777, 7873, 1733, 14024, 5832, 9934, 1742, 14030, 5840, 11986, 1747, 5841, 14034, 9942, 9943, 11991, 14040, 3806, 1759, 5858, 7907, 3814, 9959, 1776, 1778, 1780, 1784, 9977, 5882, 12027, 7928, 7931, 9983, 9989, 5895, 12044, 9998, 5902, 3857, 1813, 5911, 12057, 12058, 1820, 3868, 12061, 7965, 3874, 10022, 10026, 3883, 12076, 5933, 5938, 1849, 3903, 3905, 5955, 8007, 10056, 10059, 3915, 5965, 3918, 10063, 8012, 5969, 1874, 1879, 3930, 3931, 3944, 5993, 3946, 8042, 1904, 10096, 12144, 3953, 1913, 1914, 12154, 8057, 8059, 6017, 10121, 3977, 10126, 12177, 10132, 6040, 3993, 10138, 8092, 6059, 4013, 6062, 4015, 12208, 12209, 6063, 12211, 8114, 10168, 12216, 10171, 6075, 6079, 12229, 12232, 12233, 8138, 4046, 8151, 12250, 4060, 2013, 4061, 10210, 2020, 6120, 10217, 4074, 8170, 8171, 2033, 6129, 2036, 6132, 4087, 4089} conut: 932\n"
     ]
    }
   ],
   "source": [
    "# 在测试集中出现的用户但训练集没有出现\n",
    "out_train_in_test_user = set(dftest['User_id']) - set(dfoff['User_id'])\n",
    "print('1. User_id not in training set but in test set', out_train_in_test_user, 'conut:', len(out_train_in_test_user))\n",
    "# 在测试集中出现的商户但训练集没有出现\n",
    "out_train_in_test_merchant = set(dftest['Merchant_id']) - set(dfoff['Merchant_id'])\n",
    "print('2. Merchant_id not in training set but in test set', out_train_in_test_merchant, 'conut:', len(out_train_in_test_merchant))\n",
    "# 在测试集中出现的优惠券但训练集没有出现\n",
    "out_train_in_test_coupon = set(dftest['Coupon_id']) - set(dfoff['Coupon_id'])\n",
    "print('2. Coupon_id not in training set but in test set', out_train_in_test_coupon, 'conut:', len(out_train_in_test_coupon))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "说明大部分测试集中的商户和用户都是训练集中的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discount_rate 类型: [nan '150:20' '20:1' '200:20' '30:5' '50:10' '10:5' '100:10' '200:30'\n",
      " '20:5' '30:10' '50:5' '150:10' '100:30' '200:50' '100:50' '300:30'\n",
      " '50:20' '0.9' '10:1' '30:1' '0.95' '100:5' '5:1' '100:20' '0.8' '50:1'\n",
      " '200:10' '300:20' '100:1' '150:30' '300:50' '20:10' '0.85' '0.6' '150:50'\n",
      " '0.75' '0.5' '200:5' '0.7' '30:20' '300:10' '0.2' '50:30' '200:100'\n",
      " '150:5']\n",
      "Distance 类型: [ 0  1 11  2 10  4  7  9  3  5  6  8]\n",
      "Coupon_id 类型: [    0 11002  8591 ...  8580  9782  6757]\n"
     ]
    }
   ],
   "source": [
    "print('Discount_rate 类型:',dfoff['Discount_rate'].unique())\n",
    "print('Distance 类型:', dfoff['Distance'].unique())\n",
    "print('Coupon_id 类型:', dfoff['Coupon_id'].unique())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "优惠券描述有两种形式，满减与直接打折 - 不一样的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对优惠券描述列进行初步特征提取\n",
    "\n",
    "def get_offline_features(df, dfoff):\n",
    "    \n",
    "    # 辅助df 满足特定条件的子df\n",
    "    # 优惠券编号不为0\n",
    "    temp = dfoff[dfoff.Coupon_id != 0]\n",
    "    # 收到消费券且消费\n",
    "    coupon_consume = temp[temp.Date != date_null]\n",
    "    # 收到消费券但未消费\n",
    "    coupon_no_consume = temp[temp.Date == date_null]\n",
    "    # 收到消费券且消费的以用户为聚合列的聚合对象\n",
    "    user_coupon_consume = coupon_consume.groupby('User_id')\n",
    "    # 生成收到消费券星期列\n",
    "    df['weekday'] = df.Date_received.dt.weekday\n",
    "    # 生成收到消费券日列\n",
    "    df['day'] = df.Date_received.dt.day\n",
    "    \n",
    "    \n",
    "    '''offline coupon features'''\n",
    "    \n",
    "    # 辅助函数\n",
    "    def getDiscountType(row):\n",
    "        if pd.isnull(row):\n",
    "            return np.nan\n",
    "        elif ':' in str(row):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def convertRate(row):\n",
    "        if pd.isnull(row):\n",
    "            return 1.0\n",
    "        elif ':' in str(row):\n",
    "            rows = row.split(':')\n",
    "            return 1.0 - float(rows[1])/float(rows[0])\n",
    "        else:\n",
    "            return float(row)\n",
    "\n",
    "    def getDiscountMan(row):\n",
    "        if ':' in str(row):\n",
    "            rows = row.split(':')\n",
    "            return int(rows[0])\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def getDiscountJian(row):\n",
    "        if ':' in str(row):\n",
    "            rows = row.split(':')\n",
    "            return int(rows[1])\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    # 满减的满值\n",
    "    df['c1'] = df['Discount_rate'].apply(getDiscountMan)\n",
    "    \n",
    "    # 满减的减值\n",
    "    df['c2'] = df['Discount_rate'].apply(getDiscountJian)\n",
    "    \n",
    "    # 折扣率\n",
    "    df['c3'] = df['Discount_rate'].apply(convertRate)\n",
    "    \n",
    "    # 优惠类型 - 满减1 打折0\n",
    "    df['c4'] = 0\n",
    "    df.loc[df.Discount_rate.str.contains(':') == True, 'c4'] = 1\n",
    "    \n",
    "    # 优惠券发行次数\n",
    "    temp = dfoff[dfoff.Coupon_id != 0].groupby('Coupon_id').size().reset_index(name='c5') # 计数专用，只会生成计数列一列，不用count()\n",
    "    df = pd.merge(df, temp, how='left', on='Coupon_id')\n",
    "    \n",
    "    # 某优惠券一共被使用多少张\n",
    "    temp = coupon_consume.groupby('Coupon_id').size().reset_index(name='c6')\n",
    "    df = pd.merge(df, temp, how='left', on='Coupon_id')\n",
    "    \n",
    "    # 某优惠券使用率\n",
    "    df['c7'] = df.c6 / df.c5\n",
    "    \n",
    "    # 某优惠券没有使用的数目\n",
    "    df['c8'] = df.c5 - df.c6\n",
    "    \n",
    "    # 某优惠券在当天发行了多少张\n",
    "    temp = dfoff.groupby(['Coupon_id', 'Date_received']).size().reset_index(name='c9') # 某优惠券+某天 作为唯一键\n",
    "    df = pd.merge(df, temp, how='left', on=['Coupon_id', 'Date_received'])\n",
    "    \n",
    "    # 不同打折优惠券领取次数\n",
    "    temp = dfoff.groupby('Discount_rate').size().reset_index(name='c10')\n",
    "    df = pd.merge(df, temp, how='left', on='Discount_rate')\n",
    "\n",
    "    # 不同打折优惠券使用次数\n",
    "    temp = coupon_consume.groupby('Discount_rate').size().reset_index(name='c11')\n",
    "    df = pd.merge(df, temp, how='left', on='Discount_rate')\n",
    "\n",
    "    # 不同打折优惠券不使用次数\n",
    "    df['c12'] = df.c10 - df.c11\n",
    "    \n",
    "    # 不同打折优惠券使用率\n",
    "    df['c13'] = df.c11 / df.c10\n",
    "    \n",
    "    # 优惠券核销最早最晚天数差 & 优惠券核销平均时间？\n",
    "    temp = coupon_consume.groupby('Coupon_id').Date.max().reset_index(name='max') # 某优惠券最后核销时间\n",
    "    temp = pd.merge(coupon_consume, temp)\n",
    "    temp_1 = coupon_consume.groupby('Coupon_id').Date.min().reset_index(name='min') # 某优惠券最早核销时间\n",
    "    temp = pd.merge(temp, temp_1)\n",
    "    temp_2 = coupon_consume.groupby('Coupon_id').size().reset_index(name='count') # 某优惠券核销次数\n",
    "    temp = pd.merge(temp, temp_2)\n",
    "    temp['c14'] = (temp['max'] - temp['min']).dt.days\n",
    "    temp['c15'] = ((temp['max'] - temp['min']).dt.days / (temp['count'] - 1)) # 为什么减一？\n",
    "    temp = temp.drop_duplicates('Coupon_id')\n",
    "    df = pd.merge(df, temp[['Coupon_id', 'c14', 'c15']], how='left', on='Coupon_id')\n",
    "\n",
    "    \n",
    "    '''user features'''\n",
    "    \n",
    "    # 用户消费优惠券次数\n",
    "    temp = user_coupon_consume.size().reset_index(name='u1')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "    # X.u2.fillna(0, inplace=True)\n",
    "    # X.u2 = X.u2.astype(int)\n",
    "\n",
    "    # 用户领取优惠券不消费次数\n",
    "    temp = coupon_no_consume.groupby('User_id').size().reset_index(name='u2')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户领取优惠券并消费比领取未消费\n",
    "    df['u3'] = df.u1 / df.u2\n",
    "\n",
    "    # 用户领取优惠券次数\n",
    "    df['u4'] = df.u1.fillna(0) + df.u2.fillna(0)\n",
    "\n",
    "    # 用户的优惠券核销率\n",
    "    df['u4'] = df.u1 / df.u4\n",
    "\n",
    "    # 普通消费次数 - 没领取优惠券但消费\n",
    "    temp = dfoff[(dfoff.Coupon_id == 0) & (dfoff.Date != date_null)]\n",
    "    temp1 = temp.groupby('User_id').size().reset_index(name='u5')\n",
    "    df = pd.merge(df, temp1, how='left', on='User_id')\n",
    "\n",
    "    # 用户一共消费多少次\n",
    "    df['u6'] = df.u1 + df.u5\n",
    "\n",
    "    # 用户使用优惠券消费占比\n",
    "    df['u7'] = df.u1 / df.u6\n",
    "\n",
    "    # 用户正常消费最长间隔 & 平均间隔\n",
    "    temp = dfoff[dfoff.Date != date_null]\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').Date.max().reset_index(name='max'))\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').Date.min().reset_index(name='min'))\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').size().reset_index(name='count'))\n",
    "    temp['u8'] = (temp['max'] - temp['min']).dt.days\n",
    "    temp['u9'] = ((temp['max'] - temp['min']).dt.days / (temp['count'] - 1))\n",
    "    temp = temp.drop_duplicates('User_id')\n",
    "    df = pd.merge(df, temp[['User_id', 'u8', 'u9']], how='left', on='User_id')\n",
    "    \n",
    "    # 用户优惠券消费最长间隔 & 平均间隔\n",
    "    temp = dfoff[(dfoff.Coupon_id != 0) & (dfoff.Date != date_null)]\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').Date.max().reset_index(name='max'))\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').Date.min().reset_index(name='min'))\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').size().reset_index(name='count'))\n",
    "    temp['u10'] = (temp['max'] - temp['min']).dt.days\n",
    "    temp['u11'] = ((temp['max'] - temp['min']).dt.days / (temp['count'] - 1))\n",
    "    temp = temp.drop_duplicates('User_id')\n",
    "    df = pd.merge(df, temp[['User_id', 'u10', 'u11']], how='left', on='User_id')\n",
    "                   \n",
    "#     # 15天内平均会普通消费几次\n",
    "#     X['u8'] = X.u6 / 15\n",
    "\n",
    "#     # 15天内平均会优惠券消费几次\n",
    "#     X['u9'] = X.u7 / 15\n",
    "\n",
    "    # 用户领取优惠券到使用优惠券的 总&平均 间隔时间\n",
    "    temp = coupon_consume.copy()\n",
    "    temp['days'] = (temp.Date - temp.Date_received).dt.days\n",
    "    temp_1 = (temp.groupby('User_id').days.sum()).reset_index(name='u12')\n",
    "    temp_2 = (temp.groupby('User_id').days.sum() / temp.groupby('User_id').size()).reset_index(name='u13')\n",
    "    df = pd.merge(df, temp_1, how='left', on='User_id')\n",
    "    df = pd.merge(df, temp_2, how='left', on='User_id')\n",
    "\n",
    "#     # 在15天内使用掉优惠券的值大小\n",
    "#     X['u11'] = X.u10 / 15\n",
    "\n",
    "    # 用户领取优惠券到使用优惠券间隔小于15天的次数\n",
    "    temp = coupon_consume.copy()\n",
    "    temp['days'] = (temp.Date - temp.Date_received).dt.days\n",
    "    temp = temp[temp.days <= 15]\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='u14')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户领取后15天使用掉优惠券的次数除以总使用优惠券的次数\n",
    "    df['u15'] = df.u14 / df.u1\n",
    "\n",
    "    # 用户领取后15天使用掉优惠券的次数除以领取优惠券未消费的次数\n",
    "    df['u16'] = df.u14 / df.u2\n",
    "\n",
    "    # 用户15天使用掉优惠券的次数除以领取优惠券的总次数\n",
    "    df['u17'] = df.u14 / df.u4\n",
    "\n",
    "    # 用户消费优惠券的平均折率\n",
    "    temp_disr = dfoff.copy() # 生成一个带折扣率的temp df\n",
    "    temp_disr['discount_rate'] = temp_disr['Discount_rate'].apply(convertRate)\n",
    "    \n",
    "    temp = temp_disr[(dfoff.Date != date_null) & dfoff.Coupon_id != 0].groupby('User_id').discount_rate.mean().reset_index(name='u18')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销优惠券的最低消费折率\n",
    "    temp = temp_disr[(dfoff.Date != date_null) & dfoff.Coupon_id != 0].groupby('User_id').discount_rate.min().reset_index(name='u19')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销优惠券的最高消费折率\n",
    "    temp = temp_disr[(dfoff.Date != date_null) & dfoff.Coupon_id != 0].groupby('User_id').discount_rate.max().reset_index(name='u20')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销过的不同优惠券数量\n",
    "    temp = coupon_consume.groupby(['User_id', 'Coupon_id']).size()\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='u21')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户领取过该优惠券的次数\n",
    "    temp = dfoff[dfoff.Date_received != date_null]\n",
    "    temp = temp.groupby(['User_id', 'Coupon_id']).size().reset_index(name='u22')\n",
    "    df = pd.merge(df, temp, how='left', on=['User_id', 'Coupon_id'])\n",
    "                   \n",
    "    # 用户领取所有的不同优惠券数量\n",
    "    temp = dfoff[dfoff.Date_received != date_null]\n",
    "    temp = temp.groupby(['User_id', 'Coupon_id']).size()\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='u23')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销过的不同优惠券数量占所有不同优惠券的比重\n",
    "    df['u24'] = df.u21 / df.u23\n",
    "\n",
    "    # 用户平均每种优惠券核销多少张\n",
    "    df['u25'] = df.u1 / df.u21\n",
    "                   \n",
    "    # 用户每领多少种优惠券会核销一张\n",
    "    df['u26'] = df.u23 / df.u1\n",
    "                   \n",
    "    # 用户每领多少种未来核销优惠券会核销一张\n",
    "    df['u27'] = df.u21 / df.u1\n",
    "\n",
    "    # 核销优惠券用户-商家平均距离\n",
    "    temp = dfoff[(dfoff.Coupon_id != 0) & (dfoff.Date != date_null) & (dfoff.Distance != 11)] # 核销+有距离\n",
    "    temp = temp.groupby('User_id').Distance # 每个用户核销优惠券的商家距离 总和 / 计数 = 平均距离\n",
    "    temp = pd.merge(temp.size().reset_index(name='x'), temp.sum().reset_index(name='y'), on='User_id')\n",
    "    temp['u28'] = temp.y / temp.x\n",
    "    temp = temp[['User_id', 'u28']]\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销优惠券中的最小用户-商家距离\n",
    "    temp = dfoff[(dfoff.Coupon_id != 0) & (dfoff.Date != date_null) & (dfoff.Distance != 11)]\n",
    "    temp = temp.groupby('User_id').Distance.min().reset_index(name='u29')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销优惠券中的最大用户-商家距离\n",
    "    temp = dfoff[(dfoff.Coupon_id != 0) & (dfoff.Date != date_null) & (dfoff.Distance != 11)]\n",
    "    temp = temp.groupby('User_id').Distance.max().reset_index(name='u30')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "\n",
    "    # 优惠券类型 - 编号\n",
    "    discount_types = [\n",
    "        '0.2', '0.5', '0.6', '0.7', '0.75', '0.8', '0.85', '0.9', '0.95', '30:20', '50:30', '10:5',\n",
    "        '20:10', '100:50', '200:100', '50:20', '30:10', '150:50', '100:30', '20:5', '200:50', '5:1',\n",
    "        '50:10', '100:20', '150:30', '30:5', '300:50', '200:30', '150:20', '10:1', '50:5', '100:10',\n",
    "        '200:20', '300:30', '150:10', '300:20', '500:30', '20:1', '100:5', '200:10', '30:1', '150:5',\n",
    "        '300:10', '200:5', '50:1', '100:1',\n",
    "    ]\n",
    "    df['u31'] = -1\n",
    "    for k, v in enumerate(discount_types):\n",
    "        df.loc[df.Discount_rate == v, 'u31'] = k\n",
    "\n",
    "    # 不同优惠券领取次数\n",
    "    temp = dfoff.groupby(['User_id', 'Discount_rate']).size().reset_index(name='u32')\n",
    "    df = pd.merge(df, temp, how='left', on=['User_id', 'Discount_rate'])\n",
    "\n",
    "    # 不同优惠券使用次数\n",
    "    temp = coupon_consume.groupby(['User_id', 'Discount_rate']).size().reset_index(name='u33')\n",
    "    df = pd.merge(df, temp, how='left', on=['User_id', 'Discount_rate'])\n",
    "    \n",
    "\n",
    "    # 不同优惠券不使用次数\n",
    "#     temp = coupon_no_consume.groupby(['User_id', 'Discount_rate']).size().reset_index(name='u34')\n",
    "#     df = pd.merge(df, temp, how='left', on=['User_id', 'Discount_rate'])\n",
    "    df['u34'] = df.u32 - df.u33\n",
    "\n",
    "    # 不同打折优惠券使用率\n",
    "    df['u35'] = df.u33 / df.u32\n",
    "\n",
    "    # 满减类型优惠券领取次数\n",
    "    temp = dfoff[dfoff.Discount_rate.str.contains(':') == True]\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='u36')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "\n",
    "    # 打折类型优惠券领取次数\n",
    "    temp = dfoff[dfoff.Discount_rate.str.contains('\\.') == True]\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='u37')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "    \n",
    "    \n",
    "    '''offline merchant features'''\n",
    "\n",
    "    # 商户消费次数\n",
    "    temp = dfoff[dfoff.Date != date_null].groupby('Merchant_id').size().reset_index(name='m1')\n",
    "    df = pd.merge(df, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家优惠券被领取后核销次数\n",
    "    temp = coupon_consume.groupby('Merchant_id').size().reset_index(name='m2')\n",
    "    df = pd.merge(df, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商户普通消费笔数\n",
    "    df['m3'] = df.m1.fillna(0) - df.m2.fillna(0)\n",
    "\n",
    "    # 商家优惠券被领取次数\n",
    "    temp = dfoff[dfoff.Date_received != date_null].groupby('Merchant_id').size().reset_index(name='m4')\n",
    "    df = pd.merge(df, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家优惠券被领取后核销率\n",
    "    df['m5'] = df.m2 / df.m4\n",
    "\n",
    "    # 商家优惠券被领取后不核销次数\n",
    "    temp = coupon_no_consume.groupby('Merchant_id').size().reset_index(name='m6')\n",
    "    df = pd.merge(df, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商户当天优惠券领取次数\n",
    "    temp = df[df.Date_received != date_null]\n",
    "    temp = temp.groupby(['Merchant_id', 'Date_received']).size().reset_index(name='m7')\n",
    "    df = pd.merge(df, temp, how='left', on=['Merchant_id', 'Date_received'])\n",
    "\n",
    "    # 商户当天优惠券领取人数 - 商户+优惠券 唯一键 -> 计数不同user_id\n",
    "    temp = df[df.Date_received != date_null]\n",
    "    temp = temp.groupby(['User_id', 'Merchant_id', 'Date_received']).size().reset_index()\n",
    "    temp = temp.groupby(['Merchant_id', 'Date_received']).size().reset_index(name='m8')\n",
    "    df = pd.merge(df, temp, how='left', on=['Merchant_id', 'Date_received'])\n",
    "\n",
    "    # 商家优惠券核销的平均消费折率\n",
    "    temp = temp_disr[(temp_disr.Coupon_id != 0) & (temp_disr.Date != date_null)]  \n",
    "    temp = temp.groupby('Merchant_id').discount_rate.mean().reset_index(name='m9')\n",
    "    df = pd.merge(df, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家优惠券核销的最小消费折率\n",
    "    temp = temp_disr[(temp_disr.Coupon_id != 0) & (temp_disr.Date != date_null)]  \n",
    "    temp = temp.groupby('Merchant_id').discount_rate.min().reset_index(name='m10')\n",
    "    df = pd.merge(df, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家优惠券核销的最大消费折率\n",
    "    temp = temp_disr[(temp_disr.Coupon_id != 0) & (temp_disr.Date != date_null)]  \n",
    "    temp = temp.groupby('Merchant_id').discount_rate.max().reset_index(name='m11')\n",
    "    df = pd.merge(df, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家优惠券核销不同的用户数量\n",
    "    temp = coupon_consume.groupby(['Merchant_id', 'User_id']).size()\n",
    "    temp = temp.groupby('Merchant_id').size().reset_index(name='m12')\n",
    "    df = pd.merge(df, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家优惠券领取不同的用户数量\n",
    "    temp = dfoff[dfoff.Date_received != date_null].groupby(['Merchant_id', 'User_id']).size()\n",
    "    temp = temp.groupby('Merchant_id').size().reset_index(name='m13')\n",
    "    df = pd.merge(df, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 核销商家优惠券的不同用户数量其占领取不同的用户比重\n",
    "    df['m14'] = df.m12 / df.m13\n",
    "\n",
    "    # 商家优惠券平均每个用户核销多少张\n",
    "    df['m15'] = df.m2 / df.m13\n",
    "\n",
    "    # 商家被核销过的不同优惠券数量\n",
    "    temp = coupon_consume.groupby(['Merchant_id', 'Coupon_id']).size()\n",
    "    temp = temp.groupby('Merchant_id').size().reset_index(name='m16')\n",
    "    df = pd.merge(df, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家领取过的不同优惠券数量的种类数\n",
    "    temp = dfoff[dfoff.Date_received != date_null].groupby(['Merchant_id', 'Coupon_id']).size()\n",
    "    temp = temp.groupby('Merchant_id').size().reset_index(name='m17')\n",
    "    df = pd.merge(df, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家被核销过的不同优惠券数量占所有领取过的不同优惠券数量的比重\n",
    "    df['m18'] = df.m16 / df.m17\n",
    "    \n",
    "    # 商家被核销优惠券中的用户-商家最小距离\n",
    "    temp = coupon_consume[coupon_consume.Distance != 11]\n",
    "    temp = temp.groupby('Merchant_id').Distance.min().reset_index(name='m19')\n",
    "    df = pd.merge(df, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家被核销优惠券中的用户-商家最大距离\n",
    "    temp = coupon_consume[coupon_consume.Distance != 11]\n",
    "    temp = temp.groupby('Merchant_id').Distance.max().reset_index(name='m20')\n",
    "    df = pd.merge(df, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家被核销优惠券的平均时间\n",
    "    temp = pd.merge(coupon_consume, coupon_consume.groupby('Merchant_id').Date.max().reset_index(name='max')) # 对应每个商户匹配了核销日最晚值\n",
    "    temp = pd.merge(temp, temp.groupby('Merchant_id').Date.min().reset_index(name='min')) # 对应每个商户匹配了核销日最早值\n",
    "    temp = pd.merge(temp, temp.groupby('Merchant_id').size().reset_index(name='len')) # 对应每个商户匹配了核销次数\n",
    "    temp['m21'] = ((temp['max'] - temp['min']).dt.days / (temp['len'] - 1))\n",
    "    temp = temp.drop_duplicates('Merchant_id')\n",
    "    df = pd.merge(df, temp[['Merchant_id', 'm21']], how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家被核销优惠券中的用户-商家平均距离\n",
    "    temp = coupon_consume[coupon_consume.Distance != 11].groupby('Merchant_id').Distance\n",
    "    temp = pd.merge(temp.size().reset_index(name='x'), temp.sum().reset_index(name='y'), on='Merchant_id')\n",
    "    temp['m22'] = temp.y / temp.x\n",
    "    temp = temp[['Merchant_id', 'm22']]\n",
    "    df = pd.merge(df, temp, how='left', on='Merchant_id')\n",
    "    \n",
    "    \n",
    "    '''user merchant feature'''\n",
    "\n",
    "    # 用户领取商家的优惠券次数\n",
    "    temp = dfoff[dfoff.Coupon_id != 0]\n",
    "    temp = temp.groupby(['User_id', 'Merchant_id']).size().reset_index(name='um1')\n",
    "    df = pd.merge(df, temp, how='left', on=['User_id', 'Merchant_id'])\n",
    "\n",
    "    # 用户领取商家的优惠券后不核销次数\n",
    "    temp = coupon_no_consume.groupby(['User_id', 'Merchant_id']).size().reset_index(name='um2')\n",
    "    df = pd.merge(df, temp, how='left', on=['User_id', 'Merchant_id'])\n",
    "\n",
    "    # 用户领取商家的优惠券后核销次数\n",
    "    temp = coupon_consume.groupby(['User_id', 'Merchant_id']).size().reset_index(name='um3')\n",
    "    df = pd.merge(df, temp, how='left', on=['User_id', 'Merchant_id'])\n",
    "\n",
    "    # 用户领取商家的优惠券后核销率\n",
    "    df['um4'] = df.um3 / df.um1\n",
    "\n",
    "    # 用户对每个商家的不核销次数占用户总的不核销次数的比重\n",
    "    temp = coupon_no_consume.groupby('User_id').size().reset_index(name='temp')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "    df['um5'] = df.um2 / df.temp\n",
    "    df.drop(columns='temp', inplace=True)\n",
    "\n",
    "    # 用户在商店总共消费过几次\n",
    "    temp = dfoff[dfoff.Date != date_null].groupby(['User_id', 'Merchant_id']).size().reset_index(name='um6')\n",
    "    df = pd.merge(df, temp, how='left', on=['User_id', 'Merchant_id'])\n",
    "\n",
    "    # 用户在商店普通消费次数\n",
    "    temp = dfoff[(dfoff.Coupon_id == 0) & (dfoff.Date != date_null)]\n",
    "    temp = temp.groupby(['User_id', 'Merchant_id']).size().reset_index(name='um7')\n",
    "    df = pd.merge(df, temp, how='left', on=['User_id', 'Merchant_id'])\n",
    "\n",
    "    # 用户当天在此商店领取的优惠券数目\n",
    "    temp = dfoff[dfoff.Date_received != date_null]\n",
    "    temp = temp.groupby(['User_id', 'Merchant_id', 'Date_received']).size().reset_index(name='um8')\n",
    "    df = pd.merge(df, temp, how='left', on=['User_id', 'Merchant_id', 'Date_received'])\n",
    "    \n",
    "    # 用户有多少天在此商店领取过优惠券\n",
    "    temp = dfoff[dfoff.Date_received != date_null]\n",
    "    temp = temp.groupby(['User_id', 'Merchant_id', 'Date_received']).size()\n",
    "    temp = temp.groupby(['User_id', 'Merchant_id']).size().reset_index(name='um9')\n",
    "    df = pd.merge(df, temp, how='left', on=['User_id', 'Merchant_id'])\n",
    "\n",
    "    # 用户领取优惠券不同商家数量\n",
    "    temp = dfoff[dfoff.Coupon_id != 0]\n",
    "    temp = temp.groupby(['User_id', 'Merchant_id']).size().reset_index()\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='um10')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销优惠券不同商家数量\n",
    "    temp = coupon_consume.groupby(['User_id', 'Merchant_id']).size()\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='um11')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销过优惠券的不同商家数量占所有不同商家的比重\n",
    "    df['um12'] = df.um11 / df.um10\n",
    "\n",
    "    # 用户平均核销每个商家多少张优惠券\n",
    "    df['um13'] = df.u3 / df.um10\n",
    "\n",
    "    \n",
    "    '''other feature'''\n",
    "\n",
    "    # 用户领取的所有优惠券数目\n",
    "    temp = df.groupby('User_id').size().reset_index(name='o1')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户领取的特定优惠券数目\n",
    "    temp = df.groupby(['User_id', 'Coupon_id']).size().reset_index(name='o2')\n",
    "    df = pd.merge(df, temp, how='left', on=['User_id', 'Coupon_id'])\n",
    "\n",
    "#     # multiple threads\n",
    "#     # data split\n",
    "#     stop = len(X)\n",
    "#     step = int(ceil(stop / cpu_jobs))\n",
    "\n",
    "#     X_chunks = [X[i:i + step] for i in range(0, stop, step)]\n",
    "#     X_list = [X] * cpu_jobs\n",
    "#     counters = [i for i in range(cpu_jobs)]\n",
    "\n",
    "#     start = datetime.datetime.now()\n",
    "#     with ProcessPoolExecutor() as e:\n",
    "#         X = pd.concat(e.map(task, X_chunks, X_list, counters))\n",
    "#         print('time:', str(datetime.datetime.now() - start).split('.')[0])\n",
    "#     # multiple threads\n",
    "\n",
    "    # 用户领取优惠券平均时间间隔\n",
    "    temp = pd.merge(df, df.groupby('User_id').Date_received.max().reset_index(name='max'))\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').Date_received.min().reset_index(name='min'))\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').size().reset_index(name='len'))\n",
    "    temp['o3'] = ((temp['max'] - temp['min']).dt.days / (temp['len'] - 1))\n",
    "    temp = temp.drop_duplicates('User_id')\n",
    "    df = pd.merge(df, temp[['User_id', 'o3']], how='left', on='User_id')\n",
    "\n",
    "    # 用户领取特定商家的优惠券数目\n",
    "    temp = df.groupby(['User_id', 'Merchant_id']).size().reset_index(name='o4')\n",
    "    df = pd.merge(df, temp, how='left', on=['User_id', 'Merchant_id'])\n",
    "\n",
    "    # 用户领取的不同商家数目\n",
    "    temp = df.groupby(['User_id', 'Merchant_id']).size()\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='o5')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户当天领取的优惠券数目\n",
    "    temp = df.groupby(['User_id', 'Date_received']).size().reset_index(name='o6')\n",
    "    df = pd.merge(df, temp, how='left', on=['User_id', 'Date_received'])\n",
    "\n",
    "    # 用户当天领取的特定优惠券数目\n",
    "    temp = df.groupby(['User_id', 'Coupon_id', 'Date_received']).size().reset_index(name='o7')\n",
    "    df = pd.merge(df, temp, how='left', on=['User_id', 'Coupon_id', 'Date_received'])\n",
    "\n",
    "    # 用户领取的所有优惠券种类数目\n",
    "    temp = df.groupby(['User_id', 'Coupon_id']).size()\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='o8')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "\n",
    "    # 商家被领取的优惠券数目\n",
    "    temp = df.groupby('Merchant_id').size().reset_index(name='o9')\n",
    "    df = pd.merge(df, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家被领取的特定优惠券数目\n",
    "    temp = df.groupby(['Merchant_id', 'Coupon_id']).size().reset_index(name='o10')\n",
    "    df = pd.merge(df, temp, how='left', on=['Merchant_id', 'Coupon_id'])\n",
    "\n",
    "    # 商家被多少不同用户领取的数目\n",
    "    temp = df.groupby(['Merchant_id', 'User_id']).size()\n",
    "    temp = temp.groupby('Merchant_id').size().reset_index(name='o11')\n",
    "    df = pd.merge(df, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家发行的所有优惠券种类数目\n",
    "    temp = df.groupby(['Merchant_id', 'Coupon_id']).size()\n",
    "    temp = temp.groupby('Merchant_id').size().reset_index(name='o12')\n",
    "    df = pd.merge(df, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    print(len(df), len(df.columns))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df, predict=False):\n",
    "    columns = ['User_id', 'Merchant_id', 'Discount_rate', 'Date_received','discount_rate_x', 'discount_rate_y']\n",
    "\n",
    "    if predict:\n",
    "        columns.append('Coupon_id')\n",
    "        pass\n",
    "    else:\n",
    "        columns.append('Date')\n",
    "#         columns.append('Coupon_id')\n",
    "\n",
    "    df.drop(columns=columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocess_data(predict=False):\n",
    "    if predict:\n",
    "        dfoff = pd.read_csv('C:/Users/jxjsj/Desktop/tianchi/o2ocoupon/ccf_offline_stage1_test_revised.csv',\n",
    "                              parse_dates=['Date_received'])\n",
    "    else:\n",
    "        dfoff = pd.read_csv('C:/Users/jxjsj/Desktop/tianchi/o2ocoupon/ccf_offline_stage1_train.csv', \n",
    "                              parse_dates=['Date_received', 'Date'])\n",
    "\n",
    "    dfoff.Distance.fillna(11, inplace=True)\n",
    "    dfoff.Distance = dfoff.Distance.astype(int)\n",
    "    dfoff.Coupon_id.fillna(0, inplace=True)\n",
    "    dfoff.Coupon_id = dfoff.Coupon_id.astype(int)\n",
    "    dfoff.Date_received.fillna(date_null, inplace=True)\n",
    "\n",
    "    dfoff[['discount_rate_x', 'discount_rate_y']] = dfoff[dfoff.Discount_rate.str.contains(':') == True][\n",
    "        'Discount_rate'].str.split(':', expand=True).astype(int)\n",
    "    dfoff['discount_rate'] = 1 - dfoff.discount_rate_y / dfoff.discount_rate_x\n",
    "    dfoff.discount_rate = dfoff.discount_rate.fillna(dfoff.Discount_rate).astype(float)\n",
    "\n",
    "    if predict:\n",
    "        return dfoff\n",
    "\n",
    "    dfoff.Date.fillna(date_null, inplace=True)\n",
    "\n",
    "#     # online\n",
    "#     online = pd.read_csv('ccf_online_stage1_train.csv', parse_dates=['Date_received', 'Date'])\n",
    "\n",
    "#     online.Coupon_id.fillna(0, inplace=True)\n",
    "#     # online.Coupon_id = online.Coupon_id.astype(int)\n",
    "#     online.Date_received.fillna(date_null, inplace=True)\n",
    "#     online.Date.fillna(date_null, inplace=True)\n",
    "\n",
    "    return dfoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data():\n",
    "    path = 'C:/Users/jxjsj/Desktop/tianchi/o2ocoupon/o2o_train.csv'\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        data = pd.read_csv(path)\n",
    "    else:\n",
    "        dfoff = get_preprocess_data()\n",
    "\n",
    "        # date received 2016-01-01 - 2016-06-15\n",
    "        # date consumed 2016-01-01 - 2016-06-30\n",
    "\n",
    "        # train data 1\n",
    "        # 2016-04-16 ~ 2016-05-15\n",
    "        data_1 = dfoff[('2016-04-16' <= dfoff.Date_received) & (dfoff.Date_received <= '2016-05-15')].copy()\n",
    "        data_1['label'] = 0\n",
    "        data_1.loc[\n",
    "            (data_1.Date != date_null) & (data_1.Date - data_1.Date_received <= datetime.timedelta(15)), 'label'] = 1\n",
    "\n",
    "        # feature data 1\n",
    "        # 领券 2016-01-01 ~ 2016-03-31\n",
    "        end = '2016-03-31'\n",
    "        data_off_1 = dfoff[dfoff.Date_received <= end]\n",
    "#         data_on_1 = online[online.Date_received <= end]\n",
    "\n",
    "        # 普通消费 2016-01-01 ~ 2016-04-15\n",
    "        end = '2016-04-15'\n",
    "        data_off_2 = dfoff[(dfoff.Coupon_id == 0) & (dfoff.Date <= end)]\n",
    "#         data_on_2 = online[(online.Coupon_id == 0) & (online.Date <= end)]\n",
    "\n",
    "        data_1 = get_offline_features(data_1, pd.concat([data_off_1, data_off_2]))\n",
    "#         data_1 = get_online_features(pd.concat([data_on_1, data_on_2]), data_1)\n",
    "\n",
    "        # train data 2\n",
    "        # 2016-05-16 ~ 2016-06-15\n",
    "        data_2 = dfoff[('2016-05-16' <= dfoff.Date_received) & (dfoff.Date_received <= '2016-06-15')].copy()\n",
    "        data_2['label'] = 0\n",
    "        data_2.loc[\n",
    "            (data_2.Date != date_null) & (data_2.Date - data_2.Date_received <= datetime.timedelta(15)), 'label'] = 1\n",
    "\n",
    "        # feature data 2\n",
    "        # 领券\n",
    "        start = '2016-02-01'\n",
    "        end = '2016-04-30'\n",
    "        data_off_1 = dfoff[(start <= dfoff.Date_received) & (dfoff.Date_received <= end)]\n",
    "#         data_on_1 = online[(start <= online.Date_received) & (online.Date_received <= end)]\n",
    "\n",
    "        # 普通消费\n",
    "        start = '2016-02-01'\n",
    "        end = '2016-05-15'\n",
    "        data_off_2 = dfoff[(dfoff.Coupon_id == 0) & (start <= dfoff.Date) & (dfoff.Date <= end)]\n",
    "#         data_on_2 = online[(online.Coupon_id == 0) & (start <= online.Date) & (online.Date <= end)]\n",
    "\n",
    "        data_2 = get_offline_features(data_2, pd.concat([data_off_1, data_off_2]))\n",
    "#         data_2 = get_online_features(pd.concat([data_on_1, data_on_2]), data_2)\n",
    "\n",
    "        # train data 3\n",
    "        # 2016-06-01 ~ 2016-06-30\n",
    "        data_3 = dfoff['2016-06-01' <= dfoff.Date_received].copy()\n",
    "        data_3['label'] = 0\n",
    "        data_3.loc[\n",
    "            (data_3.Date != date_null) & (data_3.Date - data_3.Date_received <= datetime.timedelta(15)), 'label'] = 1\n",
    "\n",
    "        # feature data 3\n",
    "        # 领券\n",
    "        start = '2016-01-15'\n",
    "        end = '2016-04-15'\n",
    "        data_off_1 = dfoff[(start <= dfoff.Date_received) & (dfoff.Date_received <= end)]\n",
    "#         data_on_1 = online[(start <= online.Date_received) & (online.Date_received <= end)]\n",
    "\n",
    "        # 普通消费\n",
    "        start = '2016-01-15'\n",
    "        end = '2016-05-30'\n",
    "        data_off_2 = dfoff[(dfoff.Coupon_id == 0) & (start <= dfoff.Date) & (dfoff.Date <= end)]\n",
    "#         data_on_2 = online[(online.Coupon_id == 0) & (start <= online.Date) & (online.Date <= end)]\n",
    "\n",
    "        data_3 = get_offline_features(data_3, pd.concat([data_off_1, data_off_2]))\n",
    "#         data_2 = get_online_features(pd.concat([data_on_1, data_on_2]), data_2)\n",
    "        \n",
    "    \n",
    "        data = pd.concat([data_1, data_2, data_3])\n",
    "\n",
    "        # undersampling\n",
    "        # if undersampling:\n",
    "        #     temp = X_1[X_1.label == 1].groupby('User_id').size().reset_index()\n",
    "        #     temp = X_1[X_1.User_id.isin(temp.User_id)]\n",
    "        #     X_1 = pd.concat([temp, X_1[~X_1.User_id.isin(temp.User_id)].sample(4041)])\n",
    "\n",
    "        # data.drop_duplicates(inplace=True)\n",
    "        drop_columns(data)\n",
    "        data.fillna(0, inplace=True)\n",
    "        data.to_csv(path, index=False)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_eval_metric(estimator, X, y, name=None):\n",
    "    if name is None:\n",
    "        name = estimator.__class__.__name__\n",
    "\n",
    "    if name is 'XGBClassifier' or name is 'LGBMClassifier':\n",
    "        estimator.fit(X, y, eval_metric='auc')\n",
    "    else:\n",
    "        estimator.fit(X, y)\n",
    "\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(clf,scale=False):\n",
    "#     global log\n",
    "\n",
    "    data = get_train_data()\n",
    "    \n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    train_data, test_data = train_test_split(data,\n",
    "                                             train_size=350000,\n",
    "                                             random_state=0\n",
    "                                             )\n",
    "\n",
    "#     _, test_data = train_test_split(data, random_state=0)\n",
    "\n",
    "    X_train = train_data.copy().drop(columns='Coupon_id')\n",
    "    if scale:\n",
    "        X_train = min_max_scaler.fit_transform(X_train)\n",
    "    y_train = X_train.pop('label')\n",
    "    \n",
    "#     print(X_train)\n",
    "#     print(y_train)\n",
    "\n",
    "    clf = fit_eval_metric(clf, X_train, y_train)\n",
    "\n",
    "    X_test = test_data.copy().drop(columns='Coupon_id')\n",
    "    if scale:\n",
    "        X_test = min_max_scaler.fit_transform(X_test)\n",
    "    y_test = X_test.pop('label')\n",
    "\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    # log += '%s\\n' % classification_report(y_test, y_pred)\n",
    "#     log += '  accuracy: %f\\n' % accuracy_score(y_true, y_pred)\n",
    "    print('valid accuracy: %f\\n' % accuracy_score(y_true, y_pred))\n",
    "    y_score = clf.predict_proba(X_test)[:, 1]\n",
    "#     log += '       auc: %f\\n' % roc_auc_score(y_true, y_score)\n",
    "    print('valid auc: %f\\n' % roc_auc_score(y_true, y_score))\n",
    "\n",
    "    # coupon average auc\n",
    "    coupons = test_data.groupby('Coupon_id').size().reset_index(name='total')\n",
    "    aucs = []\n",
    "    for _, coupon in coupons.iterrows():\n",
    "        if coupon.total > 1:\n",
    "            X_test = test_data[test_data.Coupon_id == coupon.Coupon_id].copy()\n",
    "            X_test.drop(columns='Coupon_id', inplace=True)\n",
    "\n",
    "            if len(X_test.label.unique()) != 2:\n",
    "                continue\n",
    "\n",
    "            y_true = X_test.pop('label')\n",
    "            y_score = clf.predict_proba(X_test)[:, 1]\n",
    "            aucs.append(roc_auc_score(y_true, y_score))\n",
    "\n",
    "#     log += 'coupon auc: %f\\n\\n' % np.mean(aucs)\n",
    "    print('coupon auc: %f\\n\\n' % np.mean(aucs))\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_rf = RandomForestClassifier(n_estimators=500, \n",
    "                                        max_depth=8,\n",
    "                                        criterion='gini',\n",
    "                                        max_features=0.8,\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        oob_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf = train(clf_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_bst = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(class_weight = \"balanced\",criterion='entropy',max_features=0.8), \n",
    "                                    n_estimators=30, \n",
    "                                    learning_rate=1,\n",
    "                                    algorithm='SAMME',\n",
    "                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_bst = train(clf_bst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf_gbt = GradientBoostingClassifier(loss='deviance', \n",
    "                           learning_rate=0.1, \n",
    "                           n_estimators=500, \n",
    "                           subsample=1.0, \n",
    "                           criterion='friedman_mse', \n",
    "                           min_samples_split=2, \n",
    "                           min_samples_leaf=1, \n",
    "                           min_weight_fraction_leaf=0.0, \n",
    "                           max_depth=8, \n",
    "                           min_impurity_decrease=0.0, \n",
    "                           min_impurity_split=None, \n",
    "                           init=None, \n",
    "                           random_state=None, \n",
    "                           max_features=0.8, \n",
    "                           verbose=0, \n",
    "                           max_leaf_nodes=None, \n",
    "                           warm_start=False, \n",
    "                           presort='auto', \n",
    "                           validation_fraction=0.1, \n",
    "                           n_iter_no_change=None, \n",
    "                           tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-174-ce26f755fba8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf_gbt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf_gbt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-159-37b4acc0de87>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(clf)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#     print(y_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_eval_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Coupon_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-138-6dee9cd22455>\u001b[0m in \u001b[0;36mfit_eval_metric\u001b[1;34m(estimator, X, y, name)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m   1463\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, self._rng,\n\u001b[0;32m   1464\u001b[0m                                     \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1465\u001b[1;33m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[0;32m   1466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1467\u001b[0m         \u001b[1;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, y_pred, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1527\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[0;32m   1528\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1529\u001b[1;33m                                      X_csc, X_csr)\n\u001b[0m\u001b[0;32m   1530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1531\u001b[0m             \u001b[1;31m# track deviance (= loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m   1192\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[1;32m-> 1194\u001b[1;33m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m             \u001b[1;31m# update tree leaves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1140\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1142\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1143\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    364\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf_gbt = train(clf_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.799745\n",
      "\n",
      "valid auc: 0.877007\n",
      "\n",
      "coupon auc: 0.732848\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_gbt = train(clf_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if __name__ == '__main__':\n",
    "    start = datetime.datetime.now()\n",
    "    print(start.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    log = '%s\\n' % start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    cpu_jobs = os.cpu_count() - 1\n",
    "    date_null = pd.to_datetime('1970-01-01', format='%Y-%m-%d')\n",
    "\n",
    "    # analysis()\n",
    "    # detect_duplicate_columns()\n",
    "    # feature_importance_score()\n",
    "\n",
    "    #grid_search_gbdt()\n",
    "    # train_gbdt()\n",
    "    # predict('gbdt')\n",
    "\n",
    "    # grid_search_xgb()\n",
    "    # train_xgb()\n",
    "    # predict('xgb')\n",
    "\n",
    "    # grid_search_lgb()\n",
    "    # train_lgb()\n",
    "    # predict('lgb')\n",
    "\n",
    "    # grid_search_cat()\n",
    "    # train_cat()\n",
    "    # predict('cat')\n",
    "\n",
    "    # grid_search_rf()\n",
    "    # train_rf_gini()\n",
    "    # predict('rf_gini')\n",
    "\n",
    "    # grid_search_rf('entropy')\n",
    "    # train_rf_entropy()\n",
    "    # predict('rf_entropy')\n",
    "\n",
    "    # grid_search_et()\n",
    "    # train_et_gini()\n",
    "    # predict('et_gini')\n",
    "\n",
    "    # grid_search_et('entropy')\n",
    "    # train_et_entropy()\n",
    "    # predict('et_entropy')\n",
    "\n",
    "    # blending()\n",
    "    # predict('blending')\n",
    "\n",
    "    log += 'time: %s\\n' % str((datetime.datetime.now() - start)).split('.')[0]\n",
    "    log += '----------------------------------------------------\\n'\n",
    "    open('%s.log' % os.path.basename(__file__), 'a').write(log)\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def detect_duplicate_columns():\n",
    "#     X = get_train_data()\n",
    "#     X = X[:1000]\n",
    "\n",
    "#     for index1 in range(len(X.columns) - 1):\n",
    "#         for index2 in range(index1 + 1, len(X.columns)):\n",
    "#             column1 = X.columns[index1]\n",
    "#             column2 = X.columns[index2]\n",
    "#             X[column1] = X[column1].astype(str)\n",
    "#             X[column2] = X[column2].astype(str)\n",
    "#             temp = len(X[X[column1] == X[column2]])\n",
    "#             if temp == len(X):\n",
    "#                 print(column1, column2, temp)\n",
    "#     exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_test_data():\n",
    "#     path = 'C:/Users/jxjsj/Desktop/tianchi/o2ocoupon/o2o_test.csv'\n",
    "\n",
    "#     if os.path.exists(path):\n",
    "#         X = pd.read_csv(path, parse_dates=['Date_received'])\n",
    "    \n",
    "#     else:\n",
    "#         dfoff = get_preprocess_data() # 先读取训练数据 构造右矩阵\n",
    "\n",
    "#         # 2016-03-16 ~ 2016-06-30\n",
    "#         start = '2016-03-16'\n",
    "#         dfoff = dfoff[(dfoff.Coupon_id == 0) & (start <= dfoff.Date) | (start <= dfoff.Date_received)] # 普通消费 or 领券\n",
    "# #         online = online[(online.Coupon_id == 0) & (start <= online.Date) | (start <= online.Date_received)]\n",
    "\n",
    "#         X = get_preprocess_data(True)\n",
    "#         X = get_offline_features(X, dfoff)\n",
    "# #         X = get_online_features(online, X)\n",
    "#         X.drop_duplicates(inplace=True)\n",
    "#         X.fillna(0, inplace=True)\n",
    "#         X.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,scale=False):\n",
    "    \n",
    "    path = 'C:/Users/jxjsj/Desktop/tianchi/o2ocoupon/o2o_test.csv'\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        X = pd.read_csv(path, parse_dates=['Date_received'])\n",
    "    \n",
    "    else:\n",
    "        dfoff = get_preprocess_data() # 先读取训练数据 构造右矩阵\n",
    "\n",
    "        # 2016-03-16 ~ 2016-06-30\n",
    "        start = '2016-03-16'\n",
    "        dfoff = dfoff[(dfoff.Coupon_id == 0) & (start <= dfoff.Date) | (start <= dfoff.Date_received)] # 普通消费 or 领券\n",
    "#         online = online[(online.Coupon_id == 0) & (start <= online.Date) | (start <= online.Date_received)]\n",
    "\n",
    "        X = get_preprocess_data(True)\n",
    "        X = get_offline_features(X, dfoff)\n",
    "#         X = get_online_features(online, X)\n",
    "        X.drop_duplicates(inplace=True)\n",
    "        X.fillna(0, inplace=True)\n",
    "        X.to_csv(path, index=False)\n",
    "\n",
    "#     path = 'C:/Users/jxjsj/Desktop/tianchi/o2ocoupon/o2o_test.csv'\n",
    "#     X = pd.read_csv(path, parse_dates=['Date_received'])\n",
    "    \n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    \n",
    "    sample_submission = X[['User_id', 'Coupon_id', 'Date_received']].copy()\n",
    "    sample_submission.Date_received = sample_submission.Date_received.dt.strftime('%Y%m%d')\n",
    "    drop_columns(X, True)\n",
    "    \n",
    "    if scale:\n",
    "        X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "    if model is 'blending':\n",
    "        predict = blending(X)\n",
    "    else:\n",
    "#         clf = eval('train_%s' % model)()\n",
    "        predict = clf.predict_proba(X)[:, 1]\n",
    "\n",
    "    sample_submission['Probability'] = predict\n",
    "    sample_submission.to_csv('C:/Users/jxjsj/Desktop/tianchi/o2ocoupon/o2o_submission.csv',index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到未删键的特征df\n",
    "df_F_off_train = feature_off(df_F_off)\n",
    "df_F_test_train = feature_off(df_F_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征的列名list\n",
    "feature_lst = ['c'+str(i) for i in range(1,16)]+['u'+str(i) for i in range(1,28)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成 训练集 与 测试集 特征矩阵\n",
    "df_F_off_temp = df_F_off_train[feature_lst]\n",
    "df_F_off_X = df_F_off_temp.fillna(0)\n",
    "\n",
    "df_F_test_temp = df_F_test_train[feature_lst]\n",
    "df_F_test_X = df_F_test_temp.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成标签\n",
    "y = df_F_off_train.copy()\n",
    "\n",
    "y['label'] = 0\n",
    "y.loc[(y.Date != date_null) & (y.Date - y.Date_received <= datetime.timedelta(15)), 'label'] = 1\n",
    "\n",
    "labels = y['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(df_F_off_X, labels, random_state=1, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:463: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='entropy', max_depth=5, max_features=0.8,\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=25, n_jobs=None, oob_score=True,\n",
       "            random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=25, \n",
    "                                        max_depth=5,\n",
    "                                        criterion='entropy',\n",
    "                                        max_features=0.8,\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        oob_score=True)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainAccracy: 0.9473661716908599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97   1352581\n",
      "           1       0.41      1.00      0.58     51326\n",
      "\n",
      "   micro avg       0.95      0.95      0.95   1403907\n",
      "   macro avg       0.70      0.97      0.78   1403907\n",
      "weighted avg       0.98      0.95      0.96   1403907\n",
      "\n",
      "testAccracy: 0.9480136875065888\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97    337908\n",
      "           1       0.42      1.00      0.59     13069\n",
      "\n",
      "   micro avg       0.95      0.95      0.95    350977\n",
      "   macro avg       0.71      0.97      0.78    350977\n",
      "weighted avg       0.98      0.95      0.96    350977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 预测以及结果评价\n",
    "\n",
    "y_train_predict = model.predict(x_train)\n",
    "y_valid_predict = model.predict(x_valid)\n",
    "\n",
    "print('trainAccracy:',model.score(x_train,y_train))\n",
    "print(classification_report(y_train,y_train_predict)) #真实数据在前 训练结果在后！\n",
    "print('testAccracy:',model.score(x_valid,y_valid))\n",
    "print(classification_report(y_valid,y_valid_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc: 0.9917851631389628\n"
     ]
    }
   ],
   "source": [
    "y_valid_predict_prob = model.predict_proba(x_valid)#基于SVM对验证集做出预测，prodict_prob_y 为预测的概率\n",
    "valid_auc = metrics.roc_auc_score(y_valid,y_valid_predict_prob[:,1])#验证集上的auc值\n",
    "print('valid_auc:',valid_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coupon average auc\n",
    "coupons = test_data.groupby('Coupon_id').size().reset_index(name='total')\n",
    "aucs = []\n",
    "for _, coupon in coupons.iterrows():\n",
    "    if coupon.total > 1:\n",
    "        X_test = test_data[test_data.Coupon_id == coupon.Coupon_id].copy()\n",
    "        X_test.drop(columns='Coupon_id', inplace=True)\n",
    "\n",
    "        if len(X_test.label.unique()) != 2:\n",
    "            continue\n",
    "\n",
    "        y_true = X_test.pop('label')\n",
    "        y_score = clf.predict_proba(X_test)[:, 1]\n",
    "        aucs.append(roc_auc_score(y_true, y_score))\n",
    "\n",
    "log += 'coupon auc: %f\\n\\n' % np.mean(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Coupon_id</th>\n",
       "      <th>Date_received</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4129537</td>\n",
       "      <td>9983</td>\n",
       "      <td>20160712</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6949378</td>\n",
       "      <td>3429</td>\n",
       "      <td>20160706</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2166529</td>\n",
       "      <td>6928</td>\n",
       "      <td>20160727</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2166529</td>\n",
       "      <td>1808</td>\n",
       "      <td>20160727</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6172162</td>\n",
       "      <td>6500</td>\n",
       "      <td>20160708</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_id  Coupon_id  Date_received  label\n",
       "0  4129537       9983       20160712    0.0\n",
       "1  6949378       3429       20160706    0.0\n",
       "2  2166529       6928       20160727    0.0\n",
       "3  2166529       1808       20160727    0.0\n",
       "4  6172162       6500       20160708    0.0"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tol = model\n",
    "# model_tol.fit(x_tol,y_tol)\n",
    "\n",
    "y_test_pred = model_tol.predict_proba(df_F_test_X)\n",
    "\n",
    "dftest_for_save = pd.read_csv('C:/Users/jxjsj/Desktop/tianchi/o2ocoupon/ccf_offline_stage1_test_revised.csv')\n",
    "dftest1 = dftest_for_save[['User_id','Coupon_id','Date_received']].copy()\n",
    "dftest1['label'] = y_test_pred[:,1]\n",
    "dftest1.to_csv('C:/Users/jxjsj/Desktop/tianchi/o2ocoupon/submission.csv', index=False, header=False)\n",
    "dftest1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool is ok.\n"
     ]
    }
   ],
   "source": [
    "# 1. 将满xx减yy类型(`xx:yy`)的券变成折扣率 : `1 - yy/xx`，同时建立折扣券相关的特征 `discount_rate, discount_man, discount_jian, discount_type`\n",
    "# 2. 将距离 `str` 转为 `int`\n",
    "# convert Discount_rate and Distance\n",
    "def getDiscountType(row):\n",
    "    if pd.isnull(row):\n",
    "        return np.nan\n",
    "    elif ':' in row:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def convertRate(row):\n",
    "    \"\"\"Convert discount to rate\"\"\"\n",
    "    if pd.isnull(row):\n",
    "        return 1.0\n",
    "    elif ':' in str(row):\n",
    "        rows = row.split(':')\n",
    "        return 1.0 - float(rows[1])/float(rows[0])\n",
    "    else:\n",
    "        return float(row)\n",
    "\n",
    "def getDiscountMan(row):\n",
    "    if ':' in str(row):\n",
    "        rows = row.split(':')\n",
    "        return int(rows[0])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def getDiscountJian(row):\n",
    "    if ':' in str(row):\n",
    "        rows = row.split(':')\n",
    "        return int(rows[1])\n",
    "    else:\n",
    "        return 0\n",
    "print(\"tool is ok.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(df):\n",
    "    # convert discunt_rate\n",
    "    df['discount_rate'] = df['Discount_rate'].apply(convertRate)\n",
    "    df['discount_man'] = df['Discount_rate'].apply(getDiscountMan)\n",
    "    df['discount_jian'] = df['Discount_rate'].apply(getDiscountJian)\n",
    "    df['discount_type'] = df['Discount_rate'].apply(getDiscountType)\n",
    "    print(df['discount_rate'].unique())\n",
    "    # convert distance\n",
    "    df['distance'] = df['Distance'].fillna(-1).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.86666667 0.95       0.9        0.83333333 0.8\n",
      " 0.5        0.85       0.75       0.66666667 0.93333333 0.7\n",
      " 0.6        0.96666667 0.98       0.99       0.975      0.33333333\n",
      " 0.2        0.4       ]\n",
      "[0.83333333 0.9        0.96666667 0.8        0.95       0.75\n",
      " 0.98       0.5        0.86666667 0.6        0.66666667 0.7\n",
      " 0.85       0.33333333 0.94       0.93333333 0.975      0.99      ]\n"
     ]
    }
   ],
   "source": [
    "# 特征生成\n",
    "dfoff = processData(dfoff)\n",
    "dftest = processData(dftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 收到优惠券的日子 去重 排序\n",
    "date_received = dfoff['Date_received'].unique()\n",
    "date_received = sorted(date_received[pd.notnull(date_received)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 购买商品的日子 不去重 排序\n",
    "date_buy = dfoff['Date'].unique()\n",
    "date_buy = sorted(date_buy[pd.notnull(date_buy)])\n",
    "date_buy = sorted(dfoff[dfoff['Date'].notnull()]['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对同一天收到的优惠券聚合，计数\n",
    "couponbydate = dfoff[dfoff['Date_received'].notnull()\n",
    "                    ][['Date_received', 'Date']].groupby(['Date_received'], as_index=False).count()\n",
    "couponbydate.columns = ['Date_received','count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 对同一天收到的优惠券，且未来被花掉了聚合，计数\n",
    "buybydate_temp = dfoff[(dfoff['Date'].notnull())]\n",
    "buybydate = buybydate_temp[(buybydate_temp['Date_received'].notnull())][['Date_received', 'Date']].groupby(['Date_received'], as_index=False).count()\n",
    "buybydate.columns = ['Date_received','count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date对象的方法weekday()返回0-6表示周一到周日\n",
    "def getWeekday(row):\n",
    "    if row == 'nan': # NaN被str()才可被 ==‘nan’识别\n",
    "        return np.nan\n",
    "    else:\n",
    "        return date(int(row[0:4]), int(row[4:6]), int(row[6:8])).weekday() + 1\n",
    "\n",
    "dfoff['weekday'] = dfoff['Date_received'].astype(str).apply(getWeekday)\n",
    "dftest['weekday'] = dftest['Date_received'].astype(str).apply(getWeekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekday_type :  周六和周日为1，其他为0\n",
    "dfoff['weekday_type'] = dfoff['weekday'].apply(lambda x : 1 if x in [6,7] else 0 )\n",
    "dftest['weekday_type'] = dftest['weekday'].apply(lambda x : 1 if x in [6,7] else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change weekday to one-hot encoding \n",
    "weekdaycols = ['weekday_' + str(i) for i in range(1,8)]\n",
    "# tmpdf = pd.get_dummies(dfoff['weekday'].replace('nan', np.nan))\n",
    "tmpdf = pd.get_dummies(dfoff['weekday'])\n",
    "tmpdf.columns = weekdaycols\n",
    "dfoff[weekdaycols] = tmpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdf = pd.get_dummies(dftest['weekday'])\n",
    "tmpdf.columns = weekdaycols\n",
    "dftest[weekdaycols] = tmpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(row):\n",
    "    if pd.isnull(row['Date_received']):\n",
    "        return -1\n",
    "    if pd.notnull(row['Date']):\n",
    "        td = pd.to_datetime(row['Date'], format='%Y%m%d') -  pd.to_datetime(row['Date_received'], format='%Y%m%d')\n",
    "        if td <= pd.Timedelta(15, 'D'):\n",
    "            return 1\n",
    "    return 0\n",
    "dfoff['label'] = dfoff.apply(label, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----data split------\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "# feature\n",
    "original_feature_label = ['discount_rate','discount_type','discount_man', 'discount_jian','distance', 'weekday', 'weekday_type','label'] + weekdaycols\n",
    "original_feature = ['discount_rate','discount_type','discount_man', 'discount_jian','distance', 'weekday', 'weekday_type'] + weekdaycols\n",
    "\n",
    "# data split - train2train&valid\n",
    "print(\"-----data split------\")\n",
    "df = dfoff[dfoff['label'] != -1].copy() # 把需要研究的对象0与1挑选出来\n",
    "train = df[(df['Date_received'] < 20160516)].copy()[original_feature_label]\n",
    "valid = df[(df['Date_received'] >= 20160516) & (df['Date_received'] <= 20160615)].copy()[original_feature_label]\n",
    "\n",
    "x_train = train[original_feature]\n",
    "y_train = train['label']\n",
    "x_valid = valid[original_feature]\n",
    "y_valid = valid['label']\n",
    "\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tol sample\n",
    "x_tol = pd.concat([x_train,x_valid],axis=0)\n",
    "y_tol = pd.concat([y_train,y_valid],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-d7cce8e7a1b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m                    )\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# model.fit(x_train,y_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mover_samples_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mover_samples_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    975\u001b[0m         \"\"\"\n\u001b[0;32m    976\u001b[0m         return self._fit(X, y, incremental=(self.warm_start and\n\u001b[1;32m--> 977\u001b[1;33m                                             hasattr(self, \"classes_\")))\n\u001b[0m\u001b[0;32m    978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m             self._fit_lbfgs(X, y, activations, deltas, coef_grads,\n\u001b[1;32m--> 376\u001b[1;33m                             intercept_grads, layer_units)\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit_lbfgs\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units)\u001b[0m\n\u001b[0;32m    464\u001b[0m             \u001b[0miprint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miprint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m             \u001b[0mpgtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m             args=(X, y, activations, deltas, coef_grads, intercept_grads))\n\u001b[0m\u001b[0;32m    467\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimal_parameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[1;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[1;32m--> 199\u001b[1;33m                            **opts)\n\u001b[0m\u001b[0;32m    200\u001b[0m     d = {'grad': res['jac'],\n\u001b[0;32m    201\u001b[0m          \u001b[1;34m'task'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'message'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[0;32m    333\u001b[0m             \u001b[1;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m             \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[1;34m(*wrapper_args)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_loss_grad_lbfgs\u001b[1;34m(self, packed_coef_inter, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked_coef_inter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         loss, coef_grads, intercept_grads = self._backprop(\n\u001b[1;32m--> 175\u001b[1;33m             X, y, activations, deltas, coef_grads, intercept_grads)\n\u001b[0m\u001b[0;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_pack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoef_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_backprop\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mloss_func_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'log_loss'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_activation_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m             \u001b[0mloss_func_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'binary_log_loss'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLOSS_FUNCTIONS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss_func_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[1;31m# Add L2 regularization term to loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         values = np.sum(\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_base.py\u001b[0m in \u001b[0;36mbinary_log_loss\u001b[1;34m(y_true, y_prob)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[0my_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1e-10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m     return -np.sum(y_true * np.log(y_prob) +\n\u001b[0m\u001b[0;32m    248\u001b[0m                    (1 - y_true) * np.log(1 - y_prob)) / y_prob.shape[0]\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MLPClassifier(activation='tanh', \n",
    "                    solver='lbfgs',\n",
    "                    alpha=1e-5,\n",
    "                    hidden_layer_sizes=(len(x_train.columns),),\n",
    "                    random_state=1, \n",
    "                   )\n",
    "# model.fit(x_train,y_train)\n",
    "model.fit(over_samples_x, over_samples_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.1, average=False, class_weight='balanced',\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.01, learning_rate='optimal', loss='log', max_iter=100,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=1, penalty='elasticnet',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SGDClassifier(\n",
    "    loss='log',\n",
    "    penalty='elasticnet',\n",
    "    fit_intercept=True,\n",
    "    max_iter=100,\n",
    "    shuffle=True,\n",
    "    alpha = 0.1,\n",
    "    l1_ratio = 0.01,\n",
    "    n_jobs=1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(class_weight='balanced',solver = 'lbfgs')\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "model = GaussianNB()\n",
    "# model.fit(x_train, y_train)\n",
    "model.fit(over_samples_x, over_samples_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:463: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='entropy', max_depth=5, max_features=0.8,\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=25, n_jobs=None, oob_score=True,\n",
       "            random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=25, \n",
    "                                        max_depth=5,\n",
    "                                        criterion='entropy',\n",
    "                                        max_features=0.8,\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        oob_score=True)\n",
    "model.fit(x_train, y_train)\n",
    "# model.fit(over_samples_x, over_samples_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运用SMOTE改进的平衡样本\n",
    "over_samples = SMOTE(random_state=11)\n",
    "over_samples_x, over_samples_y = over_samples.fit_sample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "model = svm.SVC(C=2, kernel='linear', decision_function_shape='ovr',class_weight = \"balanced\")\n",
    "model.fit(x_train, y_train)\n",
    "# model.fit(over_samples_x, over_samples_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainAccracy: 0.6559018653771219\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.65      0.78    759172\n",
      "           1       0.12      0.85      0.20     41524\n",
      "\n",
      "   micro avg       0.66      0.66      0.66    800696\n",
      "   macro avg       0.55      0.75      0.49    800696\n",
      "weighted avg       0.94      0.66      0.75    800696\n",
      "\n",
      "testAccracy: 0.35944589169629354\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.31      0.47    229715\n",
      "           1       0.11      0.88      0.20     22871\n",
      "\n",
      "   micro avg       0.36      0.36      0.36    252586\n",
      "   macro avg       0.54      0.59      0.33    252586\n",
      "weighted avg       0.88      0.36      0.44    252586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #### 预测以及结果评价\n",
    "# print(model.score(valid[original_feature], valid['label']))\n",
    "\n",
    "y_train_predict = model.predict(x_train)\n",
    "y_valid_predict = model.predict(x_valid)\n",
    "\n",
    "print('trainAccracy:',model.score(x_train,y_train))\n",
    "print(classification_report(y_train,y_train_predict)) #真实数据在前 训练结果在后！\n",
    "print('testAccracy:',model.score(x_valid,y_valid))\n",
    "print(classification_report(y_valid,y_valid_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc: 0.5921657961226938\n"
     ]
    }
   ],
   "source": [
    "y_valid_predict_prob = model.predict_proba(x_valid)#基于SVM对验证集做出预测，prodict_prob_y 为预测的概率\n",
    "test_auc = metrics.roc_auc_score(y_valid,list(y_valid_predict))#验证集上的auc值\n",
    "print('test_auc:',test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(y_valid_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---save model---\")\n",
    "with open('1_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "with open('1_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Coupon_id</th>\n",
       "      <th>Date_received</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4129537</td>\n",
       "      <td>9983</td>\n",
       "      <td>20160712</td>\n",
       "      <td>9.233964e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6949378</td>\n",
       "      <td>3429</td>\n",
       "      <td>20160706</td>\n",
       "      <td>1.647855e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2166529</td>\n",
       "      <td>6928</td>\n",
       "      <td>20160727</td>\n",
       "      <td>1.843014e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2166529</td>\n",
       "      <td>1808</td>\n",
       "      <td>20160727</td>\n",
       "      <td>1.896743e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6172162</td>\n",
       "      <td>6500</td>\n",
       "      <td>20160708</td>\n",
       "      <td>5.266069e-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_id  Coupon_id  Date_received         label\n",
       "0  4129537       9983       20160712  9.233964e-07\n",
       "1  6949378       3429       20160706  1.647855e-06\n",
       "2  2166529       6928       20160727  1.843014e-07\n",
       "3  2166529       1808       20160727  1.896743e-07\n",
       "4  6172162       6500       20160708  5.266069e-12"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test prediction for submission\n",
    "y_test_pred = model.predict_proba(dftest[original_feature])\n",
    "dftest1 = dftest[['User_id','Coupon_id','Date_received']].copy()\n",
    "dftest1['label'] = y_test_pred[:,1]\n",
    "dftest1.to_csv('C:/Users/jxjsj/Desktop/tianchi/submission.csv', index=False, header=False)\n",
    "dftest1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:463: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Coupon_id</th>\n",
       "      <th>Date_received</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4129537</td>\n",
       "      <td>9983</td>\n",
       "      <td>20160712</td>\n",
       "      <td>0.467298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6949378</td>\n",
       "      <td>3429</td>\n",
       "      <td>20160706</td>\n",
       "      <td>0.613431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2166529</td>\n",
       "      <td>6928</td>\n",
       "      <td>20160727</td>\n",
       "      <td>0.110725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2166529</td>\n",
       "      <td>1808</td>\n",
       "      <td>20160727</td>\n",
       "      <td>0.130222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6172162</td>\n",
       "      <td>6500</td>\n",
       "      <td>20160708</td>\n",
       "      <td>0.506071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_id  Coupon_id  Date_received     label\n",
       "0  4129537       9983       20160712  0.467298\n",
       "1  6949378       3429       20160706  0.613431\n",
       "2  2166529       6928       20160727  0.110725\n",
       "3  2166529       1808       20160727  0.130222\n",
       "4  6172162       6500       20160708  0.506071"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test prediction for submission - tol sample\n",
    "# model_tol = MLPClassifier(activation='tanh', \n",
    "#                     solver='lbfgs',\n",
    "#                     alpha=1e-5,\n",
    "#                     hidden_layer_sizes=(len(x_tol.columns)*2+1,),\n",
    "#                     random_state=1, \n",
    "#                    )\n",
    "model_tol = model\n",
    "model_tol.fit(x_tol,y_tol)\n",
    "\n",
    "y_test_pred = model_tol.predict_proba(dftest[original_feature])\n",
    "dftest1 = dftest[['User_id','Coupon_id','Date_received']].copy()\n",
    "dftest1['label'] = y_test_pred[:,1]\n",
    "dftest1.to_csv('C:/Users/jxjsj/Desktop/tianchi/submission.csv', index=False, header=False)\n",
    "dftest1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
