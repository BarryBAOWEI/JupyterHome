{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "import os, sys, pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfon = pd.read_csv('C:/Users/jxjsj/Desktop/tianchi/o2ocoupon/ccf_online_stage1_train.csv')\n",
    "dfoff = pd.read_csv('C:/Users/jxjsj/Desktop/tianchi/o2ocoupon/ccf_offline_stage1_train.csv')\n",
    "dftest = pd.read_csv('C:/Users/jxjsj/Desktop/tianchi/o2ocoupon/ccf_offline_stage1_test_revised.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('1970-01-01 00:00:00')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成一个无效日期\n",
    "date_null = pd.to_datetime('1970-01-01', format='%Y-%m-%d')\n",
    "date_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 收到优惠券日期预处理 - 处理为日期格式\n",
    "def dateprocess(row):\n",
    "    if pd.isnull(row):\n",
    "        return date_null\n",
    "    else:\n",
    "        return pd.to_datetime(str(int(row)), format='%Y-%m-%d')\n",
    "\n",
    "dfoff.Date_received = dfoff.Date_received.apply(dateprocess)\n",
    "dfoff.Date = dfoff.Date.apply(dateprocess)\n",
    "dftest.Date_received = dftest.Date_received.apply(dateprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfoff.Distance.fillna(11, inplace=True)\n",
    "dfoff.Distance = dfoff.Distance.astype(int)\n",
    "dfoff.Coupon_id.fillna(0, inplace=True)\n",
    "dfoff.Coupon_id = dfoff.Coupon_id.astype(int)\n",
    "# dfoff.Date_received.fillna(date_null, inplace=True)\n",
    "# dfoff.Date.fillna(date_null, inplace=True)\n",
    "\n",
    "\n",
    "dftest.Distance.fillna(11, inplace=True)\n",
    "dftest.Distance = dftest.Distance.astype(int)\n",
    "dftest.Coupon_id.fillna(0, inplace=True)\n",
    "dftest.Coupon_id = dftest.Coupon_id.astype(int)\n",
    "# dftest.Date_received.fillna(date_null, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复制一个df 用来保存生成的特征，不要影响到原始df \n",
    "df_F_off = dfoff.copy()\n",
    "df_F_test = dftest.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   User_id  Merchant_id  Coupon_id Discount_rate  Distance Date_received  \\\n",
      "0  1439408         2632          0           NaN         0    1970-01-01   \n",
      "1  1439408         4663      11002        150:20         1    2016-05-28   \n",
      "2  1439408         2632       8591          20:1         0    2016-02-17   \n",
      "3  1439408         2632       1078          20:1         0    2016-03-19   \n",
      "4  1439408         2632       8591          20:1         0    2016-06-13   \n",
      "\n",
      "                  Date  \n",
      "0          2.01602e+07  \n",
      "1  1970-01-01 00:00:00  \n",
      "2  1970-01-01 00:00:00  \n",
      "3  1970-01-01 00:00:00  \n",
      "4  1970-01-01 00:00:00  \n",
      "   User_id  Merchant_id  Coupon_id Discount_rate  Distance Date_received\n",
      "0  4129537          450       9983          30:5         1    2016-07-12\n",
      "1  6949378         1300       3429          30:5        11    2016-07-06\n",
      "2  2166529         7113       6928        200:20         5    2016-07-27\n",
      "3  2166529         7113       1808        100:10         5    2016-07-27\n",
      "4  6172162         7605       6500          30:1         2    2016-07-08\n"
     ]
    }
   ],
   "source": [
    "print(dfoff.head(5))\n",
    "print(dftest.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1754884 entries, 0 to 1754883\n",
      "Data columns (total 7 columns):\n",
      "User_id          int64\n",
      "Merchant_id      int64\n",
      "Coupon_id        int32\n",
      "Discount_rate    object\n",
      "Distance         int32\n",
      "Date_received    datetime64[ns]\n",
      "Date             object\n",
      "dtypes: datetime64[ns](1), int32(2), int64(2), object(2)\n",
      "memory usage: 80.3+ MB\n"
     ]
    }
   ],
   "source": [
    "dfoff.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有优惠券，购买商品条数，label 1 75382\n",
      "无优惠券，购买商品条数 701602\n",
      "有优惠券，不购买商品条数，label 0 977900\n",
      "无优惠券，不购买商品条数 0\n"
     ]
    }
   ],
   "source": [
    "print('有优惠券，购买商品条数，label 1', dfoff[(dfoff['Date_received'] != date_null) & (dfoff['Date'] != date_null)].shape[0])\n",
    "print('无优惠券，购买商品条数', dfoff[(dfoff['Date_received'] == date_null) & (dfoff['Date'] != date_null)].shape[0])\n",
    "print('有优惠券，不购买商品条数，label 0', dfoff[(dfoff['Date_received'] != date_null) & (dfoff['Date'] == date_null)].shape[0])\n",
    "print('无优惠券，不购买商品条数', dfoff[(dfoff['Date_received'] == date_null) & (dfoff['Date'] == date_null)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. User_id not in training set but in test set {2495873, 1286474} conut: 2\n",
      "2. Merchant_id not in training set but in test set {5920} conut: 1\n",
      "2. Coupon_id not in training set but in test set {6146, 12291, 7, 8, 4104, 14, 10254, 4111, 8209, 6161, 20, 6166, 12311, 10264, 2072, 6167, 31, 8224, 12321, 34, 12323, 6176, 2089, 10285, 2095, 2096, 8242, 6199, 4153, 2106, 10299, 10303, 10306, 4174, 85, 6233, 10331, 8285, 2142, 8287, 8289, 2147, 10342, 105, 2158, 113, 8306, 6258, 4217, 2178, 12428, 6285, 2192, 4241, 8338, 4251, 6299, 2206, 4256, 2209, 4257, 12455, 170, 172, 2224, 12465, 10418, 180, 4277, 6326, 8375, 4279, 10425, 6329, 10431, 4289, 4290, 10437, 10438, 8391, 6344, 2251, 8404, 10453, 12509, 8415, 8424, 232, 8426, 12522, 236, 6377, 2286, 12529, 4339, 2292, 6390, 10489, 6393, 251, 253, 10493, 259, 8453, 2309, 4357, 10505, 4361, 4362, 12559, 8464, 4369, 10521, 8479, 4383, 8482, 294, 297, 8490, 6445, 8499, 10547, 2357, 4403, 10551, 4405, 8506, 10554, 10558, 4415, 320, 6465, 327, 10567, 10568, 8523, 12619, 2381, 4427, 10575, 4431, 4433, 6477, 6480, 8533, 6487, 2395, 4446, 351, 2399, 6498, 4454, 4455, 360, 4456, 6505, 363, 6506, 2414, 369, 10609, 8568, 10618, 2435, 6532, 2440, 12681, 4491, 398, 10639, 2450, 4498, 8596, 10645, 4502, 6552, 409, 12700, 4509, 10654, 2467, 12713, 429, 2490, 8635, 12735, 4546, 10694, 2503, 8648, 12745, 461, 2509, 2511, 10703, 6609, 8659, 4563, 6614, 2521, 6619, 8676, 490, 6647, 12792, 6648, 10746, 507, 6653, 10750, 8703, 2565, 4613, 6662, 2570, 12811, 8717, 12814, 10771, 535, 10776, 540, 10781, 8739, 12836, 10793, 2602, 4649, 8750, 12846, 12848, 6703, 6705, 4661, 10807, 568, 6711, 8764, 2621, 574, 2623, 2624, 10817, 10818, 8771, 8772, 4679, 8779, 8780, 2637, 6733, 2639, 594, 8786, 6738, 6740, 10842, 4698, 6746, 8797, 10845, 6747, 6749, 2657, 2658, 10852, 613, 614, 8806, 615, 10855, 2664, 4714, 8812, 4717, 622, 12914, 10867, 12918, 632, 6777, 6780, 641, 644, 2693, 4741, 10887, 8840, 649, 12938, 10899, 4756, 8853, 662, 10903, 10906, 12954, 6811, 2719, 6818, 2723, 12963, 10919, 8872, 2730, 10925, 12973, 4785, 8886, 8890, 699, 700, 12991, 4800, 708, 8901, 12996, 8903, 2760, 4805, 4806, 4810, 8908, 4811, 719, 8911, 8915, 8921, 13022, 2784, 6882, 6883, 2795, 4844, 4850, 4851, 4854, 761, 764, 2812, 4863, 4864, 6911, 11014, 8967, 11017, 13067, 785, 2838, 791, 2844, 807, 2856, 809, 4912, 4914, 9012, 11060, 11062, 9016, 825, 4924, 11069, 9024, 9027, 838, 11083, 11084, 13131, 6990, 2899, 11093, 7005, 864, 13153, 866, 7012, 7013, 7014, 9063, 2920, 13164, 4972, 878, 11119, 880, 11124, 2934, 4987, 2941, 13181, 13183, 896, 9088, 4993, 7040, 7043, 901, 7045, 13192, 905, 11146, 5004, 5008, 914, 7058, 11156, 7059, 7060, 9111, 921, 13210, 9116, 5023, 7072, 2978, 9125, 7083, 940, 13230, 13231, 9136, 11184, 13234, 9140, 7093, 9143, 7096, 5052, 9149, 13246, 7101, 9152, 11200, 7110, 3016, 5065, 3018, 3021, 7117, 11216, 5072, 13267, 3028, 3029, 11221, 9175, 7123, 9179, 988, 9181, 9183, 11232, 5089, 994, 5091, 7138, 11239, 9193, 1003, 1004, 11243, 5099, 5100, 13293, 3057, 1010, 7147, 13302, 7160, 11261, 13309, 1023, 13311, 11266, 3074, 11270, 1031, 1032, 3080, 5126, 9227, 3084, 13320, 13330, 3093, 9238, 3095, 5148, 1053, 3102, 9252, 13350, 3112, 3113, 7209, 1067, 3115, 13357, 13360, 1079, 11319, 9273, 11322, 3131, 5178, 3134, 1089, 7233, 1092, 9288, 5194, 11340, 7244, 7245, 3157, 3159, 5208, 7255, 9306, 5215, 3169, 5218, 5224, 13416, 3180, 5229, 1135, 5231, 1137, 1138, 13424, 5235, 13431, 5245, 13440, 7296, 13444, 11400, 7307, 7310, 1170, 11410, 3221, 9370, 1191, 9385, 11436, 7341, 5294, 5299, 11447, 9400, 9401, 7353, 11452, 3261, 5311, 11456, 3264, 13506, 3268, 1223, 13513, 11467, 9420, 9422, 7375, 9424, 11473, 7378, 11483, 13535, 3299, 9444, 5349, 11494, 7398, 13547, 13548, 13549, 5365, 9462, 1271, 11512, 9465, 5370, 7417, 3331, 9477, 9478, 9479, 3336, 7437, 11535, 1297, 1298, 13586, 11541, 9498, 1309, 3357, 7453, 13600, 13602, 7459, 11556, 5412, 3366, 13614, 9523, 5428, 7475, 9529, 13627, 5438, 3391, 13630, 3393, 1348, 9543, 7499, 9548, 5452, 13648, 3409, 11602, 5457, 9559, 5463, 9562, 9565, 3421, 13662, 3424, 7519, 9573, 3429, 13669, 13670, 5482, 3438, 1391, 13678, 5488, 7535, 1395, 3443, 3445, 1399, 11639, 1402, 1403, 11642, 11646, 13696, 13700, 7557, 13702, 11656, 1419, 9612, 1420, 3468, 5516, 9617, 3473, 9619, 3475, 9622, 3482, 3484, 1438, 11682, 13731, 5547, 9648, 11696, 5554, 3515, 3518, 9666, 9667, 1490, 7635, 5593, 7642, 1502, 13790, 9700, 3556, 1510, 5608, 3561, 7657, 13803, 7658, 13805, 7662, 13807, 13813, 9718, 1528, 7673, 1530, 9723, 11770, 13821, 11778, 9731, 1540, 13831, 3594, 1550, 3603, 13843, 11799, 13849, 1562, 11802, 7706, 11806, 3619, 7715, 1573, 1574, 5669, 13863, 5673, 13870, 9775, 1587, 3636, 1591, 3639, 3640, 1595, 9790, 3647, 11840, 9793, 13890, 1606, 9798, 5702, 3657, 9803, 7763, 7766, 3671, 11876, 9829, 13926, 7782, 3688, 1642, 3692, 3693, 13933, 5743, 7788, 1650, 9843, 11892, 1653, 11894, 5750, 13946, 13950, 7809, 3716, 7814, 13963, 1678, 7826, 7831, 5785, 1690, 3738, 3742, 11935, 3744, 13983, 9894, 11943, 13991, 13993, 3758, 1711, 9905, 1715, 3764, 1717, 14003, 7861, 5816, 1721, 11962, 9916, 3773, 14014, 3777, 7873, 1733, 14024, 5832, 9934, 1742, 14030, 5840, 11986, 1747, 5841, 14034, 9942, 9943, 11991, 14040, 3806, 1759, 5858, 7907, 3814, 9959, 1776, 1778, 1780, 1784, 9977, 5882, 12027, 7928, 7931, 9983, 9989, 5895, 12044, 9998, 5902, 3857, 1813, 5911, 12057, 12058, 1820, 3868, 12061, 7965, 3874, 10022, 10026, 3883, 12076, 5933, 5938, 1849, 3903, 3905, 5955, 8007, 10056, 10059, 3915, 5965, 3918, 10063, 8012, 5969, 1874, 1879, 3930, 3931, 3944, 5993, 3946, 8042, 1904, 10096, 12144, 3953, 1913, 1914, 12154, 8057, 8059, 6017, 10121, 3977, 10126, 12177, 10132, 6040, 3993, 10138, 8092, 6059, 4013, 6062, 4015, 12208, 12209, 6063, 12211, 8114, 10168, 12216, 10171, 6075, 6079, 12229, 12232, 12233, 8138, 4046, 8151, 12250, 4060, 2013, 4061, 10210, 2020, 6120, 10217, 4074, 8170, 8171, 2033, 6129, 2036, 6132, 4087, 4089} conut: 932\n"
     ]
    }
   ],
   "source": [
    "# 在测试集中出现的用户但训练集没有出现\n",
    "out_train_in_test_user = set(dftest['User_id']) - set(dfoff['User_id'])\n",
    "print('1. User_id not in training set but in test set', out_train_in_test_user, 'conut:', len(out_train_in_test_user))\n",
    "# 在测试集中出现的商户但训练集没有出现\n",
    "out_train_in_test_merchant = set(dftest['Merchant_id']) - set(dfoff['Merchant_id'])\n",
    "print('2. Merchant_id not in training set but in test set', out_train_in_test_merchant, 'conut:', len(out_train_in_test_merchant))\n",
    "# 在测试集中出现的优惠券但训练集没有出现\n",
    "out_train_in_test_coupon = set(dftest['Coupon_id']) - set(dfoff['Coupon_id'])\n",
    "print('2. Coupon_id not in training set but in test set', out_train_in_test_coupon, 'conut:', len(out_train_in_test_coupon))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "说明大部分测试集中的商户和用户都是训练集中的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discount_rate 类型: [nan '150:20' '20:1' '200:20' '30:5' '50:10' '10:5' '100:10' '200:30'\n",
      " '20:5' '30:10' '50:5' '150:10' '100:30' '200:50' '100:50' '300:30'\n",
      " '50:20' '0.9' '10:1' '30:1' '0.95' '100:5' '5:1' '100:20' '0.8' '50:1'\n",
      " '200:10' '300:20' '100:1' '150:30' '300:50' '20:10' '0.85' '0.6' '150:50'\n",
      " '0.75' '0.5' '200:5' '0.7' '30:20' '300:10' '0.2' '50:30' '200:100'\n",
      " '150:5']\n",
      "Distance 类型: [ 0  1 11  2 10  4  7  9  3  5  6  8]\n",
      "Coupon_id 类型: [    0 11002  8591 ...  8580  9782  6757]\n"
     ]
    }
   ],
   "source": [
    "print('Discount_rate 类型:',dfoff['Discount_rate'].unique())\n",
    "print('Distance 类型:', dfoff['Distance'].unique())\n",
    "print('Coupon_id 类型:', dfoff['Coupon_id'].unique())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "优惠券描述有两种形式，满减与直接打折 - 不一样的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Merchant_id</th>\n",
       "      <th>Coupon_id</th>\n",
       "      <th>Discount_rate</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Date_received</th>\n",
       "      <th>Date</th>\n",
       "      <th>weekday</th>\n",
       "      <th>day</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1439408</td>\n",
       "      <td>2632</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2016-02-17</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1439408</td>\n",
       "      <td>4663</td>\n",
       "      <td>11002</td>\n",
       "      <td>150:20</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1439408</td>\n",
       "      <td>2632</td>\n",
       "      <td>8591</td>\n",
       "      <td>20:1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-02-17</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1439408</td>\n",
       "      <td>2632</td>\n",
       "      <td>1078</td>\n",
       "      <td>20:1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1439408</td>\n",
       "      <td>2632</td>\n",
       "      <td>8591</td>\n",
       "      <td>20:1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-13</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1439408</td>\n",
       "      <td>2632</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2016-05-16</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1439408</td>\n",
       "      <td>2632</td>\n",
       "      <td>8591</td>\n",
       "      <td>20:1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-16</td>\n",
       "      <td>2016-06-13</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1832624</td>\n",
       "      <td>3381</td>\n",
       "      <td>7610</td>\n",
       "      <td>200:20</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-04-29</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2029232</td>\n",
       "      <td>3381</td>\n",
       "      <td>11951</td>\n",
       "      <td>200:20</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-29</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2029232</td>\n",
       "      <td>450</td>\n",
       "      <td>1532</td>\n",
       "      <td>30:5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-30</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2029232</td>\n",
       "      <td>6459</td>\n",
       "      <td>12737</td>\n",
       "      <td>20:1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-19</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2029232</td>\n",
       "      <td>6459</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2016-06-26</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2029232</td>\n",
       "      <td>6459</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2016-05-19</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2747744</td>\n",
       "      <td>6901</td>\n",
       "      <td>1097</td>\n",
       "      <td>50:10</td>\n",
       "      <td>11</td>\n",
       "      <td>2016-06-06</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>196342</td>\n",
       "      <td>1579</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2016-06-06</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>196342</td>\n",
       "      <td>1579</td>\n",
       "      <td>10698</td>\n",
       "      <td>20:1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-06</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2223968</td>\n",
       "      <td>3381</td>\n",
       "      <td>9776</td>\n",
       "      <td>10:5</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-29</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>73611</td>\n",
       "      <td>2099</td>\n",
       "      <td>12034</td>\n",
       "      <td>100:10</td>\n",
       "      <td>11</td>\n",
       "      <td>2016-02-07</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>163606</td>\n",
       "      <td>1569</td>\n",
       "      <td>5054</td>\n",
       "      <td>200:30</td>\n",
       "      <td>10</td>\n",
       "      <td>2016-04-21</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>200</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3273056</td>\n",
       "      <td>4833</td>\n",
       "      <td>7802</td>\n",
       "      <td>200:20</td>\n",
       "      <td>10</td>\n",
       "      <td>2016-01-30</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>94107</td>\n",
       "      <td>3381</td>\n",
       "      <td>7610</td>\n",
       "      <td>200:20</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-12</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>253750</td>\n",
       "      <td>8390</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>253750</td>\n",
       "      <td>6901</td>\n",
       "      <td>2366</td>\n",
       "      <td>30:5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-18</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>253750</td>\n",
       "      <td>8390</td>\n",
       "      <td>7531</td>\n",
       "      <td>20:5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>343660</td>\n",
       "      <td>4663</td>\n",
       "      <td>11002</td>\n",
       "      <td>150:20</td>\n",
       "      <td>11</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>376492</td>\n",
       "      <td>1041</td>\n",
       "      <td>13490</td>\n",
       "      <td>30:5</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-27</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1964720</td>\n",
       "      <td>7884</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2016-01-15</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1964720</td>\n",
       "      <td>7884</td>\n",
       "      <td>6704</td>\n",
       "      <td>20:1</td>\n",
       "      <td>10</td>\n",
       "      <td>2016-02-15</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4191584</td>\n",
       "      <td>3051</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2016-05-19</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4191584</td>\n",
       "      <td>3051</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2016-05-16</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754854</th>\n",
       "      <td>179830</td>\n",
       "      <td>6284</td>\n",
       "      <td>4567</td>\n",
       "      <td>50:10</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-24</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754855</th>\n",
       "      <td>179830</td>\n",
       "      <td>6284</td>\n",
       "      <td>7379</td>\n",
       "      <td>50:10</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-24</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754856</th>\n",
       "      <td>179830</td>\n",
       "      <td>3710</td>\n",
       "      <td>13056</td>\n",
       "      <td>150:10</td>\n",
       "      <td>4</td>\n",
       "      <td>2016-01-24</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754857</th>\n",
       "      <td>179830</td>\n",
       "      <td>2099</td>\n",
       "      <td>12034</td>\n",
       "      <td>100:10</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-24</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754858</th>\n",
       "      <td>179830</td>\n",
       "      <td>4660</td>\n",
       "      <td>1480</td>\n",
       "      <td>100:10</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-25</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754859</th>\n",
       "      <td>179830</td>\n",
       "      <td>5341</td>\n",
       "      <td>7751</td>\n",
       "      <td>50:10</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-23</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754860</th>\n",
       "      <td>179830</td>\n",
       "      <td>7555</td>\n",
       "      <td>17</td>\n",
       "      <td>30:5</td>\n",
       "      <td>7</td>\n",
       "      <td>2016-01-23</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754861</th>\n",
       "      <td>179830</td>\n",
       "      <td>760</td>\n",
       "      <td>3237</td>\n",
       "      <td>20:5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-24</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754862</th>\n",
       "      <td>179830</td>\n",
       "      <td>2099</td>\n",
       "      <td>12034</td>\n",
       "      <td>100:10</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-02-03</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754863</th>\n",
       "      <td>343276</td>\n",
       "      <td>4206</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2016-04-25</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754864</th>\n",
       "      <td>653784</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2016-04-18</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754865</th>\n",
       "      <td>653784</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2016-04-16</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754866</th>\n",
       "      <td>3795296</td>\n",
       "      <td>5341</td>\n",
       "      <td>111</td>\n",
       "      <td>30:5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-29</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754867</th>\n",
       "      <td>188086</td>\n",
       "      <td>6568</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2016-04-15</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754868</th>\n",
       "      <td>188086</td>\n",
       "      <td>6568</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2016-06-26</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754869</th>\n",
       "      <td>188086</td>\n",
       "      <td>6568</td>\n",
       "      <td>4723</td>\n",
       "      <td>30:1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-04-15</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754870</th>\n",
       "      <td>188086</td>\n",
       "      <td>6568</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2016-04-17</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754871</th>\n",
       "      <td>188086</td>\n",
       "      <td>6568</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2016-06-04</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754872</th>\n",
       "      <td>212662</td>\n",
       "      <td>3532</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2016-03-08</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754873</th>\n",
       "      <td>212662</td>\n",
       "      <td>2934</td>\n",
       "      <td>5686</td>\n",
       "      <td>30:5</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-21</td>\n",
       "      <td>2016-03-30</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754874</th>\n",
       "      <td>212662</td>\n",
       "      <td>2934</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2016-05-13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754875</th>\n",
       "      <td>212662</td>\n",
       "      <td>2934</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754876</th>\n",
       "      <td>212662</td>\n",
       "      <td>3532</td>\n",
       "      <td>5267</td>\n",
       "      <td>30:5</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-03-22</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754877</th>\n",
       "      <td>212662</td>\n",
       "      <td>3021</td>\n",
       "      <td>3739</td>\n",
       "      <td>30:1</td>\n",
       "      <td>6</td>\n",
       "      <td>2016-05-04</td>\n",
       "      <td>2016-05-08</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754878</th>\n",
       "      <td>212662</td>\n",
       "      <td>2934</td>\n",
       "      <td>5686</td>\n",
       "      <td>30:5</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-21</td>\n",
       "      <td>2016-03-22</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754879</th>\n",
       "      <td>212662</td>\n",
       "      <td>3532</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2016-03-22</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754880</th>\n",
       "      <td>212662</td>\n",
       "      <td>3021</td>\n",
       "      <td>3739</td>\n",
       "      <td>30:1</td>\n",
       "      <td>6</td>\n",
       "      <td>2016-05-08</td>\n",
       "      <td>2016-06-02</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754881</th>\n",
       "      <td>212662</td>\n",
       "      <td>2934</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2016-03-21</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754882</th>\n",
       "      <td>752472</td>\n",
       "      <td>7113</td>\n",
       "      <td>1633</td>\n",
       "      <td>50:10</td>\n",
       "      <td>6</td>\n",
       "      <td>2016-06-13</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754883</th>\n",
       "      <td>752472</td>\n",
       "      <td>3621</td>\n",
       "      <td>2705</td>\n",
       "      <td>20:5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-23</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1754884 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         User_id  Merchant_id  Coupon_id Discount_rate  Distance  \\\n",
       "0        1439408         2632          0           NaN         0   \n",
       "1        1439408         4663      11002        150:20         1   \n",
       "2        1439408         2632       8591          20:1         0   \n",
       "3        1439408         2632       1078          20:1         0   \n",
       "4        1439408         2632       8591          20:1         0   \n",
       "5        1439408         2632          0           NaN         0   \n",
       "6        1439408         2632       8591          20:1         0   \n",
       "7        1832624         3381       7610        200:20         0   \n",
       "8        2029232         3381      11951        200:20         1   \n",
       "9        2029232          450       1532          30:5         0   \n",
       "10       2029232         6459      12737          20:1         0   \n",
       "11       2029232         6459          0           NaN         0   \n",
       "12       2029232         6459          0           NaN         0   \n",
       "13       2747744         6901       1097         50:10        11   \n",
       "14        196342         1579          0           NaN         1   \n",
       "15        196342         1579      10698          20:1         1   \n",
       "16       2223968         3381       9776          10:5         2   \n",
       "17         73611         2099      12034        100:10        11   \n",
       "18        163606         1569       5054        200:30        10   \n",
       "19       3273056         4833       7802        200:20        10   \n",
       "20         94107         3381       7610        200:20         2   \n",
       "21        253750         8390          0           NaN         0   \n",
       "22        253750         6901       2366          30:5         0   \n",
       "23        253750         8390       7531          20:5         0   \n",
       "24        343660         4663      11002        150:20        11   \n",
       "25        376492         1041      13490          30:5         2   \n",
       "26       1964720         7884          0           NaN        10   \n",
       "27       1964720         7884       6704          20:1        10   \n",
       "28       4191584         3051          0           NaN         0   \n",
       "29       4191584         3051          0           NaN         0   \n",
       "...          ...          ...        ...           ...       ...   \n",
       "1754854   179830         6284       4567         50:10         1   \n",
       "1754855   179830         6284       7379         50:10         1   \n",
       "1754856   179830         3710      13056        150:10         4   \n",
       "1754857   179830         2099      12034        100:10         1   \n",
       "1754858   179830         4660       1480        100:10         0   \n",
       "1754859   179830         5341       7751         50:10         0   \n",
       "1754860   179830         7555         17          30:5         7   \n",
       "1754861   179830          760       3237          20:5         0   \n",
       "1754862   179830         2099      12034        100:10         1   \n",
       "1754863   343276         4206          0           NaN         5   \n",
       "1754864   653784          195          0           NaN         0   \n",
       "1754865   653784          195          0           NaN         0   \n",
       "1754866  3795296         5341        111          30:5         0   \n",
       "1754867   188086         6568          0           NaN         0   \n",
       "1754868   188086         6568          0           NaN         0   \n",
       "1754869   188086         6568       4723          30:1         0   \n",
       "1754870   188086         6568          0           NaN         0   \n",
       "1754871   188086         6568          0           NaN         0   \n",
       "1754872   212662         3532          0           NaN         1   \n",
       "1754873   212662         2934       5686          30:5         2   \n",
       "1754874   212662         2934          0           NaN         2   \n",
       "1754875   212662         2934          0           NaN         2   \n",
       "1754876   212662         3532       5267          30:5         1   \n",
       "1754877   212662         3021       3739          30:1         6   \n",
       "1754878   212662         2934       5686          30:5         2   \n",
       "1754879   212662         3532          0           NaN         1   \n",
       "1754880   212662         3021       3739          30:1         6   \n",
       "1754881   212662         2934          0           NaN         2   \n",
       "1754882   752472         7113       1633         50:10         6   \n",
       "1754883   752472         3621       2705          20:5         0   \n",
       "\n",
       "        Date_received       Date  weekday  day   c1  c2  \n",
       "0          1970-01-01 2016-02-17        3    1    0   0  \n",
       "1          2016-05-28 1970-01-01        5   28  150  20  \n",
       "2          2016-02-17 1970-01-01        2   17   20   1  \n",
       "3          2016-03-19 1970-01-01        5   19   20   1  \n",
       "4          2016-06-13 1970-01-01        0   13   20   1  \n",
       "5          1970-01-01 2016-05-16        3    1    0   0  \n",
       "6          2016-05-16 2016-06-13        0   16   20   1  \n",
       "7          2016-04-29 1970-01-01        4   29  200  20  \n",
       "8          2016-01-29 1970-01-01        4   29  200  20  \n",
       "9          2016-05-30 1970-01-01        0   30   30   5  \n",
       "10         2016-05-19 1970-01-01        3   19   20   1  \n",
       "11         1970-01-01 2016-06-26        3    1    0   0  \n",
       "12         1970-01-01 2016-05-19        3    1    0   0  \n",
       "13         2016-06-06 1970-01-01        0    6   50  10  \n",
       "14         1970-01-01 2016-06-06        3    1    0   0  \n",
       "15         2016-06-06 1970-01-01        0    6   20   1  \n",
       "16         2016-01-29 1970-01-01        4   29   10   5  \n",
       "17         2016-02-07 1970-01-01        6    7  100  10  \n",
       "18         2016-04-21 1970-01-01        3   21  200  30  \n",
       "19         2016-01-30 1970-01-01        5   30  200  20  \n",
       "20         2016-04-12 1970-01-01        1   12  200  20  \n",
       "21         1970-01-01 2016-03-27        3    1    0   0  \n",
       "22         2016-05-18 1970-01-01        2   18   30   5  \n",
       "23         2016-03-27 1970-01-01        6   27   20   5  \n",
       "24         2016-05-28 1970-01-01        5   28  150  20  \n",
       "25         2016-01-27 1970-01-01        2   27   30   5  \n",
       "26         1970-01-01 2016-01-15        3    1    0   0  \n",
       "27         2016-02-15 1970-01-01        0   15   20   1  \n",
       "28         1970-01-01 2016-05-19        3    1    0   0  \n",
       "29         1970-01-01 2016-05-16        3    1    0   0  \n",
       "...               ...        ...      ...  ...  ...  ..  \n",
       "1754854    2016-01-24 1970-01-01        6   24   50  10  \n",
       "1754855    2016-01-24 1970-01-01        6   24   50  10  \n",
       "1754856    2016-01-24 1970-01-01        6   24  150  10  \n",
       "1754857    2016-01-24 1970-01-01        6   24  100  10  \n",
       "1754858    2016-01-25 1970-01-01        0   25  100  10  \n",
       "1754859    2016-01-23 1970-01-01        5   23   50  10  \n",
       "1754860    2016-01-23 1970-01-01        5   23   30   5  \n",
       "1754861    2016-01-24 1970-01-01        6   24   20   5  \n",
       "1754862    2016-02-03 1970-01-01        2    3  100  10  \n",
       "1754863    1970-01-01 2016-04-25        3    1    0   0  \n",
       "1754864    1970-01-01 2016-04-18        3    1    0   0  \n",
       "1754865    1970-01-01 2016-04-16        3    1    0   0  \n",
       "1754866    2016-01-29 1970-01-01        4   29   30   5  \n",
       "1754867    1970-01-01 2016-04-15        3    1    0   0  \n",
       "1754868    1970-01-01 2016-06-26        3    1    0   0  \n",
       "1754869    2016-04-15 1970-01-01        4   15   30   1  \n",
       "1754870    1970-01-01 2016-04-17        3    1    0   0  \n",
       "1754871    1970-01-01 2016-06-04        3    1    0   0  \n",
       "1754872    1970-01-01 2016-03-08        3    1    0   0  \n",
       "1754873    2016-03-21 2016-03-30        0   21   30   5  \n",
       "1754874    1970-01-01 2016-05-13        3    1    0   0  \n",
       "1754875    1970-01-01 2016-05-12        3    1    0   0  \n",
       "1754876    2016-03-22 1970-01-01        1   22   30   5  \n",
       "1754877    2016-05-04 2016-05-08        2    4   30   1  \n",
       "1754878    2016-03-21 2016-03-22        0   21   30   5  \n",
       "1754879    1970-01-01 2016-03-22        3    1    0   0  \n",
       "1754880    2016-05-08 2016-06-02        6    8   30   1  \n",
       "1754881    1970-01-01 2016-03-21        3    1    0   0  \n",
       "1754882    2016-06-13 1970-01-01        0   13   50  10  \n",
       "1754883    2016-05-23 1970-01-01        0   23   20   5  \n",
       "\n",
       "[1754884 rows x 11 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_F_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对优惠券描述列进行初步特征提取 - c\n",
    "\n",
    "def feature_off(df):\n",
    "    \n",
    "    # 辅助df 满足特定条件的子df\n",
    "    # 优惠券编号不为0\n",
    "    temp = dfoff[dfoff.Coupon_id != 0]\n",
    "    # 收到消费券且消费\n",
    "    coupon_consume = temp[temp.Date != date_null]\n",
    "    # 收到消费券但未消费\n",
    "    coupon_no_consume = temp[temp.Date == date_null]\n",
    "    # 收到消费券且消费的以用户为聚合列的聚合对象\n",
    "    user_coupon_consume = coupon_consume.groupby('User_id')\n",
    "    # 生成收到消费券星期列\n",
    "    df['weekday'] = df.Date_received.dt.weekday\n",
    "    # 生成收到消费券日列\n",
    "    df['day'] = df.Date_received.dt.day\n",
    "    \n",
    "    \n",
    "    '''offline coupon features'''\n",
    "    # 辅助函数\n",
    "    def getDiscountType(row):\n",
    "        if pd.isnull(row):\n",
    "            return np.nan\n",
    "        elif ':' in str(row):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def convertRate(row):\n",
    "        if pd.isnull(row):\n",
    "            return 1.0\n",
    "        elif ':' in str(row):\n",
    "            rows = row.split(':')\n",
    "            return 1.0 - float(rows[1])/float(rows[0])\n",
    "        else:\n",
    "            return float(row)\n",
    "\n",
    "    def getDiscountMan(row):\n",
    "        if ':' in str(row):\n",
    "            rows = row.split(':')\n",
    "            return int(rows[0])\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def getDiscountJian(row):\n",
    "        if ':' in str(row):\n",
    "            rows = row.split(':')\n",
    "            return int(rows[1])\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    # 满减的满值\n",
    "    df['c1'] = df['Discount_rate'].apply(getDiscountMan)\n",
    "    \n",
    "    # 满减的减值\n",
    "    df['c2'] = df['Discount_rate'].apply(getDiscountJian)\n",
    "    \n",
    "    # 折扣率\n",
    "    df['c3'] = df['Discount_rate'].apply(convertRate)\n",
    "    \n",
    "    # 优惠类型 - 满减1 打折0\n",
    "    df['c4'] = 0\n",
    "    df.loc[df.Discount_rate.str.contains(':') == True, 'c4'] = 1\n",
    "    \n",
    "    # 优惠券发行次数\n",
    "    temp = dfoff[dfoff.Coupon_id != 0].groupby('Coupon_id').size().reset_index(name='c5') # 计数专用，只会生成计数列一列，不用count()\n",
    "    df = pd.merge(df, temp, how='left', on='Coupon_id')\n",
    "    \n",
    "    # 某优惠券一共被使用多少张\n",
    "    temp = coupon_consume.groupby('Coupon_id').size().reset_index(name='c6')\n",
    "    df = pd.merge(df, temp, how='left', on='Coupon_id')\n",
    "    \n",
    "    # 某优惠券使用率\n",
    "    df['c7'] = df.c6 / df.c5\n",
    "    \n",
    "    # 某优惠券没有使用的数目\n",
    "    df['c8'] = df.c5 - df.c6\n",
    "    \n",
    "    # 某优惠券在当天发行了多少张\n",
    "    temp = dfoff.groupby(['Coupon_id', 'Date_received']).size().reset_index(name='c9') # 某优惠券+某天 作为唯一键\n",
    "    df = pd.merge(df, temp, how='left', on=['Coupon_id', 'Date_received'])\n",
    "    \n",
    "    # 不同打折优惠券领取次数\n",
    "    temp = dfoff.groupby('Discount_rate').size().reset_index(name='c10')\n",
    "    df = pd.merge(df, temp, how='left', on='Discount_rate')\n",
    "\n",
    "    # 不同打折优惠券使用次数\n",
    "    temp = coupon_consume.groupby('Discount_rate').size().reset_index(name='c11')\n",
    "    df = pd.merge(df, temp, how='left', on='Discount_rate')\n",
    "\n",
    "    # 不同打折优惠券不使用次数\n",
    "    df['c12'] = df.c10 - df.c11\n",
    "    \n",
    "    # 不同打折优惠券使用率\n",
    "    df['c13'] = df.c11 / df.c10\n",
    "    \n",
    "    # 优惠券核销最早最晚天数差 & 优惠券核销平均时间？\n",
    "    temp = coupon_consume.groupby('Coupon_id').Date.max().reset_index(name='max') # 某优惠券最后核销时间\n",
    "    temp = pd.merge(coupon_consume, temp)\n",
    "    temp_1 = coupon_consume.groupby('Coupon_id').Date.min().reset_index(name='min') # 某优惠券最早核销时间\n",
    "    temp = pd.merge(temp, temp_1)\n",
    "    temp_2 = coupon_consume.groupby('Coupon_id').size().reset_index(name='count') # 某优惠券核销次数\n",
    "    temp = pd.merge(temp, temp_2)\n",
    "    temp['c14'] = (temp['max'] - temp['min']).dt.days\n",
    "    temp['c15'] = ((temp['max'] - temp['min']).dt.days / (temp['count'] - 1)) # 为什么减一？\n",
    "    temp = temp.drop_duplicates('Coupon_id')\n",
    "    df = pd.merge(df, temp[['Coupon_id', 'c14', 'c15']], how='left', on='Coupon_id')\n",
    "\n",
    "    '''user features'''\n",
    "    # 用户消费优惠券次数\n",
    "    temp = user_coupon_consume.size().reset_index(name='u1')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "    # X.u2.fillna(0, inplace=True)\n",
    "    # X.u2 = X.u2.astype(int)\n",
    "\n",
    "    # 用户领取优惠券不消费次数\n",
    "    temp = coupon_no_consume.groupby('User_id').size().reset_index(name='u2')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户领取优惠券并消费比领取未消费\n",
    "    df['u3'] = df.u1 / df.u2\n",
    "\n",
    "    # 用户领取优惠券次数\n",
    "    df['u4'] = df.u1.fillna(0) + df.u2.fillna(0)\n",
    "\n",
    "    # 用户的优惠券核销率\n",
    "    df['u4'] = df.u1 / df.u4\n",
    "\n",
    "    # 普通消费次数 - 没领取优惠券但消费\n",
    "    temp = dfoff[(dfoff.Coupon_id == 0) & (dfoff.Date != date_null)]\n",
    "    temp1 = temp.groupby('User_id').size().reset_index(name='u5')\n",
    "    df = pd.merge(df, temp1, how='left', on='User_id')\n",
    "\n",
    "    # 用户一共消费多少次\n",
    "    df['u6'] = df.u1 + df.u5\n",
    "\n",
    "    # 用户使用优惠券消费占比\n",
    "    df['u7'] = df.u1 / df.u6\n",
    "\n",
    "    # 用户正常消费最长间隔 & 平均间隔\n",
    "    temp = dfoff[dfoff.Date != date_null]\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').Date.max().reset_index(name='max'))\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').Date.min().reset_index(name='min'))\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').size().reset_index(name='count'))\n",
    "    temp['u8'] = (temp['max'] - temp['min']).dt.days\n",
    "    temp['u9'] = ((temp['max'] - temp['min']).dt.days / (temp['count'] - 1))\n",
    "    temp = temp.drop_duplicates('User_id')\n",
    "    df = pd.merge(df, temp[['User_id', 'u8', 'u9']], how='left', on='User_id')\n",
    "    \n",
    "    # 用户优惠券消费最长间隔 & 平均间隔\n",
    "    temp = dfoff[(dfoff.Coupon_id != 0) & (dfoff.Date != date_null)]\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').Date.max().reset_index(name='max'))\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').Date.min().reset_index(name='min'))\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').size().reset_index(name='count'))\n",
    "    temp['u10'] = (temp['max'] - temp['min']).dt.days\n",
    "    temp['u11'] = ((temp['max'] - temp['min']).dt.days / (temp['count'] - 1))\n",
    "    temp = temp.drop_duplicates('User_id')\n",
    "    df = pd.merge(df, temp[['User_id', 'u10', 'u11']], how='left', on='User_id')\n",
    "                   \n",
    "#     # 15天内平均会普通消费几次\n",
    "#     X['u8'] = X.u6 / 15\n",
    "\n",
    "#     # 15天内平均会优惠券消费几次\n",
    "#     X['u9'] = X.u7 / 15\n",
    "\n",
    "    # 用户领取优惠券到使用优惠券的 总&平均 间隔时间\n",
    "    temp = coupon_consume.copy()\n",
    "    temp['days'] = (temp.Date - temp.Date_received).dt.days\n",
    "    temp_1 = (temp.groupby('User_id').days.sum()).reset_index(name='u12')\n",
    "    temp_2 = (temp.groupby('User_id').days.sum() / temp.groupby('User_id').size()).reset_index(name='u13')\n",
    "    df = pd.merge(df, temp_1, how='left', on='User_id')\n",
    "    df = pd.merge(df, temp_2, how='left', on='User_id')\n",
    "\n",
    "#     # 在15天内使用掉优惠券的值大小\n",
    "#     X['u11'] = X.u10 / 15\n",
    "\n",
    "    # 用户领取优惠券到使用优惠券间隔小于15天的次数\n",
    "    temp = coupon_consume.copy()\n",
    "    temp['days'] = (temp.Date - temp.Date_received).dt.days\n",
    "    temp = temp[temp.days <= 15]\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='u14')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户领取后15天使用掉优惠券的次数除以总使用优惠券的次数\n",
    "    df['u15'] = df.u14 / df.u1\n",
    "\n",
    "    # 用户领取后15天使用掉优惠券的次数除以领取优惠券未消费的次数\n",
    "    df['u16'] = df.u14 / df.u2\n",
    "\n",
    "    # 用户15天使用掉优惠券的次数除以领取优惠券的总次数\n",
    "    df['u17'] = df.u14 / df.u4\n",
    "\n",
    "    # 用户消费优惠券的平均折率\n",
    "    temp_disr = dfoff.copy() # 生成一个带折扣率的temp df\n",
    "    temp_disr['discount_rate'] = temp_disr['Discount_rate'].apply(convertRate)\n",
    "    \n",
    "    temp = temp_disr[(dfoff.Date != date_null) & dfoff.Coupon_id != 0].groupby('User_id').discount_rate.mean().reset_index(name='u18')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销优惠券的最低消费折率\n",
    "    temp = temp_disr[(dfoff.Date != date_null) & dfoff.Coupon_id != 0].groupby('User_id').discount_rate.min().reset_index(name='u19')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销优惠券的最高消费折率\n",
    "    temp = temp_disr[(dfoff.Date != date_null) & dfoff.Coupon_id != 0].groupby('User_id').discount_rate.max().reset_index(name='u20')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销过的不同优惠券数量\n",
    "    temp = coupon_consume.groupby(['User_id', 'Coupon_id']).size()\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='u21')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户领取过该优惠券的次数\n",
    "    temp = dfoff[dfoff.Date_received != date_null]\n",
    "    temp = temp.groupby(['User_id', 'Coupon_id']).size().reset_index(name='u22')\n",
    "    df = pd.merge(df, temp, how='left', on=['User_id', 'Coupon_id'])\n",
    "                   \n",
    "    # 用户领取所有的不同优惠券数量\n",
    "    temp = dfoff[dfoff.Date_received != date_null]\n",
    "    temp = temp.groupby(['User_id', 'Coupon_id']).size()\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='u23')\n",
    "    df = pd.merge(df, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销过的不同优惠券数量占所有不同优惠券的比重\n",
    "    df['u24'] = df.u21 / df.u23\n",
    "\n",
    "    # 用户平均每种优惠券核销多少张\n",
    "    df['u25'] = df.u1 / df.u21\n",
    "                   \n",
    "    # 用户每领多少种优惠券会核销一张\n",
    "    df['u26'] = df.u23 / df.u1\n",
    "                   \n",
    "    # 用户每领多少种未来核销优惠券会核销一张\n",
    "    df['u27'] = df.u21 / df.u1\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "#     # 核销优惠券用户-商家平均距离\n",
    "#     temp = offline[(offline.Coupon_id != 0) & (offline.Date != date_null) & (offline.Distance != 11)]\n",
    "#     temp = temp.groupby('User_id').Distance\n",
    "#     temp = pd.merge(temp.count().reset_index(name='x'), temp.sum().reset_index(name='y'), on='User_id')\n",
    "#     temp['u35'] = temp.y / temp.x\n",
    "#     temp = temp[['User_id', 'u35']]\n",
    "#     X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "#     # 用户核销优惠券中的最小用户-商家距离\n",
    "#     temp = coupon_consume[coupon_consume.Distance != 11]\n",
    "#     temp = temp.groupby('User_id').Distance.min().reset_index(name='u36')\n",
    "#     X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "#     # 用户核销优惠券中的最大用户-商家距离\n",
    "#     temp = coupon_consume[coupon_consume.Distance != 11]\n",
    "#     temp = temp.groupby('User_id').Distance.max().reset_index(name='u37')\n",
    "#     X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "#     # 优惠券类型\n",
    "#     discount_types = [\n",
    "#         '0.2', '0.5', '0.6', '0.7', '0.75', '0.8', '0.85', '0.9', '0.95', '30:20', '50:30', '10:5',\n",
    "#         '20:10', '100:50', '200:100', '50:20', '30:10', '150:50', '100:30', '20:5', '200:50', '5:1',\n",
    "#         '50:10', '100:20', '150:30', '30:5', '300:50', '200:30', '150:20', '10:1', '50:5', '100:10',\n",
    "#         '200:20', '300:30', '150:10', '300:20', '500:30', '20:1', '100:5', '200:10', '30:1', '150:5',\n",
    "#         '300:10', '200:5', '50:1', '100:1',\n",
    "#     ]\n",
    "#     X['discount_type'] = -1\n",
    "#     for k, v in enumerate(discount_types):\n",
    "#         X.loc[X.Discount_rate == v, 'discount_type'] = k\n",
    "\n",
    "#     # 不同优惠券领取次数\n",
    "#     temp = offline.groupby(['User_id', 'Discount_rate']).size().reset_index(name='u41')\n",
    "#     X = pd.merge(X, temp, how='left', on=['User_id', 'Discount_rate'])\n",
    "\n",
    "#     # 不同优惠券使用次数\n",
    "#     temp = coupon_consume.groupby(['User_id', 'Discount_rate']).size().reset_index(name='u42')\n",
    "#     X = pd.merge(X, temp, how='left', on=['User_id', 'Discount_rate'])\n",
    "\n",
    "#     # 不同优惠券不使用次数\n",
    "#     temp = coupon_no_consume.groupby(['User_id', 'Discount_rate']).size().reset_index(name='u43')\n",
    "#     X = pd.merge(X, temp, how='left', on=['User_id', 'Discount_rate'])\n",
    "\n",
    "#     # 不同打折优惠券使用率\n",
    "#     X['u44'] = X.u42 / X.u41\n",
    "\n",
    "#     # 满减类型优惠券领取次数\n",
    "#     temp = offline[offline.Discount_rate.str.contains(':') == True]\n",
    "#     temp = temp.groupby('User_id').size().reset_index(name='u48')\n",
    "#     X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "#     # 打折类型优惠券领取次数\n",
    "#     temp = offline[offline.Discount_rate.str.contains('\\.') == True]\n",
    "#     temp = temp.groupby('User_id').size().reset_index(name='u49')\n",
    "#     X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到未删键的特征df\n",
    "df_F_off_train = feature_off(df_F_off)\n",
    "df_F_test_train = feature_off(df_F_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征的列名list\n",
    "feature_lst = ['c'+str(i) for i in range(1,16)]+['u'+str(i) for i in range(1,28)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成 训练集 与 测试集 特征矩阵\n",
    "df_F_off_temp = df_F_off_train[feature_lst]\n",
    "df_F_off_X = df_F_off_temp.fillna(0)\n",
    "\n",
    "df_F_test_temp = df_F_test_train[feature_lst]\n",
    "df_F_test_X = df_F_test_temp.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成标签\n",
    "y = df_F_off_train.copy()\n",
    "\n",
    "y['label'] = 0\n",
    "y.loc[(y.Date != date_null) & (y.Date - y.Date_received <= datetime.timedelta(15)), 'label'] = 1\n",
    "\n",
    "labels = y['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(df_F_off_X, labels, random_state=1, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:463: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='entropy', max_depth=5, max_features=0.8,\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=25, n_jobs=None, oob_score=True,\n",
       "            random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=25, \n",
    "                                        max_depth=5,\n",
    "                                        criterion='entropy',\n",
    "                                        max_features=0.8,\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        oob_score=True)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainAccracy: 0.9473661716908599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97   1352581\n",
      "           1       0.41      1.00      0.58     51326\n",
      "\n",
      "   micro avg       0.95      0.95      0.95   1403907\n",
      "   macro avg       0.70      0.97      0.78   1403907\n",
      "weighted avg       0.98      0.95      0.96   1403907\n",
      "\n",
      "testAccracy: 0.9480136875065888\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97    337908\n",
      "           1       0.42      1.00      0.59     13069\n",
      "\n",
      "   micro avg       0.95      0.95      0.95    350977\n",
      "   macro avg       0.71      0.97      0.78    350977\n",
      "weighted avg       0.98      0.95      0.96    350977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 预测以及结果评价\n",
    "\n",
    "y_train_predict = model.predict(x_train)\n",
    "y_valid_predict = model.predict(x_valid)\n",
    "\n",
    "print('trainAccracy:',model.score(x_train,y_train))\n",
    "print(classification_report(y_train,y_train_predict)) #真实数据在前 训练结果在后！\n",
    "print('testAccracy:',model.score(x_valid,y_valid))\n",
    "print(classification_report(y_valid,y_valid_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc: 0.9917851631389628\n"
     ]
    }
   ],
   "source": [
    "y_valid_predict_prob = model.predict_proba(x_valid)#基于SVM对验证集做出预测，prodict_prob_y 为预测的概率\n",
    "valid_auc = metrics.roc_auc_score(y_valid,y_valid_predict_prob[:,1])#验证集上的auc值\n",
    "print('valid_auc:',valid_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Coupon_id</th>\n",
       "      <th>Date_received</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4129537</td>\n",
       "      <td>9983</td>\n",
       "      <td>20160712</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6949378</td>\n",
       "      <td>3429</td>\n",
       "      <td>20160706</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2166529</td>\n",
       "      <td>6928</td>\n",
       "      <td>20160727</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2166529</td>\n",
       "      <td>1808</td>\n",
       "      <td>20160727</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6172162</td>\n",
       "      <td>6500</td>\n",
       "      <td>20160708</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_id  Coupon_id  Date_received  label\n",
       "0  4129537       9983       20160712    0.0\n",
       "1  6949378       3429       20160706    0.0\n",
       "2  2166529       6928       20160727    0.0\n",
       "3  2166529       1808       20160727    0.0\n",
       "4  6172162       6500       20160708    0.0"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tol = model\n",
    "# model_tol.fit(x_tol,y_tol)\n",
    "\n",
    "y_test_pred = model_tol.predict_proba(df_F_test_X)\n",
    "\n",
    "dftest_for_save = pd.read_csv('C:/Users/jxjsj/Desktop/tianchi/o2ocoupon/ccf_offline_stage1_test_revised.csv')\n",
    "dftest1 = dftest_for_save[['User_id','Coupon_id','Date_received']].copy()\n",
    "dftest1['label'] = y_test_pred[:,1]\n",
    "dftest1.to_csv('C:/Users/jxjsj/Desktop/tianchi/o2ocoupon/submission.csv', index=False, header=False)\n",
    "dftest1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool is ok.\n"
     ]
    }
   ],
   "source": [
    "# 1. 将满xx减yy类型(`xx:yy`)的券变成折扣率 : `1 - yy/xx`，同时建立折扣券相关的特征 `discount_rate, discount_man, discount_jian, discount_type`\n",
    "# 2. 将距离 `str` 转为 `int`\n",
    "# convert Discount_rate and Distance\n",
    "def getDiscountType(row):\n",
    "    if pd.isnull(row):\n",
    "        return np.nan\n",
    "    elif ':' in row:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def convertRate(row):\n",
    "    \"\"\"Convert discount to rate\"\"\"\n",
    "    if pd.isnull(row):\n",
    "        return 1.0\n",
    "    elif ':' in str(row):\n",
    "        rows = row.split(':')\n",
    "        return 1.0 - float(rows[1])/float(rows[0])\n",
    "    else:\n",
    "        return float(row)\n",
    "\n",
    "def getDiscountMan(row):\n",
    "    if ':' in str(row):\n",
    "        rows = row.split(':')\n",
    "        return int(rows[0])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def getDiscountJian(row):\n",
    "    if ':' in str(row):\n",
    "        rows = row.split(':')\n",
    "        return int(rows[1])\n",
    "    else:\n",
    "        return 0\n",
    "print(\"tool is ok.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(df):\n",
    "    # convert discunt_rate\n",
    "    df['discount_rate'] = df['Discount_rate'].apply(convertRate)\n",
    "    df['discount_man'] = df['Discount_rate'].apply(getDiscountMan)\n",
    "    df['discount_jian'] = df['Discount_rate'].apply(getDiscountJian)\n",
    "    df['discount_type'] = df['Discount_rate'].apply(getDiscountType)\n",
    "    print(df['discount_rate'].unique())\n",
    "    # convert distance\n",
    "    df['distance'] = df['Distance'].fillna(-1).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.86666667 0.95       0.9        0.83333333 0.8\n",
      " 0.5        0.85       0.75       0.66666667 0.93333333 0.7\n",
      " 0.6        0.96666667 0.98       0.99       0.975      0.33333333\n",
      " 0.2        0.4       ]\n",
      "[0.83333333 0.9        0.96666667 0.8        0.95       0.75\n",
      " 0.98       0.5        0.86666667 0.6        0.66666667 0.7\n",
      " 0.85       0.33333333 0.94       0.93333333 0.975      0.99      ]\n"
     ]
    }
   ],
   "source": [
    "# 特征生成\n",
    "dfoff = processData(dfoff)\n",
    "dftest = processData(dftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 收到优惠券的日子 去重 排序\n",
    "date_received = dfoff['Date_received'].unique()\n",
    "date_received = sorted(date_received[pd.notnull(date_received)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 购买商品的日子 不去重 排序\n",
    "date_buy = dfoff['Date'].unique()\n",
    "date_buy = sorted(date_buy[pd.notnull(date_buy)])\n",
    "date_buy = sorted(dfoff[dfoff['Date'].notnull()]['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对同一天收到的优惠券聚合，计数\n",
    "couponbydate = dfoff[dfoff['Date_received'].notnull()\n",
    "                    ][['Date_received', 'Date']].groupby(['Date_received'], as_index=False).count()\n",
    "couponbydate.columns = ['Date_received','count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 对同一天收到的优惠券，且未来被花掉了聚合，计数\n",
    "buybydate_temp = dfoff[(dfoff['Date'].notnull())]\n",
    "buybydate = buybydate_temp[(buybydate_temp['Date_received'].notnull())][['Date_received', 'Date']].groupby(['Date_received'], as_index=False).count()\n",
    "buybydate.columns = ['Date_received','count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date对象的方法weekday()返回0-6表示周一到周日\n",
    "def getWeekday(row):\n",
    "    if row == 'nan': # NaN被str()才可被 ==‘nan’识别\n",
    "        return np.nan\n",
    "    else:\n",
    "        return date(int(row[0:4]), int(row[4:6]), int(row[6:8])).weekday() + 1\n",
    "\n",
    "dfoff['weekday'] = dfoff['Date_received'].astype(str).apply(getWeekday)\n",
    "dftest['weekday'] = dftest['Date_received'].astype(str).apply(getWeekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekday_type :  周六和周日为1，其他为0\n",
    "dfoff['weekday_type'] = dfoff['weekday'].apply(lambda x : 1 if x in [6,7] else 0 )\n",
    "dftest['weekday_type'] = dftest['weekday'].apply(lambda x : 1 if x in [6,7] else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change weekday to one-hot encoding \n",
    "weekdaycols = ['weekday_' + str(i) for i in range(1,8)]\n",
    "# tmpdf = pd.get_dummies(dfoff['weekday'].replace('nan', np.nan))\n",
    "tmpdf = pd.get_dummies(dfoff['weekday'])\n",
    "tmpdf.columns = weekdaycols\n",
    "dfoff[weekdaycols] = tmpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdf = pd.get_dummies(dftest['weekday'])\n",
    "tmpdf.columns = weekdaycols\n",
    "dftest[weekdaycols] = tmpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(row):\n",
    "    if pd.isnull(row['Date_received']):\n",
    "        return -1\n",
    "    if pd.notnull(row['Date']):\n",
    "        td = pd.to_datetime(row['Date'], format='%Y%m%d') -  pd.to_datetime(row['Date_received'], format='%Y%m%d')\n",
    "        if td <= pd.Timedelta(15, 'D'):\n",
    "            return 1\n",
    "    return 0\n",
    "dfoff['label'] = dfoff.apply(label, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----data split------\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "# feature\n",
    "original_feature_label = ['discount_rate','discount_type','discount_man', 'discount_jian','distance', 'weekday', 'weekday_type','label'] + weekdaycols\n",
    "original_feature = ['discount_rate','discount_type','discount_man', 'discount_jian','distance', 'weekday', 'weekday_type'] + weekdaycols\n",
    "\n",
    "# data split - train2train&valid\n",
    "print(\"-----data split------\")\n",
    "df = dfoff[dfoff['label'] != -1].copy() # 把需要研究的对象0与1挑选出来\n",
    "train = df[(df['Date_received'] < 20160516)].copy()[original_feature_label]\n",
    "valid = df[(df['Date_received'] >= 20160516) & (df['Date_received'] <= 20160615)].copy()[original_feature_label]\n",
    "\n",
    "x_train = train[original_feature]\n",
    "y_train = train['label']\n",
    "x_valid = valid[original_feature]\n",
    "y_valid = valid['label']\n",
    "\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tol sample\n",
    "x_tol = pd.concat([x_train,x_valid],axis=0)\n",
    "y_tol = pd.concat([y_train,y_valid],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-d7cce8e7a1b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m                    )\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# model.fit(x_train,y_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mover_samples_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mover_samples_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    975\u001b[0m         \"\"\"\n\u001b[0;32m    976\u001b[0m         return self._fit(X, y, incremental=(self.warm_start and\n\u001b[1;32m--> 977\u001b[1;33m                                             hasattr(self, \"classes_\")))\n\u001b[0m\u001b[0;32m    978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m             self._fit_lbfgs(X, y, activations, deltas, coef_grads,\n\u001b[1;32m--> 376\u001b[1;33m                             intercept_grads, layer_units)\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit_lbfgs\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units)\u001b[0m\n\u001b[0;32m    464\u001b[0m             \u001b[0miprint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miprint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m             \u001b[0mpgtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m             args=(X, y, activations, deltas, coef_grads, intercept_grads))\n\u001b[0m\u001b[0;32m    467\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimal_parameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[1;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[1;32m--> 199\u001b[1;33m                            **opts)\n\u001b[0m\u001b[0;32m    200\u001b[0m     d = {'grad': res['jac'],\n\u001b[0;32m    201\u001b[0m          \u001b[1;34m'task'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'message'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[0;32m    333\u001b[0m             \u001b[1;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m             \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[1;34m(*wrapper_args)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_loss_grad_lbfgs\u001b[1;34m(self, packed_coef_inter, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked_coef_inter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         loss, coef_grads, intercept_grads = self._backprop(\n\u001b[1;32m--> 175\u001b[1;33m             X, y, activations, deltas, coef_grads, intercept_grads)\n\u001b[0m\u001b[0;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_pack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoef_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_backprop\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mloss_func_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'log_loss'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_activation_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m             \u001b[0mloss_func_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'binary_log_loss'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLOSS_FUNCTIONS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss_func_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[1;31m# Add L2 regularization term to loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         values = np.sum(\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_base.py\u001b[0m in \u001b[0;36mbinary_log_loss\u001b[1;34m(y_true, y_prob)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[0my_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1e-10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m     return -np.sum(y_true * np.log(y_prob) +\n\u001b[0m\u001b[0;32m    248\u001b[0m                    (1 - y_true) * np.log(1 - y_prob)) / y_prob.shape[0]\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MLPClassifier(activation='tanh', \n",
    "                    solver='lbfgs',\n",
    "                    alpha=1e-5,\n",
    "                    hidden_layer_sizes=(len(x_train.columns),),\n",
    "                    random_state=1, \n",
    "                   )\n",
    "# model.fit(x_train,y_train)\n",
    "model.fit(over_samples_x, over_samples_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.1, average=False, class_weight='balanced',\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.01, learning_rate='optimal', loss='log', max_iter=100,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=1, penalty='elasticnet',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SGDClassifier(\n",
    "    loss='log',\n",
    "    penalty='elasticnet',\n",
    "    fit_intercept=True,\n",
    "    max_iter=100,\n",
    "    shuffle=True,\n",
    "    alpha = 0.1,\n",
    "    l1_ratio = 0.01,\n",
    "    n_jobs=1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(class_weight='balanced',solver = 'lbfgs')\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "model = GaussianNB()\n",
    "# model.fit(x_train, y_train)\n",
    "model.fit(over_samples_x, over_samples_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:463: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='entropy', max_depth=5, max_features=0.8,\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=25, n_jobs=None, oob_score=True,\n",
       "            random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=25, \n",
    "                                        max_depth=5,\n",
    "                                        criterion='entropy',\n",
    "                                        max_features=0.8,\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        oob_score=True)\n",
    "model.fit(x_train, y_train)\n",
    "# model.fit(over_samples_x, over_samples_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运用SMOTE改进的平衡样本\n",
    "over_samples = SMOTE(random_state=11)\n",
    "over_samples_x, over_samples_y = over_samples.fit_sample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "model = svm.SVC(C=2, kernel='linear', decision_function_shape='ovr',class_weight = \"balanced\")\n",
    "model.fit(x_train, y_train)\n",
    "# model.fit(over_samples_x, over_samples_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainAccracy: 0.6559018653771219\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.65      0.78    759172\n",
      "           1       0.12      0.85      0.20     41524\n",
      "\n",
      "   micro avg       0.66      0.66      0.66    800696\n",
      "   macro avg       0.55      0.75      0.49    800696\n",
      "weighted avg       0.94      0.66      0.75    800696\n",
      "\n",
      "testAccracy: 0.35944589169629354\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.31      0.47    229715\n",
      "           1       0.11      0.88      0.20     22871\n",
      "\n",
      "   micro avg       0.36      0.36      0.36    252586\n",
      "   macro avg       0.54      0.59      0.33    252586\n",
      "weighted avg       0.88      0.36      0.44    252586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #### 预测以及结果评价\n",
    "# print(model.score(valid[original_feature], valid['label']))\n",
    "\n",
    "y_train_predict = model.predict(x_train)\n",
    "y_valid_predict = model.predict(x_valid)\n",
    "\n",
    "print('trainAccracy:',model.score(x_train,y_train))\n",
    "print(classification_report(y_train,y_train_predict)) #真实数据在前 训练结果在后！\n",
    "print('testAccracy:',model.score(x_valid,y_valid))\n",
    "print(classification_report(y_valid,y_valid_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc: 0.5921657961226938\n"
     ]
    }
   ],
   "source": [
    "y_valid_predict_prob = model.predict_proba(x_valid)#基于SVM对验证集做出预测，prodict_prob_y 为预测的概率\n",
    "test_auc = metrics.roc_auc_score(y_valid,list(y_valid_predict))#验证集上的auc值\n",
    "print('test_auc:',test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(y_valid_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---save model---\")\n",
    "with open('1_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "with open('1_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Coupon_id</th>\n",
       "      <th>Date_received</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4129537</td>\n",
       "      <td>9983</td>\n",
       "      <td>20160712</td>\n",
       "      <td>9.233964e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6949378</td>\n",
       "      <td>3429</td>\n",
       "      <td>20160706</td>\n",
       "      <td>1.647855e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2166529</td>\n",
       "      <td>6928</td>\n",
       "      <td>20160727</td>\n",
       "      <td>1.843014e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2166529</td>\n",
       "      <td>1808</td>\n",
       "      <td>20160727</td>\n",
       "      <td>1.896743e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6172162</td>\n",
       "      <td>6500</td>\n",
       "      <td>20160708</td>\n",
       "      <td>5.266069e-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_id  Coupon_id  Date_received         label\n",
       "0  4129537       9983       20160712  9.233964e-07\n",
       "1  6949378       3429       20160706  1.647855e-06\n",
       "2  2166529       6928       20160727  1.843014e-07\n",
       "3  2166529       1808       20160727  1.896743e-07\n",
       "4  6172162       6500       20160708  5.266069e-12"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test prediction for submission\n",
    "y_test_pred = model.predict_proba(dftest[original_feature])\n",
    "dftest1 = dftest[['User_id','Coupon_id','Date_received']].copy()\n",
    "dftest1['label'] = y_test_pred[:,1]\n",
    "dftest1.to_csv('C:/Users/jxjsj/Desktop/tianchi/submission.csv', index=False, header=False)\n",
    "dftest1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:463: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Coupon_id</th>\n",
       "      <th>Date_received</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4129537</td>\n",
       "      <td>9983</td>\n",
       "      <td>20160712</td>\n",
       "      <td>0.467298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6949378</td>\n",
       "      <td>3429</td>\n",
       "      <td>20160706</td>\n",
       "      <td>0.613431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2166529</td>\n",
       "      <td>6928</td>\n",
       "      <td>20160727</td>\n",
       "      <td>0.110725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2166529</td>\n",
       "      <td>1808</td>\n",
       "      <td>20160727</td>\n",
       "      <td>0.130222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6172162</td>\n",
       "      <td>6500</td>\n",
       "      <td>20160708</td>\n",
       "      <td>0.506071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_id  Coupon_id  Date_received     label\n",
       "0  4129537       9983       20160712  0.467298\n",
       "1  6949378       3429       20160706  0.613431\n",
       "2  2166529       6928       20160727  0.110725\n",
       "3  2166529       1808       20160727  0.130222\n",
       "4  6172162       6500       20160708  0.506071"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test prediction for submission - tol sample\n",
    "# model_tol = MLPClassifier(activation='tanh', \n",
    "#                     solver='lbfgs',\n",
    "#                     alpha=1e-5,\n",
    "#                     hidden_layer_sizes=(len(x_tol.columns)*2+1,),\n",
    "#                     random_state=1, \n",
    "#                    )\n",
    "model_tol = model\n",
    "model_tol.fit(x_tol,y_tol)\n",
    "\n",
    "y_test_pred = model_tol.predict_proba(dftest[original_feature])\n",
    "dftest1 = dftest[['User_id','Coupon_id','Date_received']].copy()\n",
    "dftest1['label'] = y_test_pred[:,1]\n",
    "dftest1.to_csv('C:/Users/jxjsj/Desktop/tianchi/submission.csv', index=False, header=False)\n",
    "dftest1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
