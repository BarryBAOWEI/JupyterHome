{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from math import ceil\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "\n",
    "def drop_columns(X, predict=False):\n",
    "    columns = [\n",
    "        'User_id', 'Merchant_id', 'Discount_rate', 'Date_received', 'discount_rate_x', 'discount_rate_y',\n",
    "        # 'u33', 'u34'\n",
    "    ]\n",
    "\n",
    "    if predict:\n",
    "        columns.append('Coupon_id')\n",
    "    else:\n",
    "        columns.append('Date')\n",
    "\n",
    "    X.drop(columns=columns, inplace=True)\n",
    "\n",
    "\n",
    "def get_preprocess_data(predict=False):\n",
    "    if predict:\n",
    "        offline = pd.read_csv('ccf_offline_stage1_test_revised.csv', parse_dates=['Date_received'])\n",
    "    else:\n",
    "        offline = pd.read_csv('ccf_offline_stage1_train.csv', parse_dates=['Date_received', 'Date'])\n",
    "\n",
    "    offline.Distance.fillna(11, inplace=True)\n",
    "    offline.Distance = offline.Distance.astype(int)\n",
    "    offline.Coupon_id.fillna(0, inplace=True)\n",
    "    offline.Coupon_id = offline.Coupon_id.astype(int)\n",
    "    offline.Date_received.fillna(date_null, inplace=True)\n",
    "\n",
    "    offline[['discount_rate_x', 'discount_rate_y']] = offline[offline.Discount_rate.str.contains(':') == True][\n",
    "        'Discount_rate'].str.split(':', expand=True).astype(int)\n",
    "    offline['discount_rate'] = 1 - offline.discount_rate_y / offline.discount_rate_x\n",
    "    offline.discount_rate = offline.discount_rate.fillna(offline.Discount_rate).astype(float)\n",
    "\n",
    "    if predict:\n",
    "        return offline\n",
    "\n",
    "    offline.Date.fillna(date_null, inplace=True)\n",
    "\n",
    "    # online\n",
    "    online = pd.read_csv('ccf_online_stage1_train.csv', parse_dates=['Date_received', 'Date'])\n",
    "\n",
    "    online.Coupon_id.fillna(0, inplace=True)\n",
    "    # online.Coupon_id = online.Coupon_id.astype(int)\n",
    "    online.Date_received.fillna(date_null, inplace=True)\n",
    "    online.Date.fillna(date_null, inplace=True)\n",
    "\n",
    "    return offline, online\n",
    "\n",
    "\n",
    "def task(X_chunk, X, counter):\n",
    "    print(counter, end=',', flush=True)\n",
    "    X_chunk = X_chunk.copy()\n",
    "\n",
    "    X_chunk['o17'] = -1\n",
    "    X_chunk['o18'] = -1\n",
    "\n",
    "    for i, user in X_chunk.iterrows():\n",
    "        temp = X[X.User_id == user.User_id]\n",
    "\n",
    "        temp1 = temp[temp.Date_received < user.Date_received]\n",
    "        temp2 = temp[temp.Date_received > user.Date_received]\n",
    "\n",
    "        # 用户此次之后/前领取的所有优惠券数目\n",
    "        X_chunk.loc[i, 'o3'] = len(temp1)\n",
    "        X_chunk.loc[i, 'o4'] = len(temp2)\n",
    "\n",
    "        # 用户此次之后/前领取的特定优惠券数目\n",
    "        X_chunk.loc[i, 'o5'] = len(temp1[temp1.Coupon_id == user.Coupon_id])\n",
    "        X_chunk.loc[i, 'o6'] = len(temp2[temp2.Coupon_id == user.Coupon_id])\n",
    "\n",
    "        # 用户上/下一次领取的时间间隔\n",
    "        temp1 = temp1.sort_values(by='Date_received', ascending=False)\n",
    "        if len(temp1):\n",
    "            X_chunk.loc[i, 'o17'] = (user.Date_received - temp1.iloc[0].Date_received).days\n",
    "\n",
    "        temp2 = temp2.sort_values(by='Date_received')\n",
    "        if len(temp2):\n",
    "            X_chunk.loc[i, 'o18'] = (temp2.iloc[0].Date_received - user.Date_received).days\n",
    "\n",
    "    return X_chunk\n",
    "\n",
    "\n",
    "def get_offline_features(X, offline):\n",
    "    # X = X[:1000]\n",
    "\n",
    "    print(len(X), len(X.columns))\n",
    "\n",
    "    temp = offline[offline.Coupon_id != 0]\n",
    "    coupon_consume = temp[temp.Date != date_null]\n",
    "    coupon_no_consume = temp[temp.Date == date_null]\n",
    "\n",
    "    user_coupon_consume = coupon_consume.groupby('User_id')\n",
    "\n",
    "    X['weekday'] = X.Date_received.dt.weekday\n",
    "    X['day'] = X.Date_received.dt.day\n",
    "\n",
    "    # # 距离优惠券消费次数\n",
    "    # temp = coupon_consume.groupby('Distance').size().reset_index(name='distance_0')\n",
    "    # X = pd.merge(X, temp, how='left', on='Distance')\n",
    "    #\n",
    "    # # 距离优惠券不消费次数\n",
    "    # temp = coupon_no_consume.groupby('Distance').size().reset_index(name='distance_1')\n",
    "    # X = pd.merge(X, temp, how='left', on='Distance')\n",
    "    #\n",
    "    # # 距离优惠券领取次数\n",
    "    # X['distance_2'] = X.distance_0 + X.distance_1\n",
    "    #\n",
    "    # # 距离优惠券消费率\n",
    "    # X['distance_3'] = X.distance_0 / X.distance_2\n",
    "\n",
    "    # temp = coupon_consume[coupon_consume.Distance != 11].groupby('Distance').size()\n",
    "    # temp['d4'] = temp.Distance.sum() / len(temp)\n",
    "    # X = pd.merge(X, temp, how='left', on='Distance')\n",
    "\n",
    "    '''user features'''\n",
    "\n",
    "    # 优惠券消费次数\n",
    "    temp = user_coupon_consume.size().reset_index(name='u2')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "    # X.u2.fillna(0, inplace=True)\n",
    "    # X.u2 = X.u2.astype(int)\n",
    "\n",
    "    # 优惠券不消费次数\n",
    "    temp = coupon_no_consume.groupby('User_id').size().reset_index(name='u3')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 使用优惠券次数与没使用优惠券次数比值\n",
    "    X['u19'] = X.u2 / X.u3\n",
    "\n",
    "    # 领取优惠券次数\n",
    "    X['u1'] = X.u2.fillna(0) + X.u3.fillna(0)\n",
    "\n",
    "    # 优惠券核销率\n",
    "    X['u4'] = X.u2 / X.u1\n",
    "\n",
    "    # 普通消费次数\n",
    "    temp = offline[(offline.Coupon_id == 0) & (offline.Date != date_null)]\n",
    "    temp1 = temp.groupby('User_id').size().reset_index(name='u5')\n",
    "    X = pd.merge(X, temp1, how='left', on='User_id')\n",
    "\n",
    "    # 一共消费多少次\n",
    "    X['u25'] = X.u2 + X.u5\n",
    "\n",
    "    # 用户使用优惠券消费占比\n",
    "    X['u20'] = X.u2 / X.u25\n",
    "\n",
    "    # 正常消费平均间隔\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').Date.max().reset_index(name='max'))\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').Date.min().reset_index(name='min'))\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').size().reset_index(name='len'))\n",
    "    temp['u6'] = ((temp['max'] - temp['min']).dt.days / (temp['len'] - 1))\n",
    "    temp = temp.drop_duplicates('User_id')\n",
    "    X = pd.merge(X, temp[['User_id', 'u6']], how='left', on='User_id')\n",
    "\n",
    "    # 优惠券消费平均间隔\n",
    "    temp = pd.merge(coupon_consume, user_coupon_consume.Date.max().reset_index(name='max'))\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').Date.min().reset_index(name='min'))\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').size().reset_index(name='len'))\n",
    "    temp['u7'] = ((temp['max'] - temp['min']).dt.days / (temp['len'] - 1))\n",
    "    temp = temp.drop_duplicates('User_id')\n",
    "    X = pd.merge(X, temp[['User_id', 'u7']], how='left', on='User_id')\n",
    "\n",
    "    # 15天内平均会普通消费几次\n",
    "    X['u8'] = X.u6 / 15\n",
    "\n",
    "    # 15天内平均会优惠券消费几次\n",
    "    X['u9'] = X.u7 / 15\n",
    "\n",
    "    # 领取优惠券到使用优惠券的平均间隔时间\n",
    "    temp = coupon_consume.copy()\n",
    "    temp['days'] = (temp.Date - temp.Date_received).dt.days\n",
    "    temp = (temp.groupby('User_id').days.sum() / temp.groupby('User_id').size()).reset_index(name='u10')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 在15天内使用掉优惠券的值大小\n",
    "    X['u11'] = X.u10 / 15\n",
    "\n",
    "    # 领取优惠券到使用优惠券间隔小于15天的次数\n",
    "    temp = coupon_consume.copy()\n",
    "    temp['days'] = (temp.Date - temp.Date_received).dt.days\n",
    "    temp = temp[temp.days <= 15]\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='u21')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户15天使用掉优惠券的次数除以使用优惠券的次数\n",
    "    X['u22'] = X.u21 / X.u2\n",
    "\n",
    "    # 用户15天使用掉优惠券的次数除以领取优惠券未消费的次数\n",
    "    X['u23'] = X.u21 / X.u3\n",
    "\n",
    "    # 用户15天使用掉优惠券的次数除以领取优惠券的总次数\n",
    "    X['u24'] = X.u21 / X.u1\n",
    "\n",
    "    # 消费优惠券的平均折率\n",
    "    temp = user_coupon_consume.discount_rate.mean().reset_index(name='u45')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销优惠券的最低消费折率\n",
    "    temp = user_coupon_consume.discount_rate.min().reset_index(name='u27')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销优惠券的最高消费折率\n",
    "    temp = user_coupon_consume.discount_rate.max().reset_index(name='u28')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销过的不同优惠券数量\n",
    "    temp = coupon_consume.groupby(['User_id', 'Coupon_id']).size()\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='u32')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户领取所有不同优惠券数量\n",
    "    temp = offline[offline.Date_received != date_null]\n",
    "    temp = temp.groupby(['User_id', 'Coupon_id']).size().reset_index(name='u47')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Coupon_id'])\n",
    "\n",
    "    # 用户核销过的不同优惠券数量占所有不同优惠券的比重\n",
    "    X['u33'] = X.u32 / X.u47\n",
    "\n",
    "    # 用户平均每种优惠券核销多少张\n",
    "    X['u34'] = X.u2 / X.u47\n",
    "\n",
    "    # 核销优惠券用户-商家平均距离\n",
    "    temp = offline[(offline.Coupon_id != 0) & (offline.Date != date_null) & (offline.Distance != 11)]\n",
    "    temp = temp.groupby('User_id').Distance\n",
    "    temp = pd.merge(temp.count().reset_index(name='x'), temp.sum().reset_index(name='y'), on='User_id')\n",
    "    temp['u35'] = temp.y / temp.x\n",
    "    temp = temp[['User_id', 'u35']]\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销优惠券中的最小用户-商家距离\n",
    "    temp = coupon_consume[coupon_consume.Distance != 11]\n",
    "    temp = temp.groupby('User_id').Distance.min().reset_index(name='u36')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销优惠券中的最大用户-商家距离\n",
    "    temp = coupon_consume[coupon_consume.Distance != 11]\n",
    "    temp = temp.groupby('User_id').Distance.max().reset_index(name='u37')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 优惠券类型\n",
    "    discount_types = [\n",
    "        '0.2', '0.5', '0.6', '0.7', '0.75', '0.8', '0.85', '0.9', '0.95', '30:20', '50:30', '10:5',\n",
    "        '20:10', '100:50', '200:100', '50:20', '30:10', '150:50', '100:30', '20:5', '200:50', '5:1',\n",
    "        '50:10', '100:20', '150:30', '30:5', '300:50', '200:30', '150:20', '10:1', '50:5', '100:10',\n",
    "        '200:20', '300:30', '150:10', '300:20', '500:30', '20:1', '100:5', '200:10', '30:1', '150:5',\n",
    "        '300:10', '200:5', '50:1', '100:1',\n",
    "    ]\n",
    "    X['discount_type'] = -1\n",
    "    for k, v in enumerate(discount_types):\n",
    "        X.loc[X.Discount_rate == v, 'discount_type'] = k\n",
    "\n",
    "    # 不同优惠券领取次数\n",
    "    temp = offline.groupby(['User_id', 'Discount_rate']).size().reset_index(name='u41')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Discount_rate'])\n",
    "\n",
    "    # 不同优惠券使用次数\n",
    "    temp = coupon_consume.groupby(['User_id', 'Discount_rate']).size().reset_index(name='u42')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Discount_rate'])\n",
    "\n",
    "    # 不同优惠券不使用次数\n",
    "    temp = coupon_no_consume.groupby(['User_id', 'Discount_rate']).size().reset_index(name='u43')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Discount_rate'])\n",
    "\n",
    "    # 不同打折优惠券使用率\n",
    "    X['u44'] = X.u42 / X.u41\n",
    "\n",
    "    # 满减类型优惠券领取次数\n",
    "    temp = offline[offline.Discount_rate.str.contains(':') == True]\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='u48')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 打折类型优惠券领取次数\n",
    "    temp = offline[offline.Discount_rate.str.contains('\\.') == True]\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='u49')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    '''offline merchant features'''\n",
    "\n",
    "    # 商户消费次数\n",
    "    temp = offline[offline.Date != date_null].groupby('Merchant_id').size().reset_index(name='m0')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家优惠券被领取后核销次数\n",
    "    temp = coupon_consume.groupby('Merchant_id').size().reset_index(name='m1')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商户正常消费笔数\n",
    "    X['m2'] = X.m0.fillna(0) - X.m1.fillna(0)\n",
    "\n",
    "    # 商家优惠券被领取次数\n",
    "    temp = offline[offline.Date_received != date_null].groupby('Merchant_id').size().reset_index(name='m3')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家优惠券被领取后核销率\n",
    "    X['m4'] = X.m1 / X.m3\n",
    "\n",
    "    # 商家优惠券被领取后不核销次数\n",
    "    temp = coupon_no_consume.groupby('Merchant_id').size().reset_index(name='m7')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商户当天优惠券领取次数\n",
    "    temp = X[X.Date_received != date_null]\n",
    "    temp = temp.groupby(['Merchant_id', 'Date_received']).size().reset_index(name='m5')\n",
    "    X = pd.merge(X, temp, how='left', on=['Merchant_id', 'Date_received'])\n",
    "\n",
    "    # 商户当天优惠券领取人数\n",
    "    temp = X[X.Date_received != date_null]\n",
    "    temp = temp.groupby(['User_id', 'Merchant_id', 'Date_received']).size().reset_index()\n",
    "    temp = temp.groupby(['Merchant_id', 'Date_received']).size().reset_index(name='m6')\n",
    "    X = pd.merge(X, temp, how='left', on=['Merchant_id', 'Date_received'])\n",
    "\n",
    "    # 商家优惠券核销的平均消费折率\n",
    "    temp = coupon_consume.groupby('Merchant_id').discount_rate.mean().reset_index(name='m8')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家优惠券核销的最小消费折率\n",
    "    temp = coupon_consume.groupby('Merchant_id').discount_rate.max().reset_index(name='m9')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家优惠券核销的最大消费折率\n",
    "    temp = coupon_consume.groupby('Merchant_id').discount_rate.min().reset_index(name='m10')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家优惠券核销不同的用户数量\n",
    "    temp = coupon_consume.groupby(['Merchant_id', 'User_id']).size()\n",
    "    temp = temp.groupby('Merchant_id').size().reset_index(name='m11')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家优惠券领取不同的用户数量\n",
    "    temp = offline[offline.Date_received != date_null].groupby(['Merchant_id', 'User_id']).size()\n",
    "    temp = temp.groupby('Merchant_id').size().reset_index(name='m12')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 核销商家优惠券的不同用户数量其占领取不同的用户比重\n",
    "    X['m13'] = X.m11 / X.m12\n",
    "\n",
    "    # 商家优惠券平均每个用户核销多少张\n",
    "    X['m14'] = X.m1 / X.m12\n",
    "\n",
    "    # 商家被核销过的不同优惠券数量\n",
    "    temp = coupon_consume.groupby(['Merchant_id', 'Coupon_id']).size()\n",
    "    temp = temp.groupby('Merchant_id').size().reset_index(name='m15')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家领取过的不同优惠券数量的比重\n",
    "    temp = offline[offline.Date_received != date_null].groupby(['Merchant_id', 'Coupon_id']).size()\n",
    "    temp = temp.groupby('Merchant_id').count().reset_index(name='m18')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家被核销过的不同优惠券数量占所有领取过的不同优惠券数量的比重\n",
    "    X['m19'] = X.m15 / X.m18\n",
    "\n",
    "    # 商家被核销优惠券的平均时间\n",
    "    temp = pd.merge(coupon_consume, coupon_consume.groupby('Merchant_id').Date.max().reset_index(name='max'))\n",
    "    temp = pd.merge(temp, temp.groupby('Merchant_id').Date.min().reset_index(name='min'))\n",
    "    temp = pd.merge(temp, temp.groupby('Merchant_id').size().reset_index(name='len'))\n",
    "    temp['m20'] = ((temp['max'] - temp['min']).dt.days / (temp['len'] - 1))\n",
    "    temp = temp.drop_duplicates('Merchant_id')\n",
    "    X = pd.merge(X, temp[['Merchant_id', 'm20']], how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家被核销优惠券中的用户-商家平均距离\n",
    "    temp = coupon_consume[coupon_consume.Distance != 11].groupby('Merchant_id').Distance\n",
    "    temp = pd.merge(temp.count().reset_index(name='x'), temp.sum().reset_index(name='y'), on='Merchant_id')\n",
    "    temp['m21'] = temp.y / temp.x\n",
    "    temp = temp[['Merchant_id', 'm21']]\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家被核销优惠券中的用户-商家最小距离\n",
    "    temp = coupon_consume[coupon_consume.Distance != 11]\n",
    "    temp = temp.groupby('Merchant_id').Distance.min().reset_index(name='m22')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家被核销优惠券中的用户-商家最大距离\n",
    "    temp = coupon_consume[coupon_consume.Distance != 11]\n",
    "    temp = temp.groupby('Merchant_id').Distance.max().reset_index(name='m23')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    \"\"\"offline coupon features\"\"\"\n",
    "\n",
    "    # 此优惠券一共发行多少张\n",
    "    temp = offline[offline.Coupon_id != 0].groupby('Coupon_id').size().reset_index(name='c1')\n",
    "    X = pd.merge(X, temp, how='left', on='Coupon_id')\n",
    "\n",
    "    # 此优惠券一共被使用多少张\n",
    "    temp = coupon_consume.groupby('Coupon_id').size().reset_index(name='c2')\n",
    "    X = pd.merge(X, temp, how='left', on='Coupon_id')\n",
    "\n",
    "    # 优惠券使用率\n",
    "    X['c3'] = X.c2 / X.c1\n",
    "\n",
    "    # 没有使用的数目\n",
    "    X['c4'] = X.c1 - X.c2\n",
    "\n",
    "    # 此优惠券在当天发行了多少张\n",
    "    temp = X.groupby(['Coupon_id', 'Date_received']).size().reset_index(name='c5')\n",
    "    X = pd.merge(X, temp, how='left', on=['Coupon_id', 'Date_received'])\n",
    "\n",
    "    # 优惠券类型(直接优惠为0, 满减为1)\n",
    "    X['c6'] = 0\n",
    "    X.loc[X.Discount_rate.str.contains(':') == True, 'c6'] = 1\n",
    "\n",
    "    # 不同打折优惠券领取次数\n",
    "    temp = offline.groupby('Discount_rate').size().reset_index(name='c8')\n",
    "    X = pd.merge(X, temp, how='left', on='Discount_rate')\n",
    "\n",
    "    # 不同打折优惠券使用次数\n",
    "    temp = coupon_consume.groupby('Discount_rate').size().reset_index(name='c9')\n",
    "    X = pd.merge(X, temp, how='left', on='Discount_rate')\n",
    "\n",
    "    # 不同打折优惠券不使用次数\n",
    "    temp = coupon_no_consume.groupby('Discount_rate').size().reset_index(name='c10')\n",
    "    X = pd.merge(X, temp, how='left', on='Discount_rate')\n",
    "\n",
    "    # 不同打折优惠券使用率\n",
    "    X['c11'] = X.c9 / X.c8\n",
    "\n",
    "    # 优惠券核销平均时间\n",
    "    temp = pd.merge(coupon_consume, coupon_consume.groupby('Coupon_id').Date.max().reset_index(name='max'))\n",
    "    temp = pd.merge(temp, temp.groupby('Coupon_id').Date.min().reset_index(name='min'))\n",
    "    temp = pd.merge(temp, temp.groupby('Coupon_id').size().reset_index(name='count'))\n",
    "    temp['c12'] = ((temp['max'] - temp['min']).dt.days / (temp['count'] - 1))\n",
    "    temp = temp.drop_duplicates('Coupon_id')\n",
    "    X = pd.merge(X, temp[['Coupon_id', 'c12']], how='left', on='Coupon_id')\n",
    "\n",
    "    '''user merchant feature'''\n",
    "\n",
    "    # 用户领取商家的优惠券次数\n",
    "    temp = offline[offline.Coupon_id != 0]\n",
    "    temp = temp.groupby(['User_id', 'Merchant_id']).size().reset_index(name='um1')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Merchant_id'])\n",
    "\n",
    "    # 用户领取商家的优惠券后不核销次数\n",
    "    temp = coupon_no_consume.groupby(['User_id', 'Merchant_id']).size().reset_index(name='um2')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Merchant_id'])\n",
    "\n",
    "    # 用户领取商家的优惠券后核销次数\n",
    "    temp = coupon_consume.groupby(['User_id', 'Merchant_id']).size().reset_index(name='um3')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Merchant_id'])\n",
    "\n",
    "    # 用户领取商家的优惠券后核销率\n",
    "    X['um4'] = X.um3 / X.um1\n",
    "\n",
    "    # 用户对每个商家的不核销次数占用户总的不核销次数的比重\n",
    "    temp = coupon_no_consume.groupby('User_id').size().reset_index(name='temp')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "    X['um5'] = X.um2 / X.temp\n",
    "    X.drop(columns='temp', inplace=True)\n",
    "\n",
    "    # 用户在商店总共消费过几次\n",
    "    temp = offline[offline.Date != date_null].groupby(['User_id', 'Merchant_id']).size().reset_index(name='um6')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Merchant_id'])\n",
    "\n",
    "    # 用户在商店普通消费次数\n",
    "    temp = offline[(offline.Coupon_id == 0) & (offline.Date != date_null)]\n",
    "    temp = temp.groupby(['User_id', 'Merchant_id']).size().reset_index(name='um7')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Merchant_id'])\n",
    "\n",
    "    # 用户当天在此商店领取的优惠券数目\n",
    "    temp = offline[offline.Date_received != date_null]\n",
    "    temp = temp.groupby(['User_id', 'Merchant_id', 'Date_received']).size().reset_index(name='um8')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Merchant_id', 'Date_received'])\n",
    "\n",
    "    # 用户领取优惠券不同商家数量\n",
    "    temp = offline[offline.Coupon_id == offline.Coupon_id]\n",
    "    temp = temp.groupby(['User_id', 'Merchant_id']).size().reset_index()\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='um9')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销优惠券不同商家数量\n",
    "    temp = coupon_consume.groupby(['User_id', 'Merchant_id']).size()\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='um10')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销过优惠券的不同商家数量占所有不同商家的比重\n",
    "    X['um11'] = X.um10 / X.um9\n",
    "\n",
    "    # 用户平均核销每个商家多少张优惠券\n",
    "    X['um12'] = X.u2 / X.um9\n",
    "\n",
    "    '''other feature'''\n",
    "\n",
    "    # 用户领取的所有优惠券数目\n",
    "    temp = X.groupby('User_id').size().reset_index(name='o1')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户领取的特定优惠券数目\n",
    "    temp = X.groupby(['User_id', 'Coupon_id']).size().reset_index(name='o2')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Coupon_id'])\n",
    "\n",
    "    # multiple threads\n",
    "    # data split\n",
    "    stop = len(X)\n",
    "    step = int(ceil(stop / cpu_jobs))\n",
    "\n",
    "    X_chunks = [X[i:i + step] for i in range(0, stop, step)]\n",
    "    X_list = [X] * cpu_jobs\n",
    "    counters = [i for i in range(cpu_jobs)]\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    with ProcessPoolExecutor() as e:\n",
    "        X = pd.concat(e.map(task, X_chunks, X_list, counters))\n",
    "        print('time:', str(datetime.datetime.now() - start).split('.')[0])\n",
    "    # multiple threads\n",
    "\n",
    "    # 用户领取优惠券平均时间间隔\n",
    "    temp = pd.merge(X, X.groupby('User_id').Date_received.max().reset_index(name='max'))\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').Date_received.min().reset_index(name='min'))\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').size().reset_index(name='len'))\n",
    "    temp['o7'] = ((temp['max'] - temp['min']).dt.days / (temp['len'] - 1))\n",
    "    temp = temp.drop_duplicates('User_id')\n",
    "    X = pd.merge(X, temp[['User_id', 'o7']], how='left', on='User_id')\n",
    "\n",
    "    # 用户领取特定商家的优惠券数目\n",
    "    temp = X.groupby(['User_id', 'Merchant_id']).size().reset_index(name='o8')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Merchant_id'])\n",
    "\n",
    "    # 用户领取的不同商家数目\n",
    "    temp = X.groupby(['User_id', 'Merchant_id']).size()\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='o9')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户当天领取的优惠券数目\n",
    "    temp = X.groupby(['User_id', 'Date_received']).size().reset_index(name='o10')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Date_received'])\n",
    "\n",
    "    # 用户当天领取的特定优惠券数目\n",
    "    temp = X.groupby(['User_id', 'Coupon_id', 'Date_received']).size().reset_index(name='o11')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Coupon_id', 'Date_received'])\n",
    "\n",
    "    # 用户领取的所有优惠券种类数目\n",
    "    temp = X.groupby(['User_id', 'Coupon_id']).size()\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='o12')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 商家被领取的优惠券数目\n",
    "    temp = X.groupby('Merchant_id').size().reset_index(name='o13')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家被领取的特定优惠券数目\n",
    "    temp = X.groupby(['Merchant_id', 'Coupon_id']).size().reset_index(name='o14')\n",
    "    X = pd.merge(X, temp, how='left', on=['Merchant_id', 'Coupon_id'])\n",
    "\n",
    "    # 商家被多少不同用户领取的数目\n",
    "    temp = X.groupby(['Merchant_id', 'User_id']).size()\n",
    "    temp = temp.groupby('Merchant_id').size().reset_index(name='o15')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家发行的所有优惠券种类数目\n",
    "    temp = X.groupby(['Merchant_id', 'Coupon_id']).size()\n",
    "    temp = temp.groupby('Merchant_id').size().reset_index(name='o16')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    print(len(X), len(X.columns))\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_online_features(online, X):\n",
    "    # temp = online[online.Coupon_id == online.Coupon_id]\n",
    "    # coupon_consume = temp[temp.Date == temp.Date]\n",
    "    # coupon_no_consume = temp[temp.Date != temp.Date]\n",
    "\n",
    "    # 用户线上操作次数\n",
    "    temp = online.groupby('User_id').size().reset_index(name='on_u1')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户线上点击次数\n",
    "    temp = online[online.Action == 0].groupby('User_id').size().reset_index(name='on_u2')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户线上点击率\n",
    "    X['on_u3'] = X.on_u2 / X.on_u1\n",
    "\n",
    "    # 用户线上购买次数\n",
    "    temp = online[online.Action == 1].groupby('User_id').size().reset_index(name='on_u4')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户线上购买率\n",
    "    X['on_u5'] = X.on_u4 / X.on_u1\n",
    "\n",
    "    # 用户线上领取次数\n",
    "    temp = online[online.Coupon_id != 0].groupby('User_id').size().reset_index(name='on_u6')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户线上领取率\n",
    "    X['on_u7'] = X.on_u6 / X.on_u1\n",
    "\n",
    "    # 用户线上不消费次数\n",
    "    temp = online[(online.Date == date_null) & (online.Coupon_id != 0)]\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='on_u8')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户线上优惠券核销次数\n",
    "    temp = online[(online.Date != date_null) & (online.Coupon_id != 0)]\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='on_u9')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户线上优惠券核销率\n",
    "    X['on_u10'] = X.on_u9 / X.on_u6\n",
    "\n",
    "    # 用户线下不消费次数占线上线下总的不消费次数的比重\n",
    "    X['on_u11'] = X.u3 / (X.on_u8 + X.u3)\n",
    "\n",
    "    # 用户线下的优惠券核销次数占线上线下总的优惠券核销次数的比重\n",
    "    X['on_u12'] = X.u2 / (X.on_u9 + X.u2)\n",
    "\n",
    "    # 用户线下领取的记录数量占总的记录数量的比重\n",
    "    X['on_u13'] = X.u1 / (X.on_u6 + X.u1)\n",
    "\n",
    "    # # 消费优惠券的平均折率\n",
    "    # temp = coupon_consume.groupby('User_id').discount_rate.mean().reset_index(name='ou14')\n",
    "    # X = pd.merge(X, temp, how='left', on='User_id')\n",
    "    #\n",
    "    # # 用户核销优惠券的最低消费折率\n",
    "    # temp = coupon_consume.groupby('User_id').discount_rate.min().reset_index(name='ou15')\n",
    "    # X = pd.merge(X, temp, how='left', on='User_id')\n",
    "    #\n",
    "    # # 用户核销优惠券的最高消费折率\n",
    "    # temp = coupon_consume.groupby('User_id').discount_rate.max().reset_index(name='ou16')\n",
    "    # X = pd.merge(X, temp, how='left', on='User_id')\n",
    "    #\n",
    "    # # 不同打折优惠券领取次数\n",
    "    # temp = online.groupby('Discount_rate').size().reset_index(name='oc1')\n",
    "    # X = pd.merge(X, temp, how='left', on='Discount_rate')\n",
    "    #\n",
    "    # # 不同打折优惠券使用次数\n",
    "    # temp = coupon_consume.groupby('Discount_rate').size().reset_index(name='oc2')\n",
    "    # X = pd.merge(X, temp, how='left', on='Discount_rate')\n",
    "    #\n",
    "    # # 不同打折优惠券不使用次数\n",
    "    # temp = coupon_no_consume.groupby('Discount_rate').size().reset_index(name='oc3')\n",
    "    # X = pd.merge(X, temp, how='left', on='Discount_rate')\n",
    "    #\n",
    "    # # 不同打折优惠券使用率\n",
    "    # X['oc4'] = X.oc2 / X.oc1\n",
    "\n",
    "    print(len(X), len(X.columns))\n",
    "    print('----------')\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_train_data():\n",
    "    path = 'cache_%s_train.csv' % os.path.basename(__file__)\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        data = pd.read_csv(path)\n",
    "    else:\n",
    "        offline, online = get_preprocess_data()\n",
    "\n",
    "        # date received 2016-01-01 - 2016-06-15\n",
    "        # date consumed 2016-01-01 - 2016-06-30\n",
    "\n",
    "        # train data 1\n",
    "        # 2016-04-16 ~ 2016-05-15\n",
    "        data_1 = offline[('2016-04-16' <= offline.Date_received) & (offline.Date_received <= '2016-05-15')].copy()\n",
    "        data_1['label'] = 0\n",
    "        data_1.loc[\n",
    "            (data_1.Date != date_null) & (data_1.Date - data_1.Date_received <= datetime.timedelta(15)), 'label'] = 1\n",
    "\n",
    "        # feature data 1\n",
    "        # 领券 2016-01-01 ~ 2016-03-31\n",
    "        end = '2016-03-31'\n",
    "        data_off_1 = offline[offline.Date_received <= end]\n",
    "        data_on_1 = online[online.Date_received <= end]\n",
    "\n",
    "        # 普通消费 2016-01-01 ~ 2016-04-15\n",
    "        end = '2016-04-15'\n",
    "        data_off_2 = offline[(offline.Coupon_id == 0) & (offline.Date <= end)]\n",
    "        data_on_2 = online[(online.Coupon_id == 0) & (online.Date <= end)]\n",
    "\n",
    "        data_1 = get_offline_features(data_1, pd.concat([data_off_1, data_off_2]))\n",
    "        data_1 = get_online_features(pd.concat([data_on_1, data_on_2]), data_1)\n",
    "\n",
    "        # train data 2\n",
    "        # 2016-05-16 ~ 2016-06-15\n",
    "        data_2 = offline['2016-05-16' <= offline.Date_received].copy()\n",
    "        data_2['label'] = 0\n",
    "        data_2.loc[\n",
    "            (data_2.Date != date_null) & (data_2.Date - data_2.Date_received <= datetime.timedelta(15)), 'label'] = 1\n",
    "\n",
    "        # feature data 2\n",
    "        # 领券\n",
    "        start = '2016-02-01'\n",
    "        end = '2016-04-30'\n",
    "        data_off_1 = offline[(start <= offline.Date_received) & (offline.Date_received <= end)]\n",
    "        data_on_1 = online[(start <= online.Date_received) & (online.Date_received <= end)]\n",
    "\n",
    "        # 普通消费\n",
    "        start = '2016-02-01'\n",
    "        end = '2016-05-15'\n",
    "        data_off_2 = offline[(offline.Coupon_id == 0) & (start <= offline.Date) & (offline.Date <= end)]\n",
    "        data_on_2 = online[(online.Coupon_id == 0) & (start <= online.Date) & (online.Date <= end)]\n",
    "\n",
    "        data_2 = get_offline_features(data_2, pd.concat([data_off_1, data_off_2]))\n",
    "        data_2 = get_online_features(pd.concat([data_on_1, data_on_2]), data_2)\n",
    "\n",
    "        data = pd.concat([data_1, data_2])\n",
    "\n",
    "        # undersampling\n",
    "        # if undersampling:\n",
    "        #     temp = X_1[X_1.label == 1].groupby('User_id').size().reset_index()\n",
    "        #     temp = X_1[X_1.User_id.isin(temp.User_id)]\n",
    "        #     X_1 = pd.concat([temp, X_1[~X_1.User_id.isin(temp.User_id)].sample(4041)])\n",
    "\n",
    "        # data.drop_duplicates(inplace=True)\n",
    "        drop_columns(data)\n",
    "        data.fillna(0, inplace=True)\n",
    "        data.to_csv(path, index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def analysis():\n",
    "    offline, online = get_preprocess_data()\n",
    "\n",
    "    # t = offline.groupby('Discount_rate').size().reset_index(name='receive_count')\n",
    "    # t1 = offline[(offline.Coupon_id != 0) & (offline.Date != date_null)]\n",
    "    # t1 = t1.groupby('Discount_rate').size().reset_index(name='consume_count')\n",
    "    # t = pd.merge(t, t1, on='Discount_rate')\n",
    "    # t['consume_rate'] = t.consume_count / t.receive_count\n",
    "\n",
    "    # t = offline.groupby('Merchant_id').size().reset_index(name='receive_count')\n",
    "    # t1 = offline[(offline.Coupon_id != 0) & (offline.Date != date_null)]\n",
    "    # t1 = t1.groupby('Merchant_id').size().reset_index(name='consume_count')\n",
    "    # t = pd.merge(t, t1, on='Merchant_id')\n",
    "    # t['consume_rate'] = t.consume_count / t.receive_count\n",
    "\n",
    "    t = offline.groupby('Distance').size().reset_index(name='receive_count')\n",
    "    t1 = offline[(offline.Coupon_id != 0) & (offline.Date != date_null)]\n",
    "    t1 = t1.groupby('Distance').size().reset_index(name='consume_count')\n",
    "    t = pd.merge(t, t1, on='Distance')\n",
    "    t['consume_rate'] = t.consume_count / t.receive_count\n",
    "\n",
    "    t.to_csv('note.csv')\n",
    "\n",
    "    # plt.bar(temp.Discount_rate.values, temp.total.values)\n",
    "    # plt.bar(range(num), y1, bottom=y2, fc='r')\n",
    "    # plt.show()\n",
    "\n",
    "    exit()\n",
    "\n",
    "\n",
    "def detect_duplicate_columns():\n",
    "    X = get_train_data()\n",
    "    X = X[:1000]\n",
    "\n",
    "    for index1 in range(len(X.columns) - 1):\n",
    "        for index2 in range(index1 + 1, len(X.columns)):\n",
    "            column1 = X.columns[index1]\n",
    "            column2 = X.columns[index2]\n",
    "            X[column1] = X[column1].astype(str)\n",
    "            X[column2] = X[column2].astype(str)\n",
    "            temp = len(X[X[column1] == X[column2]])\n",
    "            if temp == len(X):\n",
    "                print(column1, column2, temp)\n",
    "    exit()\n",
    "\n",
    "\n",
    "def feature_importance_score():\n",
    "    clf = train_xgb()\n",
    "    fscores = pd.Series(clf.get_booster().get_fscore()).sort_values(ascending=False)\n",
    "    fscores.plot(kind='bar', title='Feature Importance')\n",
    "    plt.ylabel('Feature Importance Score')\n",
    "    plt.show()\n",
    "    exit()\n",
    "\n",
    "\n",
    "def feature_selection():\n",
    "    data = get_train_data()\n",
    "\n",
    "    train_data, test_data = train_test_split(data,\n",
    "                                             train_size=100000,\n",
    "                                             random_state=0\n",
    "                                             )\n",
    "\n",
    "    X = train_data.copy().drop(columns='Coupon_id')\n",
    "    y = X.pop('label')\n",
    "\n",
    "    # sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "    # X = sel.fit_transform(X)\n",
    "    # print(X.shape)\n",
    "    # Create the RFE object and rank each pixel\n",
    "\n",
    "\n",
    "def fit_eval_metric(estimator, X, y, name=None):\n",
    "    if name is None:\n",
    "        name = estimator.__class__.__name__\n",
    "\n",
    "    if name is 'XGBClassifier' or name is 'LGBMClassifier':\n",
    "        estimator.fit(X, y, eval_metric='auc')\n",
    "    else:\n",
    "        estimator.fit(X, y)\n",
    "\n",
    "    return estimator\n",
    "\n",
    "\n",
    "def grid_search(estimator, param_grid):\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    print('--------------------------------------------')\n",
    "    print(start.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    print(param_grid)\n",
    "    print()\n",
    "\n",
    "    data = get_train_data()\n",
    "\n",
    "    data, _ = train_test_split(data, train_size=100000, random_state=0)\n",
    "\n",
    "    X = data.copy().drop(columns='Coupon_id')\n",
    "    y = X.pop('label')\n",
    "\n",
    "    estimator_name = estimator.__class__.__name__\n",
    "    n_jobs = cpu_jobs\n",
    "    if estimator_name is 'XGBClassifier' or estimator_name is 'LGBMClassifier' or estimator_name is 'CatBoostClassifier':\n",
    "        n_jobs = 1\n",
    "\n",
    "    clf = GridSearchCV(estimator=estimator, param_grid=param_grid, scoring='roc_auc', n_jobs=n_jobs\n",
    "                       # cv=5\n",
    "                       )\n",
    "\n",
    "    clf = fit_eval_metric(clf, X, y, estimator_name)\n",
    "\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print('%0.5f (+/-%0.05f) for %r' % (mean, std * 2, params))\n",
    "    print()\n",
    "    print('best params', clf.best_params_)\n",
    "    print('best score', clf.best_score_)\n",
    "    print('time: %s' % str((datetime.datetime.now() - start)).split('.')[0])\n",
    "    print()\n",
    "\n",
    "    return clf.best_params_, clf.best_score_\n",
    "\n",
    "\n",
    "def grid_search_auto(steps, params, estimator):\n",
    "    global log\n",
    "\n",
    "    old_params = params.copy()\n",
    "\n",
    "    while 1:\n",
    "        for name, step in steps.items():\n",
    "            score = 0\n",
    "\n",
    "            start = params[name] - step['step']\n",
    "            if start <= step['min']:\n",
    "                start = step['min']\n",
    "\n",
    "            stop = params[name] + step['step']\n",
    "            if step['max'] != 'inf' and stop >= step['max']:\n",
    "                stop = step['max']\n",
    "\n",
    "            while 1:\n",
    "\n",
    "                if str(step['step']).count('.') == 1:\n",
    "                    stop += step['step'] / 10\n",
    "                else:\n",
    "                    stop += step['step']\n",
    "\n",
    "                param_grid = {\n",
    "                    name: np.arange(start, stop, step['step']),\n",
    "                }\n",
    "\n",
    "                best_params, best_score = grid_search(estimator.set_params(**params), param_grid)\n",
    "\n",
    "                if best_params[name] == params[name] or score > best_score:\n",
    "                    print(estimator.__class__.__name__, params)\n",
    "                    break\n",
    "\n",
    "                direction = (best_params[name] - params[name]) // abs(best_params[name] - params[name])\n",
    "                start = stop = best_params[name] + step['step'] * direction\n",
    "\n",
    "                score = best_score\n",
    "                params[name] = best_params[name]\n",
    "                print(estimator.__class__.__name__, params)\n",
    "\n",
    "                if best_params[name] - step['step'] < step['min'] or (\n",
    "                        step['max'] != 'inf' and best_params[name] + step['step'] > step['max']):\n",
    "                    break\n",
    "\n",
    "        if old_params == params:\n",
    "            break\n",
    "        old_params = params\n",
    "        print('--------------------------------------------')\n",
    "        print('new grid search')\n",
    "\n",
    "    print('--------------------------------------------')\n",
    "    log += 'grid search: %s\\n%r\\n' % (estimator.__class__.__name__, params)\n",
    "\n",
    "\n",
    "def grid_search_gbdt(get_param=False):\n",
    "    params = {\n",
    "        # 10\n",
    "        'learning_rate': 1e-2,\n",
    "        'n_estimators': 1900,\n",
    "        'max_depth': 9,\n",
    "        'min_samples_split': 200,\n",
    "        'min_samples_leaf': 50,\n",
    "        'subsample': .8,\n",
    "\n",
    "        # 'learning_rate': 1e-1,\n",
    "        # 'n_estimators': 200,\n",
    "        # 'max_depth': 8,\n",
    "        # 'min_samples_split': 200,\n",
    "        # 'min_samples_leaf': 50,\n",
    "        # 'subsample': .8,\n",
    "\n",
    "        'random_state': 0\n",
    "    }\n",
    "\n",
    "    if get_param:\n",
    "        return params\n",
    "\n",
    "    steps = {\n",
    "        'n_estimators': {'step': 100, 'min': 1, 'max': 'inf'},\n",
    "        'max_depth': {'step': 1, 'min': 1, 'max': 'inf'},\n",
    "        'min_samples_split': {'step': 10, 'min': 2, 'max': 'inf'},\n",
    "        'min_samples_leaf': {'step': 10, 'min': 1, 'max': 'inf'},\n",
    "        'subsample': {'step': .1, 'min': .1, 'max': 1},\n",
    "    }\n",
    "\n",
    "    grid_search_auto(steps, params, GradientBoostingClassifier())\n",
    "\n",
    "\n",
    "def grid_search_xgb(get_param=False):\n",
    "    params = {\n",
    "        # all\n",
    "        # 'learning_rate': 1e-1,\n",
    "        # 'n_estimators': 80,\n",
    "        # 'max_depth': 8,\n",
    "        # 'min_child_weight': 3,\n",
    "        # 'gamma': .2,\n",
    "        # 'subsample': .8,\n",
    "        # 'colsample_bytree': .8,\n",
    "\n",
    "        # 10\n",
    "        'learning_rate': 1e-2,\n",
    "        'n_estimators': 1260,\n",
    "        'max_depth': 8,\n",
    "        'min_child_weight': 4,\n",
    "        'gamma': .2,\n",
    "        'subsample': .6,\n",
    "        'colsample_bytree': .8,\n",
    "        'scale_pos_weight': 1,\n",
    "        'reg_alpha': 0,\n",
    "\n",
    "        # 'learning_rate': 1e-1,\n",
    "        # 'n_estimators': 80,\n",
    "        # 'max_depth': 8,\n",
    "        # 'min_child_weight': 3,\n",
    "        # 'gamma': .2,\n",
    "        # 'subsample': .8,\n",
    "        # 'colsample_bytree': .8,\n",
    "        # 'scale_pos_weight': 1,\n",
    "        # 'reg_alpha': 0,\n",
    "\n",
    "        'n_jobs': cpu_jobs,\n",
    "        'seed': 0\n",
    "    }\n",
    "\n",
    "    if get_param:\n",
    "        return params\n",
    "\n",
    "    steps = {\n",
    "        'n_estimators': {'step': 10, 'min': 1, 'max': 'inf'},\n",
    "        'max_depth': {'step': 1, 'min': 1, 'max': 'inf'},\n",
    "        'min_child_weight': {'step': 1, 'min': 1, 'max': 'inf'},\n",
    "        'gamma': {'step': .1, 'min': 0, 'max': 1},\n",
    "        'subsample': {'step': .1, 'min': .1, 'max': 1},\n",
    "        'colsample_bytree': {'step': .1, 'min': .1, 'max': 1},\n",
    "        'scale_pos_weight': {'step': 1, 'min': 1, 'max': 10},\n",
    "        'reg_alpha': {'step': .1, 'min': 0, 'max': 1},\n",
    "    }\n",
    "\n",
    "    grid_search_auto(steps, params, XGBClassifier())\n",
    "\n",
    "\n",
    "def grid_search_lgb(get_param=False):\n",
    "    params = {\n",
    "        # 10\n",
    "        'learning_rate': 1e-2,\n",
    "        'n_estimators': 1200,\n",
    "        'num_leaves': 51,\n",
    "        'min_split_gain': 0,\n",
    "        'min_child_weight': 1e-3,\n",
    "        'min_child_samples': 22,\n",
    "        'subsample': .8,\n",
    "        'colsample_bytree': .8,\n",
    "\n",
    "        # 'learning_rate': .1,\n",
    "        # 'n_estimators': 90,\n",
    "        # 'num_leaves': 50,\n",
    "        # 'min_split_gain': 0,\n",
    "        # 'min_child_weight': 1e-3,\n",
    "        # 'min_child_samples': 21,\n",
    "        # 'subsample': .8,\n",
    "        # 'colsample_bytree': .8,\n",
    "\n",
    "        'n_jobs': cpu_jobs,\n",
    "        'random_state': 0\n",
    "    }\n",
    "\n",
    "    if get_param:\n",
    "        return params\n",
    "\n",
    "    steps = {\n",
    "        'n_estimators': {'step': 100, 'min': 1, 'max': 'inf'},\n",
    "        'num_leaves': {'step': 1, 'min': 1, 'max': 'inf'},\n",
    "        'min_split_gain': {'step': .1, 'min': 0, 'max': 1},\n",
    "        'min_child_weight': {'step': 1e-3, 'min': 1e-3, 'max': 'inf'},\n",
    "        'min_child_samples': {'step': 1, 'min': 1, 'max': 'inf'},\n",
    "        # 'subsample': {'step': .1, 'min': .1, 'max': 1},\n",
    "        'colsample_bytree': {'step': .1, 'min': .1, 'max': 1},\n",
    "    }\n",
    "\n",
    "    grid_search_auto(steps, params, LGBMClassifier())\n",
    "\n",
    "\n",
    "def grid_search_cat(get_param=False):\n",
    "    params = {\n",
    "        # 10\n",
    "        'learning_rate': 1e-2,\n",
    "        'n_estimators': 3600,\n",
    "        'max_depth': 8,\n",
    "        'max_bin': 127,\n",
    "        'reg_lambda': 2,\n",
    "        'subsample': .7,\n",
    "\n",
    "        # 'learning_rate': 1e-1,\n",
    "        # 'iterations': 460,\n",
    "        # 'depth': 8,\n",
    "        # 'l2_leaf_reg': 8,\n",
    "        # 'border_count': 37,\n",
    "\n",
    "        # 'ctr_border_count': 16,\n",
    "        'one_hot_max_size': 2,\n",
    "        'bootstrap_type': 'Bernoulli',\n",
    "        'leaf_estimation_method': 'Newton',\n",
    "        'random_state': 0,\n",
    "        'verbose': False,\n",
    "        'eval_metric': 'AUC',\n",
    "        'thread_count': cpu_jobs\n",
    "    }\n",
    "\n",
    "    if get_param:\n",
    "        return params\n",
    "\n",
    "    steps = {\n",
    "        'n_estimators': {'step': 100, 'min': 1, 'max': 'inf'},\n",
    "        'max_depth': {'step': 1, 'min': 1, 'max': 'inf'},\n",
    "        'max_bin': {'step': 1, 'min': 1, 'max': 255},\n",
    "        'reg_lambda': {'step': 1, 'min': 0, 'max': 'inf'},\n",
    "        'subsample': {'step': .1, 'min': .1, 'max': 1},\n",
    "        'one_hot_max_size': {'step': 1, 'min': 0, 'max': 255},\n",
    "    }\n",
    "\n",
    "    grid_search_auto(steps, params, CatBoostClassifier())\n",
    "\n",
    "\n",
    "def grid_search_rf(criterion='gini', get_param=False):\n",
    "    if criterion == 'gini':\n",
    "        params = {\n",
    "            # 10\n",
    "            'n_estimators': 3090,\n",
    "            'max_depth': 15,\n",
    "            'min_samples_split': 2,\n",
    "            'min_samples_leaf': 1,\n",
    "\n",
    "            'criterion': 'gini',\n",
    "            'random_state': 0\n",
    "        }\n",
    "    else:\n",
    "        params = {\n",
    "            'n_estimators': 3110,\n",
    "            'max_depth': 13,\n",
    "            'min_samples_split': 70,\n",
    "            'min_samples_leaf': 10,\n",
    "            'criterion': 'entropy',\n",
    "            'random_state': 0\n",
    "        }\n",
    "\n",
    "    if get_param:\n",
    "        return params\n",
    "\n",
    "    steps = {\n",
    "        'n_estimators': {'step': 10, 'min': 1, 'max': 'inf'},\n",
    "        'max_depth': {'step': 1, 'min': 1, 'max': 'inf'},\n",
    "        'min_samples_split': {'step': 2, 'min': 2, 'max': 'inf'},\n",
    "        'min_samples_leaf': {'step': 2, 'min': 1, 'max': 'inf'},\n",
    "    }\n",
    "\n",
    "    grid_search_auto(steps, params, RandomForestClassifier())\n",
    "\n",
    "\n",
    "def grid_search_et(criterion='gini', get_param=False):\n",
    "    if criterion == 'gini':\n",
    "        params = {\n",
    "            # 10\n",
    "            'n_estimators': 3060,\n",
    "            'max_depth': 22,\n",
    "            'min_samples_split': 12,\n",
    "            'min_samples_leaf': 1,\n",
    "\n",
    "            'criterion': 'gini',\n",
    "            'random_state': 0,\n",
    "        }\n",
    "    else:\n",
    "        params = {\n",
    "            'n_estimators': 3100,\n",
    "            'max_depth': 13,\n",
    "            'min_samples_split': 70,\n",
    "            'min_samples_leaf': 10,\n",
    "            'criterion': 'entropy',\n",
    "            'random_state': 0\n",
    "        }\n",
    "\n",
    "    if get_param:\n",
    "        return params\n",
    "\n",
    "    steps = {\n",
    "        'n_estimators': {'step': 10, 'min': 1, 'max': 'inf'},\n",
    "        'max_depth': {'step': 1, 'min': 1, 'max': 'inf'},\n",
    "        'min_samples_split': {'step': 2, 'min': 2, 'max': 'inf'},\n",
    "        'min_samples_leaf': {'step': 2, 'min': 1, 'max': 'inf'},\n",
    "    }\n",
    "\n",
    "    grid_search_auto(steps, params, ExtraTreesClassifier())\n",
    "\n",
    "\n",
    "def train_gbdt(model=False):\n",
    "    global log\n",
    "\n",
    "    params = grid_search_gbdt(True)\n",
    "    clf = GradientBoostingClassifier().set_params(**params)\n",
    "\n",
    "    if model:\n",
    "        return clf\n",
    "\n",
    "    params = clf.get_params()\n",
    "    log += 'gbdt'\n",
    "    log += ', learning_rate: %.3f' % params['learning_rate']\n",
    "    log += ', n_estimators: %d' % params['n_estimators']\n",
    "    log += ', max_depth: %d' % params['max_depth']\n",
    "    log += ', min_samples_split: %d' % params['min_samples_split']\n",
    "    log += ', min_samples_leaf: %d' % params['min_samples_leaf']\n",
    "    log += ', subsample: %.1f' % params['subsample']\n",
    "    log += '\\n\\n'\n",
    "\n",
    "    return train(clf)\n",
    "\n",
    "\n",
    "def train_xgb(model=False):\n",
    "    global log\n",
    "\n",
    "    params = grid_search_xgb(True)\n",
    "\n",
    "    clf = XGBClassifier().set_params(**params)\n",
    "\n",
    "    if model:\n",
    "        return clf\n",
    "\n",
    "    params = clf.get_params()\n",
    "    log += 'xgb'\n",
    "    log += ', learning_rate: %.3f' % params['learning_rate']\n",
    "    log += ', n_estimators: %d' % params['n_estimators']\n",
    "    log += ', max_depth: %d' % params['max_depth']\n",
    "    log += ', min_child_weight: %d' % params['min_child_weight']\n",
    "    log += ', gamma: %.1f' % params['gamma']\n",
    "    log += ', subsample: %.1f' % params['subsample']\n",
    "    log += ', colsample_bytree: %.1f' % params['colsample_bytree']\n",
    "    log += '\\n\\n'\n",
    "\n",
    "    return train(clf)\n",
    "\n",
    "\n",
    "def train_lgb(model=False):\n",
    "    global log\n",
    "\n",
    "    params = grid_search_lgb(True)\n",
    "\n",
    "    clf = LGBMClassifier().set_params(**params)\n",
    "\n",
    "    if model:\n",
    "        return clf\n",
    "\n",
    "    params = clf.get_params()\n",
    "    log += 'lgb'\n",
    "    log += ', learning_rate: %.3f' % params['learning_rate']\n",
    "    log += ', n_estimators: %d' % params['n_estimators']\n",
    "    log += ', num_leaves: %d' % params['num_leaves']\n",
    "    log += ', min_split_gain: %.1f' % params['min_split_gain']\n",
    "    log += ', min_child_weight: %.4f' % params['min_child_weight']\n",
    "    log += ', min_child_samples: %d' % params['min_child_samples']\n",
    "    log += ', subsample: %.1f' % params['subsample']\n",
    "    log += ', colsample_bytree: %.1f' % params['colsample_bytree']\n",
    "    log += '\\n\\n'\n",
    "\n",
    "    return train(clf)\n",
    "\n",
    "\n",
    "def train_cat(model=False):\n",
    "    global log\n",
    "\n",
    "    params = grid_search_cat(True)\n",
    "\n",
    "    clf = CatBoostClassifier().set_params(**params)\n",
    "\n",
    "    if model:\n",
    "        return clf\n",
    "\n",
    "    params = clf.get_params()\n",
    "    log += 'cat'\n",
    "    log += ', learning_rate: %.3f' % params['learning_rate']\n",
    "    log += ', iterations: %d' % params['iterations']\n",
    "    log += ', depth: %d' % params['depth']\n",
    "    log += ', l2_leaf_reg: %d' % params['l2_leaf_reg']\n",
    "    log += ', border_count: %d' % params['border_count']\n",
    "    log += ', subsample: %d' % params['subsample']\n",
    "    log += ', one_hot_max_size: %d' % params['one_hot_max_size']\n",
    "    log += '\\n\\n'\n",
    "\n",
    "    return train(clf)\n",
    "\n",
    "\n",
    "def train_rf(clf):\n",
    "    global log\n",
    "\n",
    "    params = clf.get_params()\n",
    "    log += 'rf'\n",
    "    log += ', n_estimators: %d' % params['n_estimators']\n",
    "    log += ', max_depth: %d' % params['max_depth']\n",
    "    log += ', min_samples_split: %d' % params['min_samples_split']\n",
    "    log += ', min_samples_leaf: %d' % params['min_samples_leaf']\n",
    "    log += ', criterion: %s' % params['criterion']\n",
    "    log += '\\n\\n'\n",
    "\n",
    "    return train(clf)\n",
    "\n",
    "\n",
    "def train_rf_gini(model=False):\n",
    "    clf = RandomForestClassifier().set_params(**grid_search_rf('gini', True))\n",
    "    if model:\n",
    "        return clf\n",
    "    return train_rf(clf)\n",
    "\n",
    "\n",
    "def train_rf_entropy():\n",
    "    clf = RandomForestClassifier().set_params(**grid_search_rf('entropy', True))\n",
    "\n",
    "    return train_rf(clf)\n",
    "\n",
    "\n",
    "def train_et(clf):\n",
    "    global log\n",
    "\n",
    "    params = clf.get_params()\n",
    "    log += 'et'\n",
    "    log += ', n_estimators: %d' % params['n_estimators']\n",
    "    log += ', max_depth: %d' % params['max_depth']\n",
    "    log += ', min_samples_split: %d' % params['min_samples_split']\n",
    "    log += ', min_samples_leaf: %d' % params['min_samples_leaf']\n",
    "    log += ', criterion: %s' % params['criterion']\n",
    "    log += '\\n\\n'\n",
    "\n",
    "    return train(clf)\n",
    "\n",
    "\n",
    "def train_et_gini(model=False):\n",
    "    clf = ExtraTreesClassifier().set_params(**grid_search_et('gini', True))\n",
    "    if model:\n",
    "        return clf\n",
    "    return train_et(clf)\n",
    "\n",
    "\n",
    "def train_et_entropy():\n",
    "    clf = ExtraTreesClassifier().set_params(**{\n",
    "        'n_estimators': 3100,\n",
    "        'max_depth': 13,\n",
    "        'min_samples_split': 70,\n",
    "        'min_samples_leaf': 10,\n",
    "        'criterion': 'entropy',\n",
    "        'random_state': 0\n",
    "    })\n",
    "\n",
    "    return train_et(clf)\n",
    "\n",
    "\n",
    "def train(clf):\n",
    "    global log\n",
    "\n",
    "    data = get_train_data()\n",
    "\n",
    "    train_data, test_data = train_test_split(data,\n",
    "                                             train_size=100000,\n",
    "                                             random_state=0\n",
    "                                             )\n",
    "\n",
    "    _, test_data = train_test_split(data, random_state=0)\n",
    "\n",
    "    X_train = train_data.copy().drop(columns='Coupon_id')\n",
    "    y_train = X_train.pop('label')\n",
    "\n",
    "    clf = fit_eval_metric(clf, X_train, y_train)\n",
    "\n",
    "    X_test = test_data.copy().drop(columns='Coupon_id')\n",
    "    y_test = X_test.pop('label')\n",
    "\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    # log += '%s\\n' % classification_report(y_test, y_pred)\n",
    "    log += '  accuracy: %f\\n' % accuracy_score(y_true, y_pred)\n",
    "    y_score = clf.predict_proba(X_test)[:, 1]\n",
    "    log += '       auc: %f\\n' % roc_auc_score(y_true, y_score)\n",
    "\n",
    "    # coupon average auc\n",
    "    coupons = test_data.groupby('Coupon_id').size().reset_index(name='total')\n",
    "    aucs = []\n",
    "    for _, coupon in coupons.iterrows():\n",
    "        if coupon.total > 1:\n",
    "            X_test = test_data[test_data.Coupon_id == coupon.Coupon_id].copy()\n",
    "            X_test.drop(columns='Coupon_id', inplace=True)\n",
    "\n",
    "            if len(X_test.label.unique()) != 2:\n",
    "                continue\n",
    "\n",
    "            y_true = X_test.pop('label')\n",
    "            y_score = clf.predict_proba(X_test)[:, 1]\n",
    "            aucs.append(roc_auc_score(y_true, y_score))\n",
    "\n",
    "    log += 'coupon auc: %f\\n\\n' % np.mean(aucs)\n",
    "\n",
    "    return clf\n",
    "\n",
    "\n",
    "def predict(model):\n",
    "    path = 'cache_%s_predict.csv' % os.path.basename(__file__)\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        X = pd.read_csv(path, parse_dates=['Date_received'])\n",
    "    else:\n",
    "        offline, online = get_preprocess_data()\n",
    "\n",
    "        # 2016-03-16 ~ 2016-06-30\n",
    "        start = '2016-03-16'\n",
    "        offline = offline[(offline.Coupon_id == 0) & (start <= offline.Date) | (start <= offline.Date_received)]\n",
    "        online = online[(online.Coupon_id == 0) & (start <= online.Date) | (start <= online.Date_received)]\n",
    "\n",
    "        X = get_preprocess_data(True)\n",
    "        X = get_offline_features(X, offline)\n",
    "        X = get_online_features(online, X)\n",
    "        X.drop_duplicates(inplace=True)\n",
    "        X.fillna(0, inplace=True)\n",
    "        X.to_csv(path, index=False)\n",
    "\n",
    "    sample_submission = X[['User_id', 'Coupon_id', 'Date_received']].copy()\n",
    "    sample_submission.Date_received = sample_submission.Date_received.dt.strftime('%Y%m%d')\n",
    "    drop_columns(X, True)\n",
    "\n",
    "    if model is 'blending':\n",
    "        predict = blending(X)\n",
    "    else:\n",
    "        clf = eval('train_%s' % model)()\n",
    "        predict = clf.predict_proba(X)[:, 1]\n",
    "\n",
    "    sample_submission['Probability'] = predict\n",
    "    sample_submission.to_csv('submission_%s.csv' % model,\n",
    "                             #  float_format='%.5f',\n",
    "                             index=False, header=False)\n",
    "\n",
    "\n",
    "def blending(predict_X=None):\n",
    "    global log\n",
    "    log += '\\n'\n",
    "\n",
    "    X = get_train_data().drop(columns='Coupon_id')\n",
    "    y = X.pop('label')\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    _, X_submission, _, y_test_blend = train_test_split(X, y,\n",
    "                                                        random_state=0\n",
    "                                                        )\n",
    "\n",
    "    if predict_X is not None:\n",
    "        X_submission = np.asarray(predict_X)\n",
    "\n",
    "    X, _, y, _ = train_test_split(X, y,\n",
    "                                  train_size=100000,\n",
    "                                  random_state=0\n",
    "                                  )\n",
    "\n",
    "    # np.random.seed(0)\n",
    "    # idx = np.random.permutation(y.size)\n",
    "    # X = X[idx]\n",
    "    # y = y[idx]\n",
    "\n",
    "    skf = StratifiedKFold()\n",
    "    clfs = ['gbdt', 'xgb', 'lgb', 'cat',\n",
    "            # 'rf_gini', 'et_gini'\n",
    "            ]\n",
    "\n",
    "    blend_X_train = np.zeros((X.shape[0], len(clfs)))\n",
    "    blend_X_test = np.zeros((X_submission.shape[0], len(clfs)))\n",
    "\n",
    "    for j, v in enumerate(clfs):\n",
    "        clf = eval('train_%s' % v)(True)\n",
    "\n",
    "        aucs = []\n",
    "        dataset_blend_test_j = []\n",
    "\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            clf = fit_eval_metric(clf, X_train, y_train)\n",
    "\n",
    "            y_submission = clf.predict_proba(X_test)[:, 1]\n",
    "            aucs.append(roc_auc_score(y_test, y_submission))\n",
    "\n",
    "            blend_X_train[test_index, j] = y_submission\n",
    "            dataset_blend_test_j.append(clf.predict_proba(X_submission)[:, 1])\n",
    "\n",
    "        log += '%7s' % v + ' auc: %f\\n' % np.mean(aucs)\n",
    "        blend_X_test[:, j] = np.asarray(dataset_blend_test_j).T.mean(1)\n",
    "\n",
    "    print('blending')\n",
    "    clf = LogisticRegression()\n",
    "    # clf = GradientBoostingClassifier()\n",
    "    clf.fit(blend_X_train, y)\n",
    "    y_submission = clf.predict_proba(blend_X_test)[:, 1]\n",
    "\n",
    "    # Linear stretch of predictions to [0,1]\n",
    "    y_submission = (y_submission - y_submission.min()) / (y_submission.max() - y_submission.min())\n",
    "    if predict_X is not None:\n",
    "        return y_submission\n",
    "    log += '\\n  blend auc: %f\\n\\n' % roc_auc_score(y_test_blend, y_submission)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = datetime.datetime.now()\n",
    "    print(start.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    log = '%s\\n' % start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    cpu_jobs = os.cpu_count() - 1\n",
    "    date_null = pd.to_datetime('1970-01-01', format='%Y-%m-%d')\n",
    "\n",
    "    # analysis()\n",
    "    # detect_duplicate_columns()\n",
    "    # feature_importance_score()\n",
    "\n",
    "    grid_search_gbdt()\n",
    "    # train_gbdt()\n",
    "    # predict('gbdt')\n",
    "\n",
    "    # grid_search_xgb()\n",
    "    # train_xgb()\n",
    "    # predict('xgb')\n",
    "\n",
    "    # grid_search_lgb()\n",
    "    # train_lgb()\n",
    "    # predict('lgb')\n",
    "\n",
    "    # grid_search_cat()\n",
    "    # train_cat()\n",
    "    # predict('cat')\n",
    "\n",
    "    # grid_search_rf()\n",
    "    # train_rf_gini()\n",
    "    # predict('rf_gini')\n",
    "\n",
    "    # grid_search_rf('entropy')\n",
    "    # train_rf_entropy()\n",
    "    # predict('rf_entropy')\n",
    "\n",
    "    # grid_search_et()\n",
    "    # train_et_gini()\n",
    "    # predict('et_gini')\n",
    "\n",
    "    # grid_search_et('entropy')\n",
    "    # train_et_entropy()\n",
    "    # predict('et_entropy')\n",
    "\n",
    "    # blending()\n",
    "    # predict('blending')\n",
    "\n",
    "    log += 'time: %s\\n' % str((datetime.datetime.now() - start)).split('.')[0]\n",
    "    log += '----------------------------------------------------\\n'\n",
    "    open('%s.log' % os.path.basename(__file__), 'a').write(log)\n",
    "    print(log)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
