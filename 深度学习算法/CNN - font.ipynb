{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import numpy as np;\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AGENCY': 0, 'ARIAL': 1, 'BAITI': 2, 'BANKGOTHIC': 3, 'BASKERVILLE': 4, 'BAUHAUS': 5, 'BELL': 6, 'BERLIN': 7, 'BERNARD': 8, 'BITSTREAMVERA': 9, 'BLACKADDER': 10, 'BODONI': 11, 'BOOK': 12, 'BOOKMAN': 13, 'BRADLEY': 14, 'BRITANNIC': 15, 'BROADWAY': 16, 'BRUSH': 17, 'BUXTON': 18, 'CAARD': 19, 'CALIBRI': 20, 'CALIFORNIAN': 21, 'CALISTO': 22, 'CAMBRIA': 23, 'CANDARA': 24, 'CASTELLAR': 25, 'CENTAUR': 26, 'CENTURY': 27, 'CHILLER': 28, 'CITYBLUEPRINT': 29, 'COMIC': 30, 'COMMERCIALSCRIPT': 31, 'COMPLEX': 32, 'CONSOLAS': 33, 'CONSTANTIA': 34, 'COOPER': 35, 'COPPERPLATE': 36, 'CORBEL': 37, 'COUNTRYBLUEPRINT': 38, 'COURIER': 39, 'CREDITCARD': 40, 'CURLZ': 41, 'DUTCH801': 42, 'E13B': 43, 'EBRIMA': 44, 'EDWARDIAN': 45, 'ELEPHANT': 46, 'ENGLISH': 47, 'ENGRAVERS': 48, 'ERAS': 49, 'EUROROMAN': 50, 'FELIX TITLING': 51, 'FOOTLIGHT': 52, 'FORTE': 53, 'FRANKLIN': 54, 'FREESTYLE': 55, 'FRENCH': 56, 'GABRIOLA': 57, 'GADUGI': 58, 'GARAMOND': 59, 'GEORGIA': 60, 'GIGI': 61, 'GILL': 62, 'GLOUCESTER': 63, 'GOTHICE': 64, 'GOUDY': 65, 'GUNPLAY': 66, 'HAETTENSCHWEILER': 67, 'HANDPRINT': 68, 'HARLOW': 69, 'HARRINGTON': 70, 'HIGH TOWER': 71, 'HIMALAYA': 72, 'IMPACT': 73, 'IMPRINT': 74, 'INFORMAL': 75, 'ISOC': 76, 'ITALIC': 77, 'JAVANESE': 78, 'JOKERMAN': 79, 'JUICE': 80, 'KRISTEN': 81, 'KUNSTLER': 82, 'LEELAWADEE': 83, 'LUCIDA': 84, 'MAGNETO': 85, 'MAIANDRA': 86, 'MATURA': 87, 'MINGLIU': 88, 'MISTRAL': 89, 'MODERN': 90, 'MONEY': 91, 'MONOSPAC821': 92, 'MONOTXT': 93, 'MONOTYPE': 94, 'MV_BOLI': 95, 'MYANMAR': 96, 'NIAGARA': 97, 'NINA': 98, 'NIRMALA': 99, 'NUMERICS': 100, 'OCRA': 101, 'OCRB': 102, 'ONYX': 103, 'PALACE': 104, 'PALATINO': 105, 'PANROMAN': 106, 'PAPYRUS': 107, 'PERPETUA': 108, 'PHAGSPA': 109, 'PLAYBILL': 110, 'PMINGLIU-EXTB': 111, 'PRISTINA': 112, 'PROXY': 113, 'QUICKTYPE': 114, 'RAGE': 115, 'RAVIE': 116, 'REFERENCE': 117, 'RICHARD': 118, 'ROCKWELL': 119, 'ROMAN': 120, 'ROMANTIC': 121, 'SANSSERIF': 122, 'SCRIPT': 123, 'SCRIPTB': 124, 'SEGOE': 125, 'SERIF': 126, 'SHOWCARD': 127, 'SIMPLEX': 128, 'SITKA': 129, 'SKETCHFLOW': 130, 'SNAP': 131, 'STENCIL': 132, 'STYLUS': 133, 'SUPERFRENCH': 134, 'SWIS721': 135, 'SYLFAEN': 136, 'TAHOMA': 137, 'TAI': 138, 'TECHNIC': 139, 'TEMPUS': 140, 'TIMES': 141, 'TREBUCHET': 142, 'TW': 143, 'TXT': 144, 'VERDANA': 145, 'VIN': 146, 'VINER': 147, 'VINETA': 148, 'VIVALDI': 149, 'VLADIMIR': 150, 'WIDE': 151, 'YI BAITI': 152}\n",
      "AGENCY 0\n",
      "ARIAL 1\n",
      "BAITI 2\n",
      "BANKGOTHIC 3\n",
      "BASKERVILLE 4\n",
      "BAUHAUS 5\n",
      "BELL 6\n",
      "BERLIN 7\n",
      "BERNARD 8\n",
      "BITSTREAMVERA 9\n",
      "BLACKADDER 10\n",
      "BODONI 11\n",
      "BOOK 12\n",
      "BOOKMAN 13\n",
      "BRADLEY 14\n",
      "BRITANNIC 15\n",
      "BROADWAY 16\n",
      "BRUSH 17\n",
      "BUXTON 18\n",
      "CAARD 19\n",
      "CALIBRI 20\n",
      "CALIFORNIAN 21\n",
      "CALISTO 22\n",
      "CAMBRIA 23\n",
      "CANDARA 24\n",
      "CASTELLAR 25\n",
      "CENTAUR 26\n",
      "CENTURY 27\n",
      "CHILLER 28\n",
      "CITYBLUEPRINT 29\n",
      "COMIC 30\n",
      "COMMERCIALSCRIPT 31\n",
      "COMPLEX 32\n",
      "CONSOLAS 33\n",
      "CONSTANTIA 34\n",
      "COOPER 35\n",
      "COPPERPLATE 36\n",
      "CORBEL 37\n",
      "COUNTRYBLUEPRINT 38\n",
      "COURIER 39\n",
      "CREDITCARD 40\n",
      "CURLZ 41\n",
      "DUTCH801 42\n",
      "E13B 43\n",
      "EBRIMA 44\n",
      "EDWARDIAN 45\n",
      "ELEPHANT 46\n",
      "ENGLISH 47\n",
      "ENGRAVERS 48\n",
      "ERAS 49\n",
      "EUROROMAN 50\n",
      "FELIX TITLING 51\n",
      "FOOTLIGHT 52\n",
      "FORTE 53\n",
      "FRANKLIN 54\n",
      "FREESTYLE 55\n",
      "FRENCH 56\n",
      "GABRIOLA 57\n",
      "GADUGI 58\n",
      "GARAMOND 59\n",
      "GEORGIA 60\n",
      "GIGI 61\n",
      "GILL 62\n",
      "GLOUCESTER 63\n",
      "GOTHICE 64\n",
      "GOUDY 65\n",
      "GUNPLAY 66\n",
      "HAETTENSCHWEILER 67\n",
      "HANDPRINT 68\n",
      "HARLOW 69\n",
      "HARRINGTON 70\n",
      "HIGH TOWER 71\n",
      "HIMALAYA 72\n",
      "IMPACT 73\n",
      "IMPRINT 74\n",
      "INFORMAL 75\n",
      "ISOC 76\n",
      "ITALIC 77\n",
      "JAVANESE 78\n",
      "JOKERMAN 79\n",
      "JUICE 80\n",
      "KRISTEN 81\n",
      "KUNSTLER 82\n",
      "LEELAWADEE 83\n",
      "LUCIDA 84\n",
      "MAGNETO 85\n",
      "MAIANDRA 86\n",
      "MATURA 87\n",
      "MINGLIU 88\n",
      "MISTRAL 89\n",
      "MODERN 90\n",
      "MONEY 91\n",
      "MONOSPAC821 92\n",
      "MONOTXT 93\n",
      "MONOTYPE 94\n",
      "MV_BOLI 95\n",
      "MYANMAR 96\n",
      "NIAGARA 97\n",
      "NINA 98\n",
      "NIRMALA 99\n",
      "NUMERICS 100\n",
      "OCRA 101\n",
      "OCRB 102\n",
      "ONYX 103\n",
      "PALACE 104\n",
      "PALATINO 105\n",
      "PANROMAN 106\n",
      "PAPYRUS 107\n",
      "PERPETUA 108\n",
      "PHAGSPA 109\n",
      "PLAYBILL 110\n",
      "PMINGLIU-EXTB 111\n",
      "PRISTINA 112\n",
      "PROXY 113\n",
      "QUICKTYPE 114\n",
      "RAGE 115\n",
      "RAVIE 116\n",
      "REFERENCE 117\n",
      "RICHARD 118\n",
      "ROCKWELL 119\n",
      "ROMAN 120\n",
      "ROMANTIC 121\n",
      "SANSSERIF 122\n",
      "SCRIPT 123\n",
      "SCRIPTB 124\n",
      "SEGOE 125\n",
      "SERIF 126\n",
      "SHOWCARD 127\n",
      "SIMPLEX 128\n",
      "SITKA 129\n",
      "SKETCHFLOW 130\n",
      "SNAP 131\n",
      "STENCIL 132\n",
      "STYLUS 133\n",
      "SUPERFRENCH 134\n",
      "SWIS721 135\n",
      "SYLFAEN 136\n",
      "TAHOMA 137\n",
      "TAI 138\n",
      "TECHNIC 139\n",
      "TEMPUS 140\n",
      "TIMES 141\n",
      "TREBUCHET 142\n",
      "TW 143\n",
      "TXT 144\n",
      "VERDANA 145\n",
      "VIN 146\n",
      "VINER 147\n",
      "VINETA 148\n",
      "VIVALDI 149\n",
      "VLADIMIR 150\n",
      "WIDE 151\n",
      "YI BAITI 152\n"
     ]
    }
   ],
   "source": [
    "#数据预处理函数，在dir文件夹下每个子文件是一类内容\n",
    "\n",
    "def datahelper(dir):\n",
    "#返回为文本，文本对应标签，标签及索引，索引及标签\n",
    "    \n",
    "    labels_index={}\n",
    "    index_lables={}\n",
    "    fs = os.listdir(dir)\n",
    "\n",
    "    i = 0;\n",
    "    for f in fs:\n",
    "        labels_index[f[:-4]] = i;\n",
    "        index_lables[i] = f[:-4]\n",
    "        i = i + 1;\n",
    "    print(labels_index)\n",
    "    \n",
    "#     texts = []   # 每句话（jieba拆词后）的列表\n",
    "    labels = []  # list of label ids\n",
    "    X = []\n",
    "    \n",
    "    for la in labels_index.keys():\n",
    "        print(la + \" \" + str(labels_index[la]))\n",
    "        la_dir = 'D:/fonts' + \"/\" + la+'.csv'\n",
    "        font_datas = np.array(pd.read_csv(la_dir).iloc[:,12:])\n",
    "        for cnt in range(len(font_datas)):\n",
    "            labels.append(labels_index[la])\n",
    "            X.append(font_datas[cnt])\n",
    "            \n",
    "#         for f in fs:\n",
    "#             file = open(la_dir + \"/\" + f, encoding='utf-8') # 打开该txt文件\n",
    "#             lines = file.readlines();\n",
    "#             text = ''\n",
    "#             num_recs=0\n",
    "#             for line in lines:\n",
    "#                 if len(line) > 5: # 只提取大于5个字的语句\n",
    "#                     line = extract_chinese(line) # 提取中文有效信息，汉字 字母 数字\n",
    "#                     words = jieba.lcut(line, cut_all=False, HMM=True) # 对每句话拆分成词语的list\n",
    "#                     text = words\n",
    "#                     texts.append(text)\n",
    "#                     labels.append(labels_index[la])\n",
    "#                     num_recs = num_recs + 1\n",
    "    return np.array(X),labels,labels_index,index_lables\n",
    "\n",
    "train_dir = 'D:/fonts'\n",
    "# train_dir = 'D:/THUCNews'\n",
    "# train_dir = 'D:/THUCNewsSmall'\n",
    "\n",
    "X,labels,labels_index,index_lables = datahelper(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textCNN模型构造 - BN处理\n",
    "class fontCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(fontCNN, self).__init__()\n",
    "        #需要将事先训练好的词向量载入\n",
    "        self.conv1 = nn.Sequential( # 1*20*20\n",
    "                      nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5,stride=1, padding=2),\n",
    "                      nn.BatchNorm2d(num_features=16, eps=1e-05, momentum=0.1, affine=True), # BN 处理\n",
    "                      nn.ReLU(),\n",
    "                      nn.MaxPool2d(kernel_size=2) # (16,10,10)\n",
    "                     )\n",
    "        self.conv2 = nn.Sequential(\n",
    "                      nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
    "                      nn.BatchNorm2d(num_features=32, eps=1e-05, momentum=0.1, affine=True), # BN 处理\n",
    "                      nn.ReLU(),\n",
    "                      nn.MaxPool2d(2) # 32 5 5\n",
    "                     )\n",
    "        self.conv3 = nn.Sequential(\n",
    "                      nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),\n",
    "                      nn.BatchNorm2d(num_features=64, eps=1e-05, momentum=0.1, affine=True), # BN 处理\n",
    "                      nn.ReLU(),\n",
    "                      nn.MaxPool2d(2) # 64 2 2\n",
    "        )\n",
    "        self.out = nn.Linear(64*2*2, len(set(labels)))\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x=x.view(x.size(0),1,max_len,word_dim)\n",
    "        #print(x.size())\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1) # 将（batch，outchanel,w,h）展平为（batch，outchanel*w*h）\n",
    "        #print(x.size())\n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取空模型\n",
    "# cnn = fontCNN()\n",
    "\n",
    "# 加载原有模型\n",
    "# 有BN层\n",
    "cnn = fontCNN()\n",
    "cnn.load_state_dict(torch.load('C:/Users/jxjsj/Desktop/JupyterHome/DLmodel/fontCNN_BN.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建textCNN模型超参数与数据封装入加载器 - LSC\n",
    "\n",
    "#损失函数\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "#划分训练数据和测试数据\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = torch.Tensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "x_test = torch.Tensor(x_test)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=5000,shuffle=True)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2000,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Train Acc: 0.614202\n",
      "Test Acc: 0.579683\n",
      "epoch 2\n",
      "Train Acc: 0.614561\n",
      "Test Acc: 0.579269\n",
      "epoch 3\n",
      "Train Acc: 0.614667\n",
      "Test Acc: 0.580134\n",
      "epoch 4\n",
      "Train Acc: 0.614750\n",
      "Test Acc: 0.579449\n",
      "epoch 5\n",
      "Train Acc: 0.614924\n",
      "Test Acc: 0.579804\n",
      "epoch 6\n",
      "Train Acc: 0.614874\n",
      "Test Acc: 0.579954\n",
      "epoch 7\n",
      "Train Acc: 0.615365\n",
      "Test Acc: 0.580098\n",
      "epoch 8\n",
      "Train Acc: 0.615511\n",
      "Test Acc: 0.579575\n",
      "epoch 9\n",
      "Train Acc: 0.615645\n",
      "Test Acc: 0.579713\n",
      "epoch 10\n",
      "Train Acc: 0.615548\n",
      "Test Acc: 0.580344\n"
     ]
    }
   ],
   "source": [
    "# 训练 - LSC\n",
    "\n",
    "use_gpu = True\n",
    "\n",
    "LR = 0.00007\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)\n",
    "\n",
    "if use_gpu:\n",
    "    cnn = cnn.cuda()\n",
    "else:\n",
    "    cnn = cnn.cpu()\n",
    "\n",
    "for epoch in range(10):\n",
    "    print('epoch {}'.format(epoch + 1))\n",
    "    # training-----------------------------\n",
    "    cnn.train()\n",
    "    train_acc = 0.\n",
    "\n",
    "    for step, (batch_x, batch_y) in enumerate(train_data_loader):\n",
    "        batch_x, batch_y = Variable(batch_x), Variable(batch_y)\n",
    "        \n",
    "        batch_x = batch_x.view(batch_x.size(0),1,20,20)\n",
    "        \n",
    "        if use_gpu:\n",
    "            batch_x = batch_x.cuda()\n",
    "            batch_y = batch_y.cuda()\n",
    "            \n",
    "        out = cnn(batch_x)\n",
    "        loss = loss_function(out, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pred = torch.max(out, 1)[1]\n",
    "        num_correct = (pred == batch_y).sum()\n",
    "        train_acc += num_correct.data\n",
    "        \n",
    "#         print('Step:',step+1,'Finished!')\n",
    "    print('Train Acc: {:.6f}'.format(train_acc.cpu().numpy() / (len(train_dataset))))\n",
    "#     print(classification_report(L_train_real,L_train_pred))\n",
    "\n",
    "    # evaluation--------------------------------\n",
    "    cnn.eval()\n",
    "    with torch.no_grad():\n",
    "        eval_acc = 0.\n",
    "\n",
    "        for batch_x, batch_y in test_data_loader:\n",
    "            batch_x, batch_y = Variable(batch_x), Variable(batch_y)\n",
    "            \n",
    "            batch_x = batch_x.view(batch_x.size(0),1,20,20)\n",
    "\n",
    "            if use_gpu:\n",
    "                batch_x = batch_x.cuda()\n",
    "                batch_y = batch_y.cuda()\n",
    "\n",
    "            out = cnn(batch_x)\n",
    "            loss = loss_function(out, batch_y)\n",
    "            \n",
    "            pred = torch.max(out, 1)[1]\n",
    "            num_correct = (pred == batch_y).sum()\n",
    "            eval_acc += num_correct\n",
    "            \n",
    "        print('Test Acc: {:.6f}'.format(eval_acc.cpu().numpy() / (len(test_dataset))))\n",
    "#         print(classification_report(L_val_real,L_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有BN层\n",
    "torch.save(cnn.state_dict(),'C:/Users/jxjsj/Desktop/JupyterHome/DLmodel/fontCNN_BN.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
