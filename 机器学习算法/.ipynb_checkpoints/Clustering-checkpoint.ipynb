{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2069: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# 读取数据carEvaluation数据集\n",
    "path = u'C:/Users/jxjsj/Desktop/JupyterHome/Data/carEvaluation.txt'\n",
    "dataSet = pd.read_csv(path, header = None)\n",
    "x_temp, y_temp = np.split(dataSet, (6,), axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_temp, y_temp, random_state=1, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2069: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# 读取数据wine_quality\n",
    "path = u'C:/Users/jxjsj/Desktop/JupyterHome/Data/winequality-white-test.csv'\n",
    "dataSet = pd.read_csv(path, header = 0)\n",
    "x_temp, y_temp = np.split(dataSet, (11,), axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_temp, y_temp, random_state=1, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2069: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "x_temp = pd.DataFrame(iris.data, columns = ['Sepal.L','Sepal.W','Petal.L','Petal.W'])\n",
    "y_temp = pd.DataFrame(iris.target, columns =['Class'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_temp, y_temp, random_state=1, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0=high',\n",
       " '0=low',\n",
       " '0=med',\n",
       " '0=vhigh',\n",
       " '1=high',\n",
       " '1=low',\n",
       " '1=med',\n",
       " '1=vhigh',\n",
       " '2=2',\n",
       " '2=3',\n",
       " '2=4',\n",
       " '2=5more',\n",
       " '3=2',\n",
       " '3=4',\n",
       " '3=more',\n",
       " '4=big',\n",
       " '4=med',\n",
       " '4=small',\n",
       " '5=high',\n",
       " '5=low',\n",
       " '5=med']"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 属性变量编码 方法1 好用！！，方便变回原数据进行展示\n",
    "vec = DictVectorizer(sparse=False)\n",
    "x_temp = vec.fit_transform(x_temp.to_dict(orient='record'))   #对训练数据的特征进行提取\n",
    "x_train = vec.fit_transform(x_train.to_dict(orient='record'))\n",
    "x_test = vec.fit_transform(x_test.to_dict(orient='record'))\n",
    "vec.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7302382722834697"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carCluster = KMeans( n_clusters=3, \n",
    "                     init='k-means++', \n",
    "                     n_init=10, \n",
    "                     max_iter=300, \n",
    "                     tol=0.0001, \n",
    "                     precompute_distances='auto', \n",
    "                     verbose=0, \n",
    "                     random_state=1, \n",
    "                     copy_x=True, \n",
    "                     n_jobs=None, \n",
    "                     algorithm='auto')\n",
    "carCluster.fit(x_temp)\n",
    "metrics.adjusted_rand_score(carCluster.labels_, y_temp['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5681159420289855"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carCluster = DBSCAN(eps=1.3, \n",
    "                    min_samples=2, \n",
    "                    metric='cityblock', \n",
    "                    metric_params=None, \n",
    "                    algorithm='auto', \n",
    "                    leaf_size=2, \n",
    "                    p=None, \n",
    "                    n_jobs=None)\n",
    "carCluster.fit(x_temp)\n",
    "metrics.adjusted_rand_score(carCluster.labels_, y_temp['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9038742317748124"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carCluster = GaussianMixture(n_components=3, \n",
    "                             covariance_type='full', \n",
    "                             tol=0.001, \n",
    "                             reg_covar=1e-06, \n",
    "                             max_iter=100, \n",
    "                             n_init=1, \n",
    "                             init_params='kmeans', \n",
    "                             weights_init=None, \n",
    "                             means_init=None, \n",
    "                             precisions_init=None, \n",
    "                             random_state=1, \n",
    "                             warm_start=False, \n",
    "                             verbose=0, \n",
    "                             verbose_interval=10)\n",
    "carCluster.fit(x_temp)\n",
    "metrics.adjusted_rand_score(carCluster.predict(x_temp), y_temp['Class'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "总结 2018.11.20\n",
    "对于非平衡数据集，聚类效果非常差，对于平衡数据集iris，K均值聚类效果较好，以K均值聚类结果作为参数初始化依据的高斯混合聚类效果能进一步提升。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8922290101649087\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        36\n",
      "           1       0.97      0.91      0.94        32\n",
      "           2       0.92      0.97      0.95        37\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       105\n",
      "   macro avg       0.96      0.96      0.96       105\n",
      "weighted avg       0.96      0.96      0.96       105\n",
      "\n",
      "1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       1.00      1.00      1.00        18\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 学习向量量化，个人撰写\n",
    "import heapq\n",
    "def LVQ_ML(X, Y, prototype_num = 10, leaning_rate = 0.1, max_iter = 200, balanced=False, X_test=None):\n",
    "    '''\n",
    "    X             ：训练数据的特征向量矩阵，DataFrame\n",
    "    Y             ：训练数据的标签，一列DataFrame\n",
    "    prototype_num ：原型向量的预设类别标记个数（通常大于标签类别个数）\n",
    "    leaning_rate  ：原型向量的更新学习率\n",
    "    max_iter      ：最大迭代轮数\n",
    "    balanced      ：是否增大少量样本的抽取概率\n",
    "    X_test        ：测试数据的特征向量矩阵，DataFrame\n",
    "    output        ：prototype_v：原型向量\n",
    "                    y_label_pro_v：原型向量对应的标签\n",
    "                    x_cluster_lst：所有x的簇，即子类\n",
    "                    x_fit_label：预测的x的标签\n",
    "    较适用均衡数据，数值型特征向量，属性型标签\n",
    "    '''\n",
    "    x_temp = np.array(X)\n",
    "    y_temp = np.array(Y).ravel()\n",
    "    \n",
    "    y_label_lst = list(set(y_temp.tolist()))\n",
    "    y_label_num = len(y_label_lst)\n",
    "    \n",
    "    # 计算每个标签的子簇数量，平均分配（故适用平衡数据）\n",
    "    prototype_num_i = prototype_num//y_label_num\n",
    "    prototype_num_lst = [prototype_num_i for i in range(y_label_num)]\n",
    "    \n",
    "    for i in range(prototype_num-prototype_num_i*y_label_num):\n",
    "        prototype_num_lst[i] += 1\n",
    "    \n",
    "    # 将每个标签对应的数据拆分为多个Array\n",
    "    def cut_with_label(df, label):\n",
    "        df_dct = {}\n",
    "        labels_lst = list(set(df[label].tolist()))\n",
    "        for yi in labels_lst:\n",
    "            df_temp = df[df[label].isin([yi])]\n",
    "            del df_temp[label]\n",
    "            df_temp = np.array(df_temp)\n",
    "            df_dct[yi] = df_temp\n",
    "        n_list = []\n",
    "        for key in df_dct.keys():\n",
    "            n_list.append(len(df_dct[key]))\n",
    "        return df_dct\n",
    "    df_dct = cut_with_label(pd.concat([X,Y],axis=1),Y.columns[0])\n",
    "    \n",
    "    # 随机生成初始化原型向量\n",
    "    y_label_pro_v = []\n",
    "    for n in range(len(y_label_lst)):\n",
    "        # 读取该标签对应的特征向量矩阵\n",
    "        df_temp = df_dct[y_label_lst[n]]\n",
    "        pro_num_for_yi = prototype_num_lst[n]\n",
    "        pro_id_for_yi = np.random.choice(a=len(df_temp), size=pro_num_for_yi, replace=False, p=None)\n",
    "        # 保存初始化原型向量的标签\n",
    "        for cnt in range(pro_num_for_yi):\n",
    "            y_label_pro_v.append(y_label_lst[n])\n",
    "        # 合并原型向量\n",
    "        try:\n",
    "            prototype_v = np.concatenate((prototype_v,df_temp[list(pro_id_for_yi),:]),axis=0)\n",
    "        except:\n",
    "            prototype_v = df_temp[list(pro_id_for_yi),:]\n",
    "\n",
    "    # 制作非平衡数据时使用的对应随机抽取概率，p=n/n_yi\n",
    "    y_b = y_temp\n",
    "    sum_n = len(y_b)\n",
    "    labelCounts = {}\n",
    "    for i in y_b:\n",
    "        if i not in labelCounts.keys(): \n",
    "            labelCounts[i] = 0\n",
    "        labelCounts[i] += 1\n",
    "    for i in labelCounts.keys():\n",
    "        labelCounts[i] = sum_n/labelCounts[i]\n",
    "    prob_list_temp = [labelCounts[i] for i in y_temp]\n",
    "    sum_prob = sum(prob_list_temp)\n",
    "    prob_list = [i/sum_prob for i in prob_list_temp]\n",
    "    \n",
    "    # 开始迭代更新原型向量    \n",
    "    while max_iter > 0:\n",
    "        \n",
    "        # 是否使用平衡数据概率\n",
    "        if balanced:\n",
    "            j = np.random.choice(len(x_temp), 1, p=prob_list)[0]\n",
    "            \n",
    "        else:\n",
    "            j = np.random.choice(len(x_temp), 1)[0]\n",
    "        \n",
    "        x_j = x_temp[j]\n",
    "        y_j = y_temp[j]\n",
    "        # 计算随机取得的x_j与各原型向量的L2范数\n",
    "        dist_lst = [np.linalg.norm(x_j - xi_v) for xi_v in prototype_v]\n",
    "        i_v = list(map(dist_lst.index, heapq.nsmallest(1, dist_lst)))[0]\n",
    "        #  更新原型向量\n",
    "        if y_j == y_label_pro_v[i_v]:\n",
    "            prototype_v[i_v] = prototype_v[i_v] + leaning_rate*(x_j-prototype_v[i_v])\n",
    "        else:\n",
    "            prototype_v[i_v] = prototype_v[i_v] - leaning_rate*(x_j-prototype_v[i_v])       \n",
    "        max_iter -= 1\n",
    "    \n",
    "    # 保存所有训练数据的簇，以及预测的标签\n",
    "    x_cluster_lst = []\n",
    "    x_fit_label = []\n",
    "    for x_i in x_temp:\n",
    "        dist_lst = [np.linalg.norm(x_i - xi_v) for xi_v in prototype_v]\n",
    "        i_cluster = list(map(dist_lst.index, heapq.nsmallest(1, dist_lst)))[0]\n",
    "        x_i_cluster = str(y_label_pro_v[i_cluster])+'-'+str(i_cluster)\n",
    "        x_cluster_lst.append(x_i_cluster)\n",
    "        x_fit_label.append(y_label_pro_v[i_cluster])\n",
    "        \n",
    "    # 划分测试数据的簇和预测其标签\n",
    "    x_test_cluster_lst = []\n",
    "    x_test_fit_label = []\n",
    "    try: \n",
    "        if not X_test:\n",
    "            pass\n",
    "    except:\n",
    "        x_test_temp = np.array(X_test)\n",
    "        for x_i in x_test_temp:\n",
    "            dist_lst = [np.linalg.norm(x_i - xi_v) for xi_v in prototype_v]\n",
    "            i_cluster = list(map(dist_lst.index, heapq.nsmallest(1, dist_lst)))[0]\n",
    "            x_i_cluster = str(y_label_pro_v[i_cluster])+'-'+str(i_cluster)\n",
    "            x_test_cluster_lst.append(x_i_cluster)\n",
    "            x_test_fit_label.append(y_label_pro_v[i_cluster])\n",
    "   \n",
    "    return prototype_v, y_label_pro_v, x_cluster_lst, x_fit_label, x_test_cluster_lst, x_test_fit_label\n",
    "\n",
    "\n",
    "prototype_v, y_label_pro_v, x_cluster_lst, x_fit_label, x_test_cluster_lst, x_test_fit_label = \\\n",
    "LVQ_ML(x_train, y_train, prototype_num = 20, leaning_rate = 0.1, max_iter = 1000, X_test = x_test, balanced=True)\n",
    "\n",
    "print(metrics.adjusted_rand_score(y_train['Class'], x_fit_label))\n",
    "print(classification_report(y_train['Class'], x_fit_label))\n",
    "print(metrics.adjusted_rand_score(y_test['Class'], x_test_fit_label))\n",
    "print(classification_report(y_test['Class'], x_test_fit_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.get_dummies(x_train)\n",
    "x_test = pd.get_dummies(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9443455220828789\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        36\n",
      "           1       0.97      0.97      0.97        32\n",
      "           2       0.97      0.97      0.97        37\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       105\n",
      "   macro avg       0.98      0.98      0.98       105\n",
      "weighted avg       0.98      0.98      0.98       105\n",
      "\n",
      "0.9312926240202837\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       0.95      1.00      0.97        18\n",
      "           2       1.00      0.92      0.96        13\n",
      "\n",
      "   micro avg       0.98      0.98      0.98        45\n",
      "   macro avg       0.98      0.97      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 学习向量量化，个人撰写-把随机选择一个向量去更新原型向量改成了遍历\n",
    "import heapq\n",
    "def LVQ_ML(X, Y, prototype_num = 10, leaning_rate = 0.1, max_iter = 200, balanced=False, X_test=None):\n",
    "    '''\n",
    "    X             ：训练数据的特征向量矩阵，DataFrame\n",
    "    Y             ：训练数据的标签，一列DataFrame\n",
    "    prototype_num ：原型向量的预设类别标记个数（通常大于标签类别个数）\n",
    "    leaning_rate  ：原型向量的更新学习率\n",
    "    max_iter      ：最大迭代轮数\n",
    "    balanced      ：是否增大少量样本的抽取概率\n",
    "    X_test        ：测试数据的特征向量矩阵，DataFrame\n",
    "    output        ：prototype_v：原型向量\n",
    "                    y_label_pro_v：原型向量对应的标签\n",
    "                    x_cluster_lst：所有x的簇，即子类\n",
    "                    x_fit_label：预测的x的标签\n",
    "    较适用均衡数据，数值型特征向量，属性型标签\n",
    "    '''\n",
    "    x_temp = np.array(X)\n",
    "    y_temp = np.array(Y).ravel()\n",
    "    \n",
    "    y_label_lst = list(set(y_temp.tolist()))\n",
    "    y_label_num = len(y_label_lst)\n",
    "    \n",
    "    # 计算每个标签的子簇数量，平均分配（故适用平衡数据）\n",
    "    prototype_num_i = prototype_num//y_label_num\n",
    "    prototype_num_lst = [prototype_num_i for i in range(y_label_num)]\n",
    "    \n",
    "    for i in range(prototype_num-prototype_num_i*y_label_num):\n",
    "        prototype_num_lst[i] += 1\n",
    "    \n",
    "    # 将每个标签对应的数据拆分为多个Array\n",
    "    def cut_with_label(df, label):\n",
    "        df_dct = {}\n",
    "        labels_lst = list(set(df[label].tolist()))\n",
    "        for yi in labels_lst:\n",
    "            df_temp = df[df[label].isin([yi])]\n",
    "            del df_temp[label]\n",
    "            df_temp = np.array(df_temp)\n",
    "            df_dct[yi] = df_temp\n",
    "        n_list = []\n",
    "        for key in df_dct.keys():\n",
    "            n_list.append(len(df_dct[key]))\n",
    "        return df_dct\n",
    "    df_dct = cut_with_label(pd.concat([X,Y],axis=1),Y.columns[0])\n",
    "    \n",
    "    # 随机生成初始化原型向量\n",
    "    y_label_pro_v = []\n",
    "    for n in range(len(y_label_lst)):\n",
    "        # 读取该标签对应的特征向量矩阵\n",
    "        df_temp = df_dct[y_label_lst[n]]\n",
    "        pro_num_for_yi = prototype_num_lst[n]\n",
    "        pro_id_for_yi = np.random.choice(a=len(df_temp), size=pro_num_for_yi, replace=False, p=None)\n",
    "        # 保存初始化原型向量的标签\n",
    "        for cnt in range(pro_num_for_yi):\n",
    "            y_label_pro_v.append(y_label_lst[n])\n",
    "        # 合并原型向量\n",
    "        try:\n",
    "            prototype_v = np.concatenate((prototype_v,df_temp[list(pro_id_for_yi),:]),axis=0)\n",
    "        except:\n",
    "            prototype_v = df_temp[list(pro_id_for_yi),:]\n",
    "    \n",
    "    # 开始迭代更新原型向量    \n",
    "    while max_iter > 0:\n",
    "        \n",
    "        for j in range(len(x_temp)):\n",
    "            x_j = x_temp[j]\n",
    "            y_j = y_temp[j]\n",
    "            # 计算随机取得的x_j与各原型向量的L2范数\n",
    "            dist_lst = [np.linalg.norm(x_j - xi_v) for xi_v in prototype_v]\n",
    "            i_v = list(map(dist_lst.index, heapq.nsmallest(1, dist_lst)))[0]\n",
    "            #  更新原型向量\n",
    "            if y_j == y_label_pro_v[i_v]:\n",
    "                prototype_v[i_v] = prototype_v[i_v] + leaning_rate*(x_j-prototype_v[i_v])\n",
    "            else:\n",
    "                prototype_v[i_v] = prototype_v[i_v] - leaning_rate*(x_j-prototype_v[i_v])       \n",
    "        max_iter -= 1\n",
    "    \n",
    "    # 保存所有训练数据的簇，以及预测的标签\n",
    "    x_cluster_lst = []\n",
    "    x_fit_label = []\n",
    "    for x_i in x_temp:\n",
    "        dist_lst = [np.linalg.norm(x_i - xi_v) for xi_v in prototype_v]\n",
    "        i_cluster = list(map(dist_lst.index, heapq.nsmallest(1, dist_lst)))[0]\n",
    "        x_i_cluster = str(y_label_pro_v[i_cluster])+'-'+str(i_cluster)\n",
    "        x_cluster_lst.append(x_i_cluster)\n",
    "        x_fit_label.append(y_label_pro_v[i_cluster])\n",
    "        \n",
    "    # 划分测试数据的簇和预测其标签\n",
    "    x_test_cluster_lst = []\n",
    "    x_test_fit_label = []\n",
    "    try: \n",
    "        if not X_test:\n",
    "            pass\n",
    "    except:\n",
    "        x_test_temp = np.array(X_test)\n",
    "        for x_i in x_test_temp:\n",
    "            dist_lst = [np.linalg.norm(x_i - xi_v) for xi_v in prototype_v]\n",
    "            i_cluster = list(map(dist_lst.index, heapq.nsmallest(1, dist_lst)))[0]\n",
    "            x_i_cluster = str(y_label_pro_v[i_cluster])+'-'+str(i_cluster)\n",
    "            x_test_cluster_lst.append(x_i_cluster)\n",
    "            x_test_fit_label.append(y_label_pro_v[i_cluster])\n",
    "   \n",
    "    return prototype_v, y_label_pro_v, x_cluster_lst, x_fit_label, x_test_cluster_lst, x_test_fit_label\n",
    "\n",
    "\n",
    "prototype_v, y_label_pro_v, x_cluster_lst, x_fit_label, x_test_cluster_lst, x_test_fit_label = \\\n",
    "LVQ_ML(x_train, y_train, prototype_num = 20, leaning_rate = 0.1, max_iter = 10, X_test = x_test, balanced=True)\n",
    "\n",
    "print(metrics.adjusted_rand_score(y_train[y_train.columns[0]], x_fit_label))\n",
    "print(classification_report(y_train[y_train.columns[0]], x_fit_label))\n",
    "print(metrics.adjusted_rand_score(y_test[y_train.columns[0]], x_test_fit_label))\n",
    "print(classification_report(y_test[y_test.columns[0]], x_test_fit_label))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "总结 2018.11.20\n",
    "学习向量量化为有监督辅助的聚类，将其视为监督式分类器时，在平衡数据上的表现较好，在非平衡数据上的表现极差（随机选取一个向量对原型进行更新，效果很差）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3062\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3063\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3064\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 6",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-163-ee47ca98bb5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mcarCluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mmax_p\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjusted_rand_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcarCluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_temp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mmax_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mmax_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjusted_rand_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcarCluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_temp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2683\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2684\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2685\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2687\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2690\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2691\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2692\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2694\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   2484\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2485\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2486\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2487\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2488\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   4113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4114\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4115\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4116\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3063\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3065\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 6"
     ]
    }
   ],
   "source": [
    "max_p = 0\n",
    "max_x = 0\n",
    "for x in [x for x in range(2,35)]:\n",
    "    carCluster = GaussianMixture(n_components=x, \n",
    "                             covariance_type='full', \n",
    "                             tol=0.001, \n",
    "                             reg_covar=1e-06, \n",
    "                             max_iter=100, \n",
    "                             n_init=1, \n",
    "                             init_params='kmeans', \n",
    "                             weights_init=None, \n",
    "                             means_init=None, \n",
    "                             precisions_init=None, \n",
    "                             random_state=1, \n",
    "                             warm_start=False, \n",
    "                             verbose=0, \n",
    "                             verbose_interval=10)\n",
    "    carCluster.fit(x_temp)\n",
    "    \n",
    "    if max_p < metrics.adjusted_rand_score(carCluster.predict(x_temp), y_temp[6]):\n",
    "        max_x = x\n",
    "        max_p = metrics.adjusted_rand_score(carCluster.predict(x_temp), y_temp[6])\n",
    "max_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
