{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "参数说明： \n",
    "1. hidden_layer_sizes :元祖格式，长度=n_layers-2, 默认(100，），第i个元素表示第i个隐藏层的神经元的个数。 \n",
    "2. activation :{‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, 默认‘relu \n",
    "- ‘identity’： no-op activation, useful to implement linear bottleneck， \n",
    "返回f(x) = x \n",
    "- ‘logistic’：the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)). \n",
    "- ‘tanh’：the hyperbolic tan function, returns f(x) = tanh(x). \n",
    "- ‘relu’：the rectified linear unit function, returns f(x) = max(0, x) \n",
    "4. solver： {‘lbfgs’, ‘sgd’, ‘adam’}, 默认 ‘adam’，用来优化权重 \n",
    "- lbfgs：quasi-Newton方法的优化器 \n",
    "- sgd：随机梯度下降\n",
    "- adam： Kingma, Diederik, and Jimmy Ba提出的机遇随机梯度的优化器 \n",
    "注意：默认solver ‘adam’在相对较大的数据集上效果比较好（几千个样本或者更多），对小数据集来说，lbfgs收敛更快效果也更好。 \n",
    "5. alpha :float,可选的，默认0.0001,正则化项参数 \n",
    "6. batch_size : int , 可选的，默认‘auto’,随机优化的minibatches的大小，如果solver是‘lbfgs’，分类器将不使用minibatch，当设置成‘auto’，batch_size=min(200,n_samples) \n",
    "7. learning_rate :{‘constant’，‘invscaling’, ‘adaptive’},默认‘constant’，用于权重更新，只有当solver为’sgd‘时使用 \n",
    "- ‘constant’: 有‘learning_rate_init’给定的恒定学习率 \n",
    "- ‘incscaling’：随着时间t使用’power_t’的逆标度指数不断降低学习率learning_rate_ ，effective_learning_rate = learning_rate_init / pow(t, power_t) \n",
    "- ‘adaptive’：只要训练损耗在下降，就保持学习率为’learning_rate_init’不变，当连续两次不能降低训练损耗或验证分数停止升高至少tol时，将当前学习率除以5. \n",
    "8. max_iter: int，可选，默认200，最大迭代次数。 \n",
    "9. random_state:int 或RandomState，可选，默认None，随机数生成器的状态或种子。 \n",
    "10. shuffle: bool，可选，默认True,只有当solver=’sgd’或者‘adam’时使用，判断是否在每次迭代时对样本进行清洗。 \n",
    "11. tol：float, 可选，默认1e-4，优化的容忍度 \n",
    "12. learning_rate_int:double,可选，默认0.001，初始学习率，控制更新权重的补偿，只有当solver=’sgd’ 或’adam’时使用。 \n",
    "13. power_t: double, optional, default 0.5，只有solver=’sgd’时使用，是逆扩展学习率的指数.当learning_rate=’invscaling’，用来更新有效学习率。 \n",
    "14. verbose : bool, optional, default False,是否将过程打印到stdout \n",
    "15. warm_start : bool, optional, default False,当设置成True，使用之前的解决方法作为初始拟合，否则释放之前的解决方法。 \n",
    "16. momentum : float, default 0.9,Momentum(动量） for gradient descent update. Should be between 0 and 1. Only used when solver=’sgd’. \n",
    "17. nesterovs_momentum : boolean, default True, Whether to use Nesterov’s momentum. Only used when solver=’sgd’ and momentum > 0. \n",
    "18. early_stopping : bool, default False,Only effective when solver=’sgd’ or ‘adam’,判断当验证效果不再改善的时候是否终止训练，当为True时，自动选出10%的训练数据用于验证并在两步连续迭代改善低于tol时终止训练。 \n",
    "19. validation_fraction : float, optional, default 0.1,用作早期停止验证的预留训练数据集的比例，早0-1之间，只当early_stopping=True有用 \n",
    "20. beta_1 : float, optional, default 0.9，Only used when solver=’adam’，估计一阶矩向量的指数衰减速率，[0,1)之间 \n",
    "21. beta_2 : float, optional, default 0.999,Only used when solver=’adam’估计二阶矩向量的指数衰减速率[0,1)之间 \n",
    "22. epsilon : float, optional, default 1e-8,Only used when solver=’adam’数值稳定值。 \n",
    "属性说明： \n",
    "- classes_:每个输出的类标签 \n",
    "- loss_:损失函数计算出来的当前损失值 \n",
    "- coefs_:列表中的第i个元素表示i层的权重矩阵 \n",
    "- intercepts_:列表中第i个元素代表i+1层的偏差向量 \n",
    "- n_iter_ ：迭代次数 \n",
    "- n_layers_:层数 \n",
    "- n_outputs_:输出的个数 \n",
    "- out_activation_:输出激活函数的名称。 \n",
    "方法说明： \n",
    "- fit(X,y):拟合 \n",
    "- get_params([deep]):获取参数 \n",
    "- predict(X):使用MLP进行预测 \n",
    "- predic_log_proba(X):返回对数概率估计 \n",
    "- predic_proba(X)：概率估计 \n",
    "- score(X,y[,sample_weight]):返回给定测试数据和标签上的平均准确度 \n",
    "-set_params(**params):设置参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2069: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# 读取数据carEvaluation数据集\n",
    "path = u'C:/Users/jxjsj/Desktop/JupyterHome/carEvaluation.txt'\n",
    "dataSet = pd.read_csv(path, header = None)\n",
    "x_temp, y_temp = np.split(dataSet, (6,), axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_temp, y_temp, random_state=1, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0=high',\n",
       " '0=low',\n",
       " '0=med',\n",
       " '0=vhigh',\n",
       " '1=high',\n",
       " '1=low',\n",
       " '1=med',\n",
       " '1=vhigh',\n",
       " '2=2',\n",
       " '2=3',\n",
       " '2=4',\n",
       " '2=5more',\n",
       " '3=2',\n",
       " '3=4',\n",
       " '3=more',\n",
       " '4=big',\n",
       " '4=med',\n",
       " '4=small',\n",
       " '5=high',\n",
       " '5=low',\n",
       " '5=med']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 属性变量编码 方法1 好用！！，方便变回原数据进行展示\n",
    "vec = DictVectorizer(sparse=False)\n",
    "x_train = vec.fit_transform(x_train.to_dict(orient='record'))   #对训练数据的特征进行提取\n",
    "x_test = vec.transform(x_test.to_dict(orient='record'))         #对测试数据的特征进行提取\n",
    "# y_train = vec.fit_transform(y_train.to_dict(orient='record'))   #对训练数据的特征进行提取\n",
    "# y_test = vec.transform(y_test.to_dict(orient='record'))         #对测试数据的特征进行提取\n",
    "vec.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(3111,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 运用原非平衡样本-训练\n",
    "carEvaluation = MLPClassifier(activation='tanh', \n",
    "                    solver='lbfgs',\n",
    "                    alpha=1e-5,\n",
    "                    hidden_layer_sizes=(len(x_train)*2+1,),\n",
    "                    random_state=1, \n",
    "                   )\n",
    "carEvaluation.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainAccracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acc       1.00      1.00      1.00       325\n",
      "        good       1.00      1.00      1.00        55\n",
      "       unacc       1.00      1.00      1.00       950\n",
      "       vgood       1.00      1.00      1.00        52\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      1382\n",
      "   macro avg       1.00      1.00      1.00      1382\n",
      "weighted avg       1.00      1.00      1.00      1382\n",
      "\n",
      "testAccracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acc       1.00      1.00      1.00        59\n",
      "        good       1.00      1.00      1.00        14\n",
      "       unacc       1.00      1.00      1.00       260\n",
      "       vgood       1.00      1.00      1.00        13\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       346\n",
      "   macro avg       1.00      1.00      1.00       346\n",
      "weighted avg       1.00      1.00      1.00       346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 运用原非平衡样本-评估\n",
    "y_train_predict = carEvaluation.predict(x_train)\n",
    "y_test_predict = carEvaluation.predict(x_test)\n",
    "\n",
    "print('trainAccracy:',carEvaluation.score(x_train,y_train))\n",
    "print(classification_report(y_train,y_train_predict)) #真实数据在前 训练结果在后！\n",
    "print('testAccracy:',carEvaluation.score(x_test,y_test))\n",
    "print(classification_report(y_test,y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'g': 950, 'a': 950, 'u': 950, 'v': 950}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3800"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 运用SMOTE改进的平衡样本\n",
    "over_samples = SMOTE(random_state=11)\n",
    "over_samples_x, over_samples_y = over_samples.fit_sample(x_train, y_train)\n",
    "\n",
    "# 统计y标签各分类频数\n",
    "y = over_samples_y\n",
    "labelCounts = {}\n",
    "for i in y:\n",
    "    if i[0] not in labelCounts.keys(): \n",
    "        labelCounts[i[0]] = 0\n",
    "    labelCounts[i[0]] += 1\n",
    "print(labelCounts)\n",
    "sum_n = 0\n",
    "for i in labelCounts.keys():\n",
    "    sum_n += labelCounts[i]\n",
    "sum_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(3111,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 运用SMOTE改进的平衡样本-训练\n",
    "carEvaluation = MLPClassifier(activation='tanh', \n",
    "                    solver='lbfgs',\n",
    "                    alpha=1e-5,\n",
    "                    hidden_layer_sizes=(len(x_train)*2+1,),\n",
    "                   )\n",
    "carEvaluation.fit(over_samples_x,over_samples_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainAccracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acc       1.00      1.00      1.00       950\n",
      "        good       1.00      1.00      1.00       950\n",
      "       unacc       1.00      1.00      1.00       950\n",
      "       vgood       1.00      1.00      1.00       950\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      3800\n",
      "   macro avg       1.00      1.00      1.00      3800\n",
      "weighted avg       1.00      1.00      1.00      3800\n",
      "\n",
      "testAccracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acc       1.00      1.00      1.00        59\n",
      "        good       1.00      1.00      1.00        14\n",
      "       unacc       1.00      1.00      1.00       260\n",
      "       vgood       1.00      1.00      1.00        13\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       346\n",
      "   macro avg       1.00      1.00      1.00       346\n",
      "weighted avg       1.00      1.00      1.00       346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 运用SMOTE改进的平衡样本-评估\n",
    "y_train_predict = carEvaluation.predict(over_samples_x)\n",
    "y_test_predict = carEvaluation.predict(x_test)\n",
    "\n",
    "print('trainAccracy:',carEvaluation.score(over_samples_x,over_samples_y))\n",
    "print(classification_report(over_samples_y,y_train_predict)) #真实数据在前 训练结果在后！\n",
    "print('testAccracy:',carEvaluation.score(x_test,y_test))\n",
    "print(classification_report(y_test,y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainAccracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        39\n",
      "           1       1.00      1.00      1.00        37\n",
      "           2       1.00      1.00      1.00        44\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       120\n",
      "   macro avg       1.00      1.00      1.00       120\n",
      "weighted avg       1.00      1.00      1.00       120\n",
      "\n",
      "testAccracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00         6\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2069: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "######################################## iris数据 ########################################\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=1, train_size=0.8)\n",
    "\n",
    "# min-max标准化\n",
    "# x_train = pd.DataFrame(x_train).apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)))\n",
    "# x_test = pd.DataFrame(x_test).apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)))\n",
    "\n",
    "irisEvaluation = MLPClassifier(activation='tanh', \n",
    "                    solver='lbfgs',\n",
    "                    alpha=1e-5,\n",
    "                    hidden_layer_sizes=(len(x_train)*2+1,),\n",
    "                   )\n",
    "irisEvaluation.fit(x_train, y_train)\n",
    "\n",
    "y_train_predict = irisEvaluation.predict(x_train)\n",
    "y_test_predict = irisEvaluation.predict(x_test)\n",
    "\n",
    "print('trainAccracy:',irisEvaluation.score(x_train,y_train))\n",
    "print(classification_report(y_train,y_train_predict)) #真实数据在前 训练结果在后！\n",
    "print('testAccracy:',irisEvaluation.score(x_test,y_test))\n",
    "print(classification_report(y_test,y_test_predict))\n",
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugdata = pd.read_csv('c:/Users/jxjsj/Desktop/JupyterHome/Data/drugsComTrain_raw.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "总结1 2018.11.12\n",
    "基于原数据（非平衡数据）：使用logit函数作为激活函数，优化方法选用随机梯度下降法，效果较差，对结果只预测为频率最大的一类；\n",
    "                          优化方法改用拟牛顿法，效果改善显著，经验与测试错误率在2%左右；\n",
    "                          激活函数再改为tanh函数，经验错误率降低为0，测试错误率降低较少。\n",
    "基于用SMOTE算法生成的平衡数据：前两种结果与原数据类似，但激活函数再改为tanh函数时，经验与测试错误率同时降低为0。\n",
    "汽车评估数据集，拟牛顿法优化更好，激活函数tanh更好"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
